,paper_Id,DOI,abstract
2,01d576d3a45bf034bc0bac2774eba5db470ea2de,10.1007/978-3-642-39314-3_2,"What does an information retrieval system look like from a bird’s eye perspective? How can a set of documents be processed by a system to make sense out of their content and find answers to user queries? In this chapter, we will start answering these questions by providing an overview of the information retrieval process. As the search for text is the most widespread information retrieval application, we devote particular emphasis to textual retrieval. The fundamental phases of document processing are illustrated along with the principles and data structures supporting indexing."
5,03072c3d1dc19f05b37060c4fd8dfe4d6bb6a800,10.1186/1471-2288-7-40,"Background
Our objective was to systematically assess the differences in features, results, and usability of currently available meta-analysis programs."
8,080fe7987e1bbc97372177d57267ed56fb113051,10.1186/1471-2288-9-80,"Meta-analysis is increasingly used as a key source of evidence synthesis to inform clinical practice. The theory and statistical foundations of meta-analysis continually evolve, providing solutions to many new and challenging problems. In practice, most meta-analyses are performed in general statistical packages or dedicated meta-analysis programs. Herein, we introduce Meta-Analyst, a novel, powerful, intuitive, and free meta-analysis program for the meta-analysis of a variety of problems. Meta-Analyst is implemented in C# atop of the Microsoft .NET framework, and features a graphical user interface. The software performs several meta-analysis and meta-regression models for binary and continuous outcomes, as well as analyses for diagnostic and prognostic test studies in the frequentist and Bayesian frameworks. Moreover, Meta-Analyst includes a flexible tool to edit and customize generated meta-analysis graphs (e.g., forest plots) and provides output in many formats (images, Adobe PDF, Microsoft Word-ready RTF). The software architecture employed allows for rapid changes to be made to either the Graphical User Interface (GUI) or to the analytic modules. We verified the numerical precision of Meta-Analyst by comparing its output with that from standard meta-analysis routines in Stata over a large database of 11,803 meta-analyses of binary outcome data, and 6,881 meta-analyses of continuous outcome data from the Cochrane Library of Systematic Reviews. Results from analyses of diagnostic and prognostic test studies have been verified in a limited number of meta-analyses versus MetaDisc and MetaTest. Bayesian statistical analyses use the OpenBUGS calculation engine (and are thus as accurate as the standalone OpenBUGS software). We have developed and validated a new program for conducting meta-analyses that combines the advantages of existing software for this task."
10,0cd64c55c98cdc4a7ef041a843ff796a995952a4,10.1136/bmj.315.7109.629,"Abstract Objective: Funnel plots (plots of effect estimates against sample size) may be useful to detect bias in meta-analyses that were later contradicted by large trials. We examined whether a simple test of asymmetry of funnel plots predicts discordance of results when meta-analyses are compared to large trials, and we assessed the prevalence of bias in published meta-analyses. Design: Medline search to identify pairs consisting of a meta-analysis and a single large trial (concordance of results was assumed if effects were in the same direction and the meta-analytic estimate was within 30% of the trial); analysis of funnel plots from 37 meta-analyses identified from a hand search of four leading general medicine journals 1993-6 and 38 meta-analyses from the second 1996 issue of the Cochrane Database of Systematic Reviews . Main outcome measure: Degree of funnel plot asymmetry as measured by the intercept from regression of standard normal deviates against precision. Results: In the eight pairs of meta-analysis and large trial that were identified (five from cardiovascular medicine, one from diabetic medicine, one from geriatric medicine, one from perinatal medicine) there were four concordant and four discordant pairs. In all cases discordance was due to meta-analyses showing larger effects. Funnel plot asymmetry was present in three out of four discordant pairs but in none of concordant pairs. In 14 (38%) journal meta-analyses and 5 (13%) Cochrane reviews, funnel plot asymmetry indicated that there was bias. Conclusions: A simple analysis of funnel plots provides a useful test for the likely presence of bias in meta-analyses, but as the capacity to detect bias will be limited when meta-analyses are based on a limited number of small trials the results from such analyses should be treated with considerable caution. Key messages Systematic reviews of randomised trials are the best strategy for appraising evidence; however, the findings of some meta-analyses were later contradicted by large trials Funnel plots, plots of the trials9 effect estimates against sample size, are skewed and asymmetrical in the presence of publication bias and other biases Funnel plot asymmetry, measured by regression analysis, predicts discordance of results when meta-analyses are compared with single large trials Funnel plot asymmetry was found in 38% of meta-analyses published in leading general medicine journals and in 13% of reviews from the Cochrane Database of Systematic Reviews Critical examination of systematic reviews for publication and related biases should be considered a routine procedure"
22,1f0f5c88e633f25968a77d698c0253caa6d89b92,,No DOI detected
29,28db428a55711c88b1ebd4388729208e9ebfd378,10.1186/1472-6947-10-56,"Clinical trials are one of the most important sources of evidence for guiding evidence-based practice and the design of new trials. However, most of this information is available only in free text - e.g., in journal publications - which is labour intensive to process for systematic reviews, meta-analyses, and other evidence synthesis studies. This paper presents an automatic information extraction system, called ExaCT, that assists users with locating and extracting key trial characteristics (e.g., eligibility criteria, sample size, drug dosage, primary outcomes) from full-text journal articles reporting on randomized controlled trials (RCTs). ExaCT consists of two parts: an information extraction (IE) engine that searches the article for text fragments that best describe the trial characteristics, and a web browser-based user interface that allows human reviewers to assess and modify the suggested selections. The IE engine uses a statistical text classifier to locate those sentences that have the highest probability of describing a trial characteristic. Then, the IE engine's second stage applies simple rules to these sentences to extract text fragments containing the target answer. The same approach is used for all 21 trial characteristics selected for this study. We evaluated ExaCT using 50 previously unseen articles describing RCTs. The text classifier (first stage) was able to recover 88% of relevant sentences among its top five candidates (top5 recall) with the topmost candidate being relevant in 80% of cases (top1 precision). Precision and recall of the extraction rules (second stage) were 93% and 91%, respectively. Together, the two stages of the extraction engine were able to provide (partially) correct solutions in 992 out of 1050 test tasks (94%), with a majority of these (696) representing fully correct and complete answers. Our experiments confirmed the applicability and efficacy of ExaCT. Furthermore, they demonstrated that combining a statistical method with 'weak' extraction rules can identify a variety of study characteristics. The system is flexible and can be extended to handle other characteristics and document types (e.g., study protocols)."
31,3499b8feaced6f6757ea389473541bcab3bb16b9,10.1186/2046-4053-2-36,"Background: Systematic reviews provide a synthesis of evidence for practitioners, for clinical practice guideline developers, and for those designing and justifying primary research. Having an up-to-date and comprehensive review is therefore important. Our main objective was to determine the recency of systematic reviews at the time of their publication, as measured by the time from last search date to publication. We also wanted to study the time from search date to acceptance, and from acceptance to publication, and measure the proportion of systematic reviews with recorded information on search dates and information sources in the abstract and full text of the review. Methods: A descriptive analysis of published systematic reviews indexed in Medline in 2009, 2010 and 2011 by three reviewers, independently extracting data. Results: Of the 300 systematic reviews included, 271 (90%) provided the date of search in the full-text article, but only 141 (47%) stated this in the abstract. The median (standard error; minimum to maximum) survival time from last search to acceptance was 5.1 (0.58; 0 to 43.8) months (95% confidence interval = 3.9 to 6.2) and from last search to first publication time was 8.0 (0.35; 0 to 46.7) months (95% confidence interval = 7.3 to 8.7), respectively. Of the 300 reviews, 295 (98%) stated which databases had been searched, but only 181 (60%) stated the databases in the abstract. Most researchers searched three (35%) or four (21%) databases. The top-three most used databases were MEDLINE (79%), Cochrane library (76%), and EMBASE (64%). Conclusions: Being able to identify comprehensive, up-to-date reviews is important to clinicians, guideline groups, and those designing clinical trials. This study demonstrates that some reviews have a considerable delay between search and publication, but only 47% of systematic review abstracts stated the last search date and 60% stated the databases that had been searched. Improvements in the quality of abstracts of systematic reviews and ways to shorten the review and revision processes to make review publication more rapid are needed."
33,3a83c31a6b533e1e8a5e2aadba75f1f79aa3b834,10.1002/9781119536604,No DOI detected
35,3df3143323a84e4f32b8759ea1fd1b46f03cb6b8,10.1016/j.jbi.2013.11.002,"A study's lifecycle is design, execution, reporting, interpretation, application.The Ontology of Clinical Research (OCRe) models the study design of human studies.OCRe is a generic model applicable to multiple applications and clinical domains.ERGO Annotation represents the essential meaning of eligibility criteria.OCRe supports the entire study lifecycle and the science of clinical research. To date, the scientific process for generating, interpreting, and applying knowledge has received less informatics attention than operational processes for conducting clinical studies. The activities of these scientific processes - the science of clinical research - are centered on the study protocol, which is the abstract representation of the scientific design of a clinical study. The Ontology of Clinical Research (OCRe) is an OWL 2 model of the entities and relationships of study design protocols for the purpose of computationally supporting the design and analysis of human studies. OCRe's modeling is independent of any specific study design or clinical domain. It includes a study design typology and a specialized module called ERGO Annotation for capturing the meaning of eligibility criteria. In this paper, we describe the key informatics use cases of each phase of a study's scientific lifecycle, present OCRe and the principles behind its modeling, and describe applications of OCRe and associated technologies to a range of clinical research use cases. OCRe captures the central semantics that underlies the scientific processes of clinical research and can serve as an informatics foundation for supporting the entire range of knowledge activities that constitute the science of clinical research."
37,421df3f66524010228cff4197dda8ee4ff7d97c1,,No DOI detected
38,44a381e34fcfd2c1a733a8dfb9cbd58bd553d40e,10.1186/1471-2288-11-92,"Evidence mapping describes the quantity, design and characteristics of research in broad topic areas, in contrast to systematic reviews, which usually address narrowly-focused research questions. The breadth of evidence mapping helps to identify evidence gaps, and may guide future research efforts. The Global Evidence Mapping (GEM) Initiative was established in 2007 to create evidence maps providing an overview of existing research in Traumatic Brain Injury (TBI) and Spinal Cord Injury (SCI). The GEM evidence mapping method involved three core tasks: 1. Setting the boundaries and context of the map: Definitions for the fields of TBI and SCI were clarified, the prehospital, acute inhospital and rehabilitation phases of care were delineated and relevant stakeholders (patients, carers, clinicians, researchers and policymakers) who could contribute to the mapping were identified. Researchable clinical questions were developed through consultation with key stakeholders and a broad literature search. 2. Searching for and selection of relevant studies: Evidence search and selection involved development of specific search strategies, development of inclusion and exclusion criteria, searching of relevant databases and independent screening and selection by two researchers. 3. Reporting on yield and study characteristics: Data extraction was performed at two levels - 'interventions and study design' and 'detailed study characteristics'. The evidence map and commentary reflected the depth of data extraction. One hundred and twenty-nine researchable clinical questions in TBI and SCI were identified. These questions were then prioritised into high (n = 60) and low (n = 69) importance by the stakeholders involved in question development. Since 2007, 58 263 abstracts have been screened, 3 731 full text articles have been reviewed and 1 644 relevant neurotrauma publications have been mapped, covering fifty-three high priority questions. GEM Initiative evidence maps have a broad range of potential end-users including funding agencies, researchers and clinicians. Evidence mapping is at least as resource-intensive as systematic reviewing. The GEM Initiative has made advancements in evidence mapping, most notably in the area of question development and prioritisation. Evidence mapping complements other review methods for describing existing research, informing future research efforts, and addressing evidence gaps."
43,4b7f0e1ef18ba9e47d8160a04ba358ce52e0e750,10.1016/S0140-6736(10)60903-8,No abstract found
49,54f0de18f8690cbceee9ac08a8da3d8ac1464bd5,10.1016/j.jpain.2013.06.005,No abstract found
50,56183cfbacdb8374207ef0ef50cf8205055e3873,10.1186/1472-6947-8-48,"Background
This paper proposes the use of decision trees as the basis for automatically extracting information from published randomized controlled trial (RCT) reports. An exploratory analysis of RCT abstracts is undertaken to investigate the feasibility of using decision trees as a semantic structure. Quality-of-paper measures are also examined."
51,574a3188958bbdaf652cbaa28c5e5d06916d5b43,,No DOI detected
54,5d24ad9e1222085740d3cd12c48ff6ac171b814b,10.1186/1471-2105-11-55,"Systematic reviews address a specific clinical question by unbiasedly assessing and analyzing the pertinent literature. Citation screening is a time-consuming and critical step in systematic reviews. Typically, reviewers must evaluate thousands of citations to identify articles eligible for a given review. We explore the application of machine learning techniques to semi-automate citation screening, thereby reducing the reviewers' workload. We present a novel online classification strategy for citation screening to automatically discriminate relevant from irrelevant citations. We use an ensemble of Support Vector Machines (SVMs) built over different feature-spaces (e.g., abstract and title text), and trained interactively by the reviewer(s). Semi-automating the citation screening process is difficult because any such strategy must identify all citations eligible for the systematic review. This requirement is made harder still due to class imbalance; there are far fewer relevant than irrelevant citations for any given systematic review. To address these challenges we employ a custom active-learning strategy developed specifically for imbalanced datasets. Further, we introduce a novel undersampling technique. We provide experimental results over three real-world systematic review datasets, and demonstrate that our algorithm is able to reduce the number of citations that must be screened manually by nearly half in two of these, and by around 40% in the third, without excluding any of the citations eligible for the systematic review. We have developed a semi-automated citation screening algorithm for systematic reviews that has the potential to substantially reduce the number of citations reviewers have to manually screen, without compromising the quality and comprehensiveness of the review."
60,683c908f0643834049e61e4d1f3ffcbf8818f46c,10.1186/1471-2288-6-31,"Systematic reviews and meta-analyses of test accuracy studies are increasingly being recognised as central in guiding clinical practice. However, there is currently no dedicated and comprehensive software for meta-analysis of diagnostic data. In this article, we present Meta-DiSc, a Windows-based, user-friendly, freely available (for academic use) software that we have developed, piloted, and validated to perform diagnostic meta-analysis. Meta-DiSc a) allows exploration of heterogeneity, with a variety of statistics including chi-square, I-squared and Spearman correlation tests, b) implements meta-regression techniques to explore the relationships between study characteristics and accuracy estimates, c) performs statistical pooling of sensitivities, specificities, likelihood ratios and diagnostic odds ratios using fixed and random effects models, both overall and in subgroups and d) produces high quality figures, including forest plots and summary receiver operating characteristic curves that can be exported for use in manuscripts for publication. All computational algorithms have been validated through comparison with different statistical tools and published meta-analyses. Meta-DiSc has a Graphical User Interface with roll-down menus, dialog boxes, and online help facilities. Meta-DiSc is a comprehensive and dedicated test accuracy meta-analysis software. It has already been used and cited in several meta-analyses published in high-ranking journals. The software is publicly available at http://www.hrc.es/investigacion/metadisc_en.htm
 ."
63,6b0ad2fb13d3136914f635803e87bfe9e232e1ea,10.1186/1741-7015-2-23,"Clinical end users of MEDLINE have a difficult time retrieving articles that are both scientifically sound and directly relevant to clinical practice. Search filters have been developed to assist end users in increasing the success of their searches. Many filters have been developed for the literature on therapy and reviews but little has been done in the area of prognosis. The objective of this study is to determine how well various methodologic textwords, Medical Subject Headings, and their Boolean combinations retrieve methodologically sound literature on the prognosis of health disorders in MEDLINE. An analytic survey was conducted, comparing hand searches of journals with retrievals from MEDLINE for candidate search terms and combinations. Six research assistants read all issues of 161 journals for the publishing year 2000. All articles were rated using purpose and quality indicators and categorized into clinically relevant original studies, review articles, general papers, or case reports. The original and review articles were then categorized as 'pass' or 'fail' for methodologic rigor in the areas of prognosis and other clinical topics. Candidate search strategies were developed for prognosis and run in MEDLINE – the retrievals being compared with the hand search data. The sensitivity, specificity, precision, and accuracy of the search strategies were calculated. 12% of studies classified as prognosis met basic criteria for scientific merit for testing clinical applications. Combinations of terms reached peak sensitivities of 90%. Compared with the best single term, multiple terms increased sensitivity for sound studies by 25.2% (absolute increase), and increased specificity, but by a much smaller amount (1.1%) when sensitivity was maximized. Combining terms to optimize both sensitivity and specificity achieved sensitivities and specificities of approximately 83% for each. Empirically derived search strategies combining indexing terms and textwords can achieve high sensitivity and specificity for retrieving sound prognostic studies from MEDLINE."
79,8cfb167b1d40b0db72e39921d6d049b695422ecf,10.1186/2047-2501-2-1,"Background
Individuals and groups who write systematic reviews and meta-analyses in evidence-based medicine regularly carry out literature searches across multiple search engines linked to different bibliographic databases, and thus have an urgent need for a suitable metasearch engine to save time spent on repeated searches and to remove duplicate publications from initial consideration. Unlike general users who generally carry out searches to find a few highly relevant (or highly recent) articles, systematic reviewers seek to obtain a comprehensive set of articles on a given topic, satisfying specific criteria. This creates special requirements and challenges for metasearch engine design and implementation."
81,91d92735e11cb48f63b94f4750df1202d75b59bf,10.1007/b98634,No abstract found
82,92e5d42fedb1f2bc012a2b6bc4baac7610951b98,10.1016/j.jclinepi.2013.11.015,"Abstract Background and Objectives Reports of randomized controlled trials (RCTs) should set findings within the context of previous research. The resulting network of citations would also provide an alternative search method for clinicians, researchers, and systematic reviewers seeking to base decisions on all available evidence. We sought to determine the connectedness of citation networks of RCTs by examining direct (referenced trials) and indirect (through references of referenced trials, etc) citation of trials to one another. Methods Meta-analyses were used to create citation networks of RCTs addressing the same clinical questions. The primary measure was the proportion of networks where following citation links between RCTs identifies the complete set of RCTs, forming a single connected citation group. Other measures included the number of disconnected groups (islands) within each network, the number of citations in the network relative to the maximum possible, and the maximum number of links in the path between two connected trials (a measure of indirectness of citations). Results We included 259 meta-analyses with a total of 2,413 and a median of seven RCTs each. For 46% (118 of 259) of networks, the RCTs formed a single connected citation group—one island. For the other 54% of networks, where at least one RCT group was not cited by others, 39% had two citation islands and 4% (10 of 257) had 10 or more islands. On average, the citation networks had 38% of the possible citations to other trials (if each trial had cited all earlier trials). The number of citation islands and the maximum number of citation links increased with increasing numbers of trials in the network. Conclusion Available evidence to answer a clinical question may be identified by using network citations created with a small initial corpus of eligible trials. However, the number of islands means that citation networks cannot be relied on for evidence retrieval."
83,930926b14fe4de8a7adcf94772e19a4a683de9dd,10.1186/1472-6947-12-33,"Systematic Reviews (SRs) are an essential part of evidence-based medicine, providing support for clinical practice and policy on a wide range of medical topics. However, producing SRs is resource-intensive, and progress in the research they review leads to SRs becoming outdated, requiring updates. Although the question of how and when to update SRs has been studied, the best method for determining when to update is still unclear, necessitating further research. In this work we study the potential impact of a machine learning-based automated system for providing alerts when new publications become available within an SR topic. Some of these new publications are especially important, as they report findings that are more likely to initiate a review update. To this end, we have designed a classification algorithm to identify articles that are likely to be included in an SR update, along with an annotation scheme designed to identify the most important publications in a topic area. Using an SR database containing over 70,000 articles, we annotated articles from 9 topics that had received an update during the study period. The algorithm was then evaluated in terms of the overall correct and incorrect alert rate for publications meeting the topic inclusion criteria, as well as in terms of its ability to identify important, update-motivating publications in a topic area. Our initial approach, based on our previous work in topic-specific SR publication classification, identifies over 70% of the most important new publications, while maintaining a low overall alert rate. We performed an initial analysis of the opportunities and challenges in aiding the SR update planning process with an informatics-based machine learning approach. Alerts could be a useful tool in the planning, scheduling, and allocation of resources for SR updates, providing an improvement in timeliness and coverage for the large number of medical topics needing SRs. While the performance of this initial method is not perfect, it could be a useful supplement to current approaches to scheduling an SR update. Approaches specifically targeting the types of important publications identified by this work are likely to improve results."
86,97f6cecf7656de38fe66f7d913055a2bcb18106c,10.1007/978-3-642-32541-0_14,"The World Wide Web contains a massive amount of information in unstructured natural language and obtaining valuable information from informally written Web documents is a major research challenge. One research focus is Open Information Extraction (OIE) aimed at developing relation-independent information extraction. Open Information Extraction systems seek to extract all potential relations from the text rather than extracting a few pre-defined relations. Existing Open Information Extraction systems have mainly focused on Web's heterogeneity rather than the Web's informality. The performance of the REVERB system, a state-of-the-art OIE system, drops dramatically as informality increases in Web documents.

This paper proposes a Hybrid Ripple-Down Rules based Open Information Extraction (Hybrid RDROIE) system, which uses RDR on top of a conventional OIE system. The Hybrid RDROIE system applies RDR's incremental learning technique as an add-on to the state-of-the-art REVERB OIE system to correct the performance degradation of REVERB due to the Web's informality in a domain of interest. With this wrapper approach, the baseline performance is that of the REVERB system with RDR correcting errors in a domain of interest. The Hybrid RDROIE system doubled REVERB's performance in a domain of interest after two hours training."
88,9ef5ff25faf05ef6d96a1ed54fc3117aa8c1f119,10.1007/s00125-011-2325-z,"Aims/hypothesis
The renal and cardiovascular protective effects of angiotensin receptor blocker (ARB) remain controversial in type 2 diabetic patients treated with a contemporary regimen including an angiotensin converting enzyme inhibitor (ACEI)."
90,a0f5233ac414bd0674a74fc1e1f412f30abfe45c,10.1007/978-3-642-39314-3,No DOI detected
94,a54bd18e398b2a5469e5821cf70000d6e5be99a6,10.1186/1471-2288-6-4,"PubMed is the most widely used method for searches of the medical literature, but fails to identify many relevant articles. Electronic citation tracking offers an alternative search method. Articles investigating the role of depression in the aetiology and prognosis of coronary heart disease were sought through two methods: a) PubMed, and b) citation tracking where Science Citation Index was searched for all articles which cited (forward citation tracking) or were cited by (backward citation tracking) any of the articles in an index review. The number and quality of eligible articles identified by the two methods were compared. 50 articles that were not already included in the index review met our inclusion criteria; 11 were identified through Science Citation Index alone, 8 through PubMed alone, and 31 through both methods. Articles identified by Science Citation Index alone were published in higher impact factor journals, were larger and were less likely to show a positive association. Science Citation Index identified more eligible articles than PubMed, and these differed qualitatively. Failing to use citation tracking in a systematic review of observational studies may result in bias."
95,a722a23ce0f4960d65e95c61466cfcfb8897eeec,10.1186/2046-4053-2-97,"Background
Google Translate offers free Web-based translation, but it is unknown whether its translation accuracy is sufficient to use in systematic reviews to mitigate concerns about language bias."
96,a80b29a0c5229e518206b9b5683487f0d9d41e20,10.1186/1471-2105-12-S2-S5,"Given a set of pre-defined medical categories used in Evidence Based Medicine, we aim to automatically annotate sentences in medical abstracts with these labels. We constructed a corpus of 1,000 medical abstracts annotated by hand with specified medical categories (e.g. Intervention, Outcome). We explored the use of various features based on lexical, semantic, structural, and sequential information in the data, using Conditional Random Fields (CRF) for classification. For the classification tasks over all labels, our systems achieved micro-averaged f-scores of 80.9% and 66.9% over datasets of structured and unstructured abstracts respectively, using sequential features. In labeling only the key sentences, our systems produced f-scores of 89.3% and 74.0% over structured and unstructured abstracts respectively, using the same sequential features. The results over an external dataset were lower (f-scores of 63.1% for all labels, and 83.8% for key sentences). Of the features we used, the best for classifying any given sentence in an abstract were based on unigrams, section headings, and sequential information from preceding sentences. These features resulted in improved performance over a simple bag-of-words approach, and outperformed feature sets used in previous work."
99,ac931d2cd120305f61af7967f9f2ce7f61a8e0c9,10.1016/j.eswa.2013.08.047,"Medical systematic reviews answer particular questions within a very specific domain of expertise by selecting and analysing the current pertinent literature. As part of this process, the phase of screening articles usually requires a long time and significant effort as it involves a group of domain experts evaluating thousands of articles in order to find the relevant instances. Our goal is to support this process through automatic tools. There is a recent trend of applying text classification methods to semi-automate the screening phase by providing decision support to the group of experts, hence helping reduce the required time and effort. In this work, we contribute to this line of work by performing a comprehensive set of text classification experiments on a corpus resulting from an actual systematic review in the area of Internet-Based Randomised Controlled Trials. These experiments involved applying multiple machine learning algorithms combined with several feature selection techniques to different parts of the articles (i.e., titles, abstract, or both). Results are generally positive in terms of overall precision and recall measurements, reaching values of up to 84%. It is also revealing in terms of how using only article titles provides virtually as good results as when adding article abstracts. Based on the positive results, it is clear that text classification can support the screening stage of medical systematic reviews . However, selecting the most appropriate machine learning algorithms, related methods, and text sections of articles is a neglected but important requirement because of its significant impact to the end results."
102,2416ca1e27dd384f70e48f19879c765e322a5abd,10.1007/978-3-030-64573-1_43,"There are three pillars of evidence-based medicine (EBM): the evidence itself (e.g., data from clinical studies), clinical expertise, and patient values. EBM is therefore a systematic approach to decision-making that integrates these three inputs. It involves evidence production (design and conducting of clinical studies), synthesis (collecting, appraising, and combining data to answer clinical questions), implementation (e.g., through clinical practice guidelines based on these syntheses), and evaluation (monitoring the quality of care, including adherence to evidence-based recommendations).EBM faces many challenges that artificial intelligence can help solve. It can help detect research gaps and avoid funding redundant studies. It can expedite the evidence synthesis process, which currently is slow and costly, leading to outdated and incomplete evidence being used in the decision-making processes. AI can help engage patients and elicit values (e.g., chatbot-based decision aids) as well as provide coordinated care for patients with multimorbidities.However, improperly implementing AI can also exacerbate problems in EBM. For instance, if AI-enabled decision support systems fail to incorporate patient values, a return to a model of medicine characterized by low patient autonomy is possible – only this time with a computer in charge."
103,3028db25486f28a38b8d35c019071b4bbfcea1af,10.1186/s13326-022-00270-8,No DOI detected
105,3e03e8cf0fd49ebfd7947fcf3e9cd4dd058e4676,10.1007/s10664-021-10084-4,No DOI detected
107,59ce45a106dd1edf15b2bd972d7cd7d361976029,10.1016/j.knosys.2022.109266,No DOI detected
110,898a66694e6898a4278405702e10647c2e3e8879,10.1016/j.heliyon.2022.e09095,"<h2>Abstract</h2><h3>Background</h3> Environmental health and other researchers can benefit from automated or semi-automated summaries of data within published studies as summarizing study methods and results is time and resource intensive. Automated summaries can be designed to identify and extract details of interest pertaining to the study design, population, testing agent/intervention, or outcome (etc.). Much of the data reported across existing publications lack unified structure, standardization and machine-readable formats or may be presented in complex tables which serve as barriers that impede the development of automated data extraction methodologies. As full automation of data extraction seems unlikely soon, encouraging investigators to submit structured summaries of methods and results in standardized formats with meta-data tagging of content may be of value during the publication process. This would produce machine-readable content to facilitate automated data extraction, establish sharable data repositories, help make research data FAIR, and could improve reporting quality. <h3>Objectives</h3> A pilot study was conducted to assess the feasibility of asking participants to summarize study methods and results using a structured, web-based data extraction model as a potential workflow that could be implemented during the manuscript submission process. <h3>Methods</h3> Eight participants entered study details and data into the Health Assessment Workplace Collaborative (HAWC). Participants were surveyed after the extraction exercise to ascertain 1) whether this extraction exercise will impact their conducting and reporting of future research, 2) the ease of data extraction, including which fields were easiest and relatively more problematic to extract and 3) the amount of time taken to perform data extractions and other related tasks. Investigators then presented participants the potential benefits of providing structured data in the format they were extracting. After this, participants were surveyed about 1) their willingness to provide structured data during the publication process and 2) whether they felt the potential application of structured data entry approaches and their implementation during the journal submission process should continue to be further explored. <h3>Conclusions</h3> Routine provision of structured data that summarizes key information from research studies could reduce the amount of effort required for reusing that data in the future, such as in systematic reviews or agency scientific assessments. Our pilot study suggests that directly asking authors to provide that data, via structured templates, may be a viable approach to achieving this: participants were willing to do so, and the overall process was not prohibitively arduous. We also found some support for the hypothesis that use of study templates may have halo benefits in improving the conduct and completeness of reporting of future research. While limitations in the generalizability of our findings mean that the conditions of success of templates cannot be assumed, further research into how such templates might be designed and implemented does seem to have enough chance of success that it ought to be undertaken."
111,9250bd18d653cec89d98c764e7736fe174e2b5c0,10.1007/978-3-030-99736-6_39,"In the process of Systematic Literature Review, citation screening is estimated to be one of the most time-consuming steps. Multiple approaches to automate it using various machine learning techniques have been proposed. The first research papers that apply deep neural networks to this problem were published in the last two years. In this work, we conduct a replicability study of the first two deep learning papers for citation screening [8, 16] and evaluate their performance on 23 publicly available datasets. While we succeeded in replicating the results of one of the papers, we were unable to replicate the results of the other. We summarise the challenges involved in the replication, including difficulties in obtaining the datasets to match the experimental setup of the original papers and problems with executing the original source code. Motivated by this experience, we subsequently present a simpler model based on averaging word embeddings that outperforms one of the models on 18 out of 23 datasets and is, on average, 72 times faster than the second replicated approach. Finally, we measure the training time and the invariance of the models when exposed to a variety of input features and random initialisations, demonstrating differences in the robustness of these approaches.KeywordsCitation screeningStudy selectionSystematic literature review (SLR)Document retrievalReplicability"
112,9e05f446d866e5da794efa8fb64d7a2cb75e31ba,10.1016/j.iswa.2022.200091,No DOI detected
115,b2431ccff95909db6d40a266c4461bfc6d2d7ec1,10.1016/j.jclinepi.2022.04.027,"A rapidly developing scenario like a pandemic requires the prompt production of high-quality systematic reviews, which can be automated using artificial intelligence (AI) techniques. We evaluated the application of AI tools in COVID-19 evidence syntheses.After prospective registration of the review protocol, we automated the download of all open-access COVID-19 systematic reviews in the COVID-19 Living Overview of Evidence database, indexed them for AI-related keywords, and located those that used AI tools. We compared their journals' JCR Impact Factor, citations per month, screening workloads, completion times (from pre-registration to preprint or submission to a journal) and AMSTAR-2 methodology assessments (maximum score 13 points) with a set of publication date matched control reviews without AI.Of the 3999 COVID-19 reviews, 28 (0.7%, 95% CI 0.47-1.03%) made use of AI. On average, compared to controls (n=64), AI reviews were published in journals with higher Impact Factors (median 8.9 vs 3.5, p<0.001), and screened more abstracts per author (302.2 vs 140.3, p=0.009) and per included study (189.0 vs 365.8, p<0.001) while inspecting less full texts per author (5.3 vs 14.0, p=0.005). No differences were found in citation counts (0.5 vs 0.6, p=0.600), inspected full texts per included study (3.8 vs 3.4, p=0.481), completion times (74.0 vs 123.0, p=0.205) or AMSTAR-2 (7.5 vs 6.3, p=0.119).AI was an underutilized tool in COVID-19 systematic reviews. Its usage, compared to reviews without AI, was associated with more efficient screening of literature and higher publication impact. There is scope for the application of AI in automating systematic reviews."
117,d28a8909ae606698a935229a47f9773e1668ae06,10.1080/17565529.2022.2062284,No abstract found
119,05491e1b16bfcb68022cde37442ffde6a5a3ccfd,10.1016/J.INFSOF.2021.106589,"Abstract Context Systematic Literature Review (SLR) studies aim to identify relevant primary papers, extract the required data, analyze, and synthesize results to gain further and broader insight into the investigated domain. Multiple SLR studies have been conducted in several domains, such as software engineering, medicine, and pharmacy. Conducting an SLR is a time-consuming, laborious, and costly effort. As such, several researchers developed different techniques to automate the SLR process. However, a systematic overview of the current state-of-the-art in SLR automation seems to be lacking. Objective This study aims to collect and synthesize the studies that focus on the automation of SLR to pave the way for further research. Method A systematic literature review is conducted on published primary studies on the automation of SLR studies, in which 41 primary studies have been analyzed. Results This SLR identifies the objectives of automation studies, application domains, automated steps of the SLR, automation techniques, and challenges and solution directions. Conclusion According to our study, the leading automated step is the Selection of Primary Studies. Although many studies have provided automation approaches for systematic literature reviews, no study has been found to apply automation techniques in the planning and reporting phase. Further research is needed to support the automation of the other activities of the SLR process."
123,2be4020e524f4028063077f2ad8bc19612fcfe66,10.1186/s12970-021-00440-6,"Background To achieve ideal strength/power to mass ratio, athletes may attempt to lower body mass through reductions in fat mass (FM), while maintaining or increasing fat-free mass (FFM) by manipulating their training regimens and diets. Emerging evidence suggests that consumption of high-fat, ketogenic diets (KD) may be advantageous for reducing body mass and FM, while retaining FFM. Methods A systematic review of the literature was conducted using PubMed and Cochrane Library databases to compare the effects of KD versus control diets (CON) on body mass and composition in physically active populations. Randomized and non-randomized studies were included if participants were healthy (free of chronic disease), physically active men or women age ≥ 18 years consuming KD ( 0.5 mmol/L) for ≥14 days. Results Thirteen studies (9 parallel and 4 crossover/longitudinal) that met the inclusion criteria were identified. Aggregated results from the 13 identified studies show body mass decreased 2.7 kg in KD and increased 0.3 kg in CON. FM decreased by 2.3 kg in KD and 0.3 kg in CON. FFM decreased by 0.3 kg in KD and increased 0.7 kg in CON. Estimated energy balance based on changes in body composition was - 339 kcal/d in KD and 5 kcal/d in CON. Risk of bias identified some concern of bias primarily due to studies which allowed participants to self-select diet intervention groups, as well as inability to blind participants to the study intervention, and/or longitudinal study design. Conclusion KD can promote mobilization of fat stores to reduce FM while retaining FFM. However, there is variance in results of FFM across studies and some risk-of-bias in the current literature that is discussed in this systematic review."
125,3d717187cd1af55575c38b3a985cca6155a5673a,10.1016/j.jclinepi.2021.05.019,"Abstract Objective We aimed to map the resource use during systematic review (SR) production and reasons why steps of the SR production are resource intensive to discover where the largest gain in improving efficiency might be possible. Study design and setting We conducted a scoping review. An information specialist searched multiple databases (e.g., Ovid MEDLINE, Scopus) and implemented citation-based and grey literature searching. We employed dual and independent screenings of records at the title/abstract and full-text levels and data extraction. Results We included 34 studies. Thirty-two reported on the resource use—mostly time; four described reasons why steps of the review process are resource intensive. Study selection, data extraction, and critical appraisal seem to be very resource intensive, while protocol development, literature search, or study retrieval take less time. Project management and administration required a large proportion of SR production time. Lack of experience, domain knowledge, use of collaborative and SR-tailored software, and good communication and management can be reasons why SR steps are resource intensive. Conclusion Resource use during SR production varies widely. Areas with the largest resource use are administration and project management, study selection, data extraction, and critical appraisal of studies."
126,3e56ed7ebb6378c414d1c27d171a3e42f0e53cb4,10.1186/s13643-021-01640-6,"Systematic Reviews (SR), studies of studies, use a formal process to evaluate the quality of scientific literature and determine ensuing effectiveness from qualifying articles to establish consensus findings around a hypothesis. Their value is increasing as the conduct and publication of research and evaluation has expanded and the process of identifying key insights becomes more time consuming. Text analytics and machine learning (ML) techniques may help overcome this problem of scale while still maintaining the level of rigor expected of SRs. In this article, we discuss an approach that uses existing examples of SRs to build and test a method for assisting the SR title and abstract pre-screening by reducing the initial pool of potential articles down to articles that meet inclusion criteria. Our approach differs from previous approaches to using ML as a SR tool in that it incorporates ML configurations guided by previously conducted SRs, and human confirmation on ML predictions of relevant articles during multiple iterative reviews on smaller tranches of citations. We applied the tailored method to a new SR review effort to validate performance. The case study test of the approach proved a sensitivity (recall) in finding relevant articles during down selection that may rival many traditional processes and show ability to overcome most type II errors. The study achieved a sensitivity of 99.5% (213 out of 214) of total relevant articles while only conducting a human review of 31% of total articles available for review. We believe this iterative method can help overcome bias in initial ML model training by having humans reinforce ML models with new and relevant information, and is an applied step towards transfer learning for ML in SR."
128,45eb7d939f4f8b1f675863508b1f49806b685006,,No DOI detected
129,4c8828ded8acf0293661f9466383bd8a7826f92a,10.1007/978-3-030-71881-7_12,"In this chapter, we will explore how Systemic Reviews (SR) are traditionally conducted and how the process of arriving at a valuable SR can be made more efficient and less error prone using Machine Learning (ML) techniques. As the integration of ML at the screening stage of SRs has reached the highest level of maturity, we will explain the techniques utilized. We will further describe the extraction process from primary studies supported by ML techniques. The discussion of pitfalls when conducting SRs concludes the chapter, specifically how ML can address bias. Lastly, we address the inherent limitations of artificial intelligence in healthcare with a special emphasis on ML for the use in SRs."
134,7a663ac458339ff2e9685bc96aab9f77326c5276,10.1007/s10791-020-09381-1,"Systematic reviews are comprehensive literature reviews that target a highly focused research question. In the medical domain, complex Boolean queries are used to identify studies. To ensure comprehensiveness, all studies retrieved are screened for inclusion or exclusion in the review. Developing Boolean queries for this task requires the expertise of trained information specialists. However, even for these expert searchers, query formulation can be difficult and lengthy: especially when dealing with areas of medicine that they may not be knowledgeable about. To this end, two computational adaptations of methods information specialists use to formulate Boolean queries have been proposed in prior work. These adaptations can be used to assist information specialists by providing a good starting point for query development. However, a number of limitations with these computational methods have been raised, and a comparison between them has not been made. In this study, we address the limitations of previous work and evaluate the two. We found that, between the two computational adaptions, the objective method is more effective than the conceptual method for query formulation alone, however, the conceptual method provides a better starting point for manual query refinement. This work helps to inform those building search tools that assist with systematic review construction."
135,7c9035677cfba66ddb96a8fe7f7332d64885bb75,10.1016/J.EMJ.2021.01.002,"Abstract The past decade has been marked by concerns regarding the replicability and reproducibility of published research in the social sciences. Publicized failures to replicate landmark studies, along with high-profile cases of research fraud, have led scholars to reconsider the trustworthiness of both findings and institutionalized research practices. This paper considers two questions: (1) Relative to psychology and economics, what is the state of replication and reproduction research in management? (2) Are the disciplines equally advanced in the use of methods applied to study the replication problem? A systematic literature review identified 67 studies pertinent to these questions. The results indicate that the replication prevalence rate in management studies lies almost exactly between those of psychology and economics, while a high level of variation between management and other business-related disciplines can be noted. Further, similarly to psychology, but unlike economics, the surveys of published replications tend to report high replication success rates for management and other business-related disciplines. However, a comparison with recently obtained results in preregistered multi-study replications in psychology and economics suggests that these rates are almost certainly inflated. Method and data transparency are medium to low, often rendering attempts to reproduce or replicate studies impossible. Finally, the understanding of the replicability problem in management is held back by the underutilization of methods developed in other disciplines. The review also reveals that management, psychology, and economics exhibit strikingly different practices and approaches to replication, despite facing similar incentive structures. Disciplines in which replication and reproduction attempts are rare and which frequently involve authors of the original study in replication attempts lack strong deterrents against questionable research practices; thus, they are less likely to deliver replicable results."
140,898893dab1fa83d6db20acdb216877c8c4aa5c6d,10.1016/j.pnpbp.2020.110189,"Abstract Background Dementia is a chronic syndrome characterized by cognitive and behavioral symptoms, which may include short-term memory impairment and problems related to orientation, language, attention and perception. Although cognitive impairment (CI) is increasingly considered the main geriatric condition predisposing to dementia, its early management could still promote symptomatic relief and delay disease progression. Recently, probiotics treatment has been studied as a potential new therapeutic approach to attenuate dementia-related decline and mild cognitive impairment (MCI). Therefore, we conducted a systematic review and meta-analysis to review and analyse the available evidence on the effect of probiotics on MCI and dementia. Methods A systematic search and meta-analysis were performed on Cochrane Library, ProQuest, Web of Science, PubMed-Medline, The Cumulative Index to Nursing and Allied Health Literature (CINAHL), Scopus, ScienceDirect and Open Grey. Search terms included diagnoses of interest (dementia and MCI) and the intervention of interest (probiotic, lactobacillus and bifidobacterium). Original articles reporting the use of probiotics supplementation for the treatment of dementia and MCI were screened and studied independently by two researchers. After that, a random and fixed effects model was used at the meta-analysis stage of the results to determine its effect size. Results A total of 16 articles (10 preclinical and 6 clinical) that met the inclusion criteria for the systematic review, and 15 articles (10 preclinical and 5 clinical) for meta-analysis were finally included. In humans, the administration of probiotics improved general cognitive function after the treatment period. Similarly, an improvement in memory and spatial/non-spatial learning was identified in the probiotic group of animals compared to the control group. On the other hand, the results showed an increase in the levels of the brain-derived neurotrophic factor, an improvement in the inflammatory profile and regulation of cellular biomarkers after probiotics administration. Conclusion Probiotics supplementation could be an adequate therapeutic strategy both in dementia and CI based on clinical and preclinical evidence. However, it is therefore important to translate preclinical data into clinical data where the evidence is more limited."
142,9136c99cbb5a67bc5639a57c078f837f7392b936,10.1002/MAR.21491,No abstract found
143,9bb0505df7fb6e749b57ee2dc2d8acc4b204506b,10.1002/cl2.1194,No abstract found
144,9f313f31906a45c8f6228c21153f73bdbf55fd4c,10.1186/s12874-021-01354-2,"Machine learning tools that semi-automate data extraction may create efficiencies in systematic review production. We evaluated a machine learning and text mining tool’s ability to (a) automatically extract data elements from randomized trials, and (b) save time compared with manual extraction and verification. For 75 randomized trials, we manually extracted and verified data for 21 data elements. We uploaded the randomized trials to an online machine learning and text mining tool, and quantified performance by evaluating its ability to identify the reporting of data elements (reported or not reported), and the relevance of the extracted sentences, fragments, and overall solutions. For each randomized trial, we measured the time to complete manual extraction and verification, and to review and amend the data extracted by the tool. We calculated the median (interquartile range [IQR]) time for manual and semi-automated data extraction, and overall time savings. The tool identified the reporting (reported or not reported) of data elements with median (IQR) 91% (75% to 99%) accuracy. Among the top five sentences for each data element at least one sentence was relevant in a median (IQR) 88% (83% to 99%) of cases. Among a median (IQR) 90% (86% to 97%) of relevant sentences, pertinent fragments had been highlighted by the tool; exact matches were unreliable (median (IQR) 52% [33% to 73%]). A median 48% of solutions were fully correct, but performance varied greatly across data elements (IQR 21% to 71%). Using ExaCT to assist the first reviewer resulted in a modest time savings compared with manual extraction by a single reviewer (17.9 vs. 21.6 h total extraction time across 75 randomized trials). Using ExaCT to assist with data extraction resulted in modest gains in efficiency compared with manual extraction. The tool was reliable for identifying the reporting of most data elements. The tool’s ability to identify at least one relevant sentence and highlight pertinent fragments was generally good, but changes to sentence selection and/or highlighting were often required. https://doi.org/10.7939/DVN/RQPJKS"
147,aeee00b62e2ce567163dad63e88193b19055582b,10.1016/J.ESWA.2021.115261,"Abstract The systematic literature review (SLR) process includes several steps to collect secondary data and analyze it to answer research questions. In this context, the document retrieval and primary study selection steps are heavily intertwined and known for their repetitiveness, high human workload, and difficulty identifying all relevant literature. This study aims to reduce human workload and error of the document retrieval and primary study selection processes using a decision support system (DSS). An open-source DSS is proposed that supports the document retrieval step, dataset preprocessing, and citation classification. The DSS is domain-independent, as it has proven to carefully select an article’s relevance based solely on the title and abstract. These features can be consistently retrieved from scientific database APIs. Additionally, the DSS is designed to run in the cloud without any required programming knowledge for reviewers. A Multi-Channel CNN architecture is implemented to support the citation screening process. With the provided DSS, reviewers can fill in their search strategy and manually label only a subset of the citations. The remaining unlabeled citations are automatically classified and sorted based on probability. It was shown that for four out of five review datasets, the DSS's use achieved significant workload savings of at least 10%. The cross-validation results show that the system provides consistent results up to 88.3% of work saved during citation screening. In two cases, our model yielded a better performance over the benchmark review datasets. As such, the proposed approach can assist the development of systematic literature reviews independent of the domain. The proposed DSS is effective and can substantially decrease the document retrieval and citation screening steps' workload and error rate."
148,b17d2799fc5010d07303a4f21c61e8f8d4be9c2f,10.1186/s13643-021-01632-6,"Accepted systematic review (SR) methodology requires citation screening by two reviewers to maximise retrieval of eligible studies. We hypothesized that records could be excluded by a single reviewer without loss of sensitivity in two conditions; the record was ineligible for multiple reasons, or the record was ineligible for one or more specific reasons that could be reliably assessed. Twenty-four SRs performed at CHEO, a pediatric health care and research centre in Ottawa, Canada, were divided into derivation and validation sets. Exclusion criteria during abstract screening were sorted into 11 specific categories, with loss in sensitivity determined by individual category and by number of exclusion criteria endorsed. Five single reviewer algorithms that combined individual categories and multiple exclusion criteria were then tested on the derivation and validation sets, with success defined a priori as less than 5% loss of sensitivity. The 24 SRs included 930 eligible and 27390 ineligible citations. The reviews were mostly focused on pediatrics (70.8%, N=17/24), but covered various specialties. Using a single reviewer to exclude any citation led to an average loss of sensitivity of 8.6% (95%CI, 6.0–12.1%). Excluding citations with ≥2 exclusion criteria led to 1.2% average loss of sensitivity (95%CI, 0.5–3.1%). Five specific exclusion criteria performed with perfect sensitivity: conference abstract, ineligible age group, case report/series, not human research, and review article. In the derivation set, the five algorithms achieved a loss of sensitivity ranging from 0.0 to 1.9% and work-saved ranging from 14.8 to 39.1%. In the validation set, the loss of sensitivity for all 5 algorithms remained below 2.6%, with work-saved between 10.5% and 48.2%. Findings suggest that targeted application of single-reviewer screening, considering both type and number of exclusion criteria, could retain sensitivity and significantly decrease workload. Further research is required to investigate the potential for combining this approach with crowdsourcing or machine learning methodologies."
149,b8bc071a635088a269c0e8d35ce69135da0ce565,10.1007/978-3-030-77417-2_17,"Due to accelerated growth in the number of scientific papers, writing literature reviews has become an increasingly costly activity. Therefore, the search for computational tools to assist in this process has been gaining ground in recent years. This work presents an overview of the current scenario of development of artificial intelligence tools aimed to assist in the production of systematic literature reviews. The process of creating a literature review is both creative and technical. The technical part of this process is liable to automation. For the purpose of organization, we divide this technical part into four steps: searching, screening, extraction, and synthesis. For each of these steps, we present artificial intelligence techniques that can be useful to its realization. In addition, we also present the obstacles encountered for the application of each technique. Finally, we propose a pipeline for the automatic creation of systematic literature reviews, by combining and placing existing techniques in stages where they possess the greatest potential to be useful."
151,c1a10445aab00ac3c5a46cd9cdb1a384fd0261de,10.1186/s13643-021-01773-8,"Background Parents can be psychologically impacted when their children are diagnosed with eye diseases, such as blindness, strabismus, and eye cancer. Stress can reduce the quality of parental care and may be linked to the deterioration of parents' and children's mental and physical health and family dynamics. No systematic literature review on parental stress in ophthalmology has been found to provide evidence synthesis capable of stimulating and defining new studies and thereby promoting research in this field. To address this important gap, the present review aims to synthesize evidence about approaches, methods, instruments, and results from research regarding ophthalmology-related parental stress. Methods Primary epidemiological observational studies should be original in addressing parental stress caused by ophthalmological health conditions in children. They should present the characteristics of the study population and the clinical and ophthalmic characterizations of children. MEDLINE (via Ovid), EMBASE, PsycINFO, Google Scholar, and gray literature (PsycEXTRA, NTIS, and OpenSINGLE) will be searched. Controlled vocabulary, Boolean operators, and defined search strategies will be used. There will be no restrictions on the studies' publication language, which will be selected in two screening stages. Two reviewers will independently retrieve full-text studies, assess methodological quality, and extract data. Data available through December 2021 will be considered for inclusion. Discussion The socioeconomic characterization of the participants, the identification of which ophthalmological diseases have been studied in relation to parental stress, and the knowledge of each instrument and methodology peculiarities potentially contribute to this study. The results may promote the development or enhancement of public policies focused on this specific theme, thereby providing the means for potential improvement of the physical and mental health of parents and children with eye diseases. Systematic review registration PROSPERO CRD42018094972."
152,c79618eec309b9ae3c61c0cfc0215de89943967c,10.1186/s13643-021-01635-3,"Systematic reviews and meta-analyses provide the highest level of evidence to help inform policy and practice, yet their rigorous nature is associated with significant time and economic demands. The screening of titles and abstracts is the most time consuming part of the review process with analysts required review thousands of articles manually, taking on average 33 days. New technologies aimed at streamlining the screening process have provided initial promising findings, yet there are limitations with current approaches and barriers to the widespread use of these tools. In this paper, we introduce and report initial evidence on the utility of Research Screener, a semi-automated machine learning tool to facilitate abstract screening. Three sets of analyses (simulation, interactive and sensitivity) were conducted to provide evidence of the utility of the tool through both simulated and real-world examples. Research Screener delivered a workload saving of between 60 and 96% across nine systematic reviews and two scoping reviews. Findings from the real-world interactive analysis demonstrated a time saving of 12.53 days compared to the manual screening, which equates to a financial saving of USD 2444. Conservatively, our results suggest that analysts who scan 50% of the total pool of articles identified via a systematic search are highly likely to have identified 100% of eligible papers. In light of these findings, Research Screener is able to reduce the burden for researchers wishing to conduct a comprehensive systematic review without reducing the scientific rigour for which they strive to achieve."
155,da4b42f3fec466058bc127743db103eff2af1aa2,10.1016/j.jclinepi.2021.03.013,"Abstract Objective: We compared the process of developing searches with and without using text-mining tools (TMTs) for evidence synthesis products. Study Design: This descriptive comparative analysis included seven systematic reviews, classified as simple or complex. Two librarians created MEDLINE strategies for each review, using either usual practice (UP) or TMTs. For each search we calculated sensitivity, number-needed-to-read (NNR) and time spent developing the search strategy. Results: We found UP searches were more sensitive (UP 92% (95% CI, 85-99); TMT 84.9% (95% CI, 74.4-95.4)), with lower NNR (UP 83 (SD 34); TMT 90 (SD 68)). UP librarians spent an average of 12 h (SD 8) developing search strategies, compared to TMT librarians’ 5 hours (SD 2). Conclusion: Across all reviews, TMT searches were less sensitive than UP searches, but confidence intervals overlapped. For simple SR topics, TMT searches were faster and slightly less sensitive than UP. For complex SR topics, TMT searches were faster and less sensitive than UP searches but identified unique eligible citations not found by the UP searches."
165,f93a60bd0115fe8c6fa2d284c7e5a95e602b3fd6,10.1007/978-3-030-58080-3_43-1,No abstract found
168,031fa3822c285f5776acdbd9bdfe754a7b7b2ab3,10.1016/J.JCLINEPI.2020.01.008,"Abstract Background and Objectives Systematic reviews (SRs) are time and resource intensive, requiring approximately 1 year from protocol registration to submission for publication. Our aim was to describe the process, facilitators, and barriers to completing the first 2-week full SR. Study Design and Setting We systematically reviewed evidence of the impact of increased fluid intake, on urinary tract infection (UTI) recurrence, in individuals at risk for UTIs. The review was conducted by experienced systematic reviewers with complementary skills (two researcher clinicians, an information specialist, and an epidemiologist), using Systematic Review Automation tools, and blocked off time for the duration of the project. The outcomes were time to complete the SR, time to complete individual SR tasks, facilitators and barriers to progress, and peer reviewer feedback on the SR manuscript. Times to completion were analyzed quantitatively (minutes and calendar days); facilitators and barriers were mapped onto the Theoretical Domains Framework; and peer reviewer feedback was analyzed quantitatively and narratively. Results The SR was completed in 61 person-hours (9 workdays; 12 calendar days); accepted version of the manuscript required 71 person-hours. Individual SR tasks ranged from 16 person-minutes (deduplication of search results) to 461 person-minutes (data extraction). The least time-consuming SR tasks were obtaining full-texts, searches, citation analysis, data synthesis, and deduplication. The most time-consuming tasks were data extraction, write-up, abstract screening, full-text screening, and risk of bias. Facilitators and barriers mapped onto the following domains: knowledge; skills; memory, attention, and decision process; environmental context and resources; and technology and infrastructure. Two sets of peer reviewer feedback were received on the manuscript: the first included 34 comments requesting changes, 17 changes were made, requiring 173 person-minutes; the second requested 13 changes, and eight were made, requiring 121 person-minutes. Conclusion A small and experienced systematic reviewer team using Systematic Review Automation tools who have protected time to focus solely on the SR can complete a moderately sized SR in 2 weeks."
169,05b04e444d4be9c7b65b38701cdaa550b9c54975,10.1186/s13643-020-01528-x,"We evaluated the benefits and risks of using the Abstrackr machine learning (ML) tool to semi-automate title-abstract screening and explored whether Abstrackr’s predictions varied by review or study-level characteristics. For a convenience sample of 16 reviews for which adequate data were available to address our objectives (11 systematic reviews and 5 rapid reviews), we screened a 200-record training set in Abstrackr and downloaded the relevance (relevant or irrelevant) of the remaining records, as predicted by the tool. We retrospectively simulated the liberal-accelerated screening approach. We estimated the time savings and proportion missed compared with dual independent screening. For reviews with pairwise meta-analyses, we evaluated changes to the pooled effects after removing the missed studies. We explored whether the tool’s predictions varied by review and study-level characteristics. Using the ML-assisted liberal-accelerated approach, we wrongly excluded 0 to 3 (0 to 14%) records that were included in the final reports, but saved a median (IQR) 26 (9, 42) h of screening time. One missed study was included in eight pairwise meta-analyses in one systematic review. The pooled effect for just one of those meta-analyses changed considerably (from MD (95% CI) − 1.53 (− 2.92, − 0.15) to − 1.17 (− 2.70, 0.36)). Of 802 records in the final reports, 87% were correctly predicted as relevant. The correctness of the predictions did not differ by review (systematic or rapid, P = 0.37) or intervention type (simple or complex, P = 0.47). The predictions were more often correct in reviews with multiple (89%) vs. single (83%) research questions (P = 0.01), or that included only trials (95%) vs. multiple designs (86%) (P = 0.003). At the study level, trials (91%), mixed methods (100%), and qualitative (93%) studies were more often correctly predicted as relevant compared with observational studies (79%) or reviews (83%) (P = 0.0006). Studies at high or unclear (88%) vs. low risk of bias (80%) (P = 0.039), and those published more recently (mean (SD) 2008 (7) vs. 2006 (10), P = 0.02) were more often correctly predicted as relevant. Our screening approach saved time and may be suitable in conditions where the limited risk of missing relevant records is acceptable. Several of our findings are paradoxical and require further study to fully understand the tasks to which ML-assisted screening is best suited. The findings should be interpreted in light of the fact that the protocol was prepared for the funder, but not published a priori. Because we used a convenience sample, the findings may be prone to selection bias. The results may not be generalizable to other samples of reviews, ML tools, or screening approaches. The small number of missed studies across reviews with pairwise meta-analyses hindered strong conclusions about the effect of missed studies on the results and conclusions of systematic reviews."
173,3866b92084145948c25f8c07ea6d62fd21195bb8,10.1002/cl2.1128,No abstract found
178,5192d888af71fb6453f8709317322e610d2a1593,10.1016/j.jclinepi.2020.10.007,"Abstract Objectives To develop methods guidance to support the conduct of rapid reviews (RRs) produced within Cochrane and beyond, in response to requests for timely evidence syntheses for decision-making purposes including urgent health issues of high priority. Study Design and Setting Interim recommendations were informed by a scoping review of the underlying evidence, primary methods studies conducted, and a survey sent to 119 representatives from 20 Cochrane entities, who were asked to rate and rank RR methods across stages of review conduct. Discussions among those with expertise in RR methods further informed the list of recommendations with accompanying rationales provided. Results Based on survey results from 63 respondents (53% response rate), 26 RR methods recommendations are presented for which there was a high or moderate level of agreement or scored highest in the absence of such agreement. Where possible, how recommendations align with Cochrane methods guidance for systematic reviews is highlighted. Conclusion The Cochrane Rapid Reviews Methods Group offers new, interim guidance to support the conduct of RRs. Because best practice is limited by the lack of currently available evidence for some RR methods shortcuts taken, this guidance will need to be updated as additional abbreviated methods are evaluated."
179,56aef4f9b6623937c5576e8ae68c8f60460ee1f4,10.1038/s41370-020-0228-0,"Systematic reviews are powerful tools for drawing causal inference for evidence-based decision-making. Published systematic reviews and meta-analyses of environmental and occupational epidemiology studies have increased dramatically in recent years; however, the quality and utility of published reviews are variable. Most methodologies were adapted from clinical epidemiology and have not been adequately modified to evaluate and integrate evidence from observational epidemiology studies assessing environmental and occupational hazards, especially in evaluating the quality of exposure assessments. Although many reviews conduct a systematic and transparent assessment for the potential for bias, they are often deficient in subsequently integrating across a body of evidence. A cohesive review considers the impact of the direction and magnitude of potential biases on the results, systematically evaluates important scientific issues such as study sensitivity and effect modifiers, identifies how different studies complement each other, and assesses other potential sources of heterogeneity. Given these challenges of conducting informative systematic reviews of observational studies, we provide a series of specific recommendations based on practical examples for cohesive evidence integration to reach an overall conclusion on a body of evidence to better support policy making in public health."
185,86b1ef902c03882f905f2e0d6cebe6a3bbeb39e6,10.1007/S00500-018-3568-0,"During literature reviews, and specially when conducting systematic literature reviews, finding and screening relevant papers during scientific document search may involve managing and processing large amounts of unstructured text data. In those cases where the search topic is difficult to establish or has fuzzy limits, researchers require to broaden the scope of the search and, in consequence, data from retrieved scientific publications may become huge and uncorrelated. However, through a convenient analysis of these data the researcher may be able to discover new knowledge which may be hidden within the search output, thus exploring the limits of the search and enhancing the review scope. With that aim, this paper presents an iterative methodology that applies text mining and machine learning techniques to a downloaded corpus of abstracts from scientific databases, combining automatic processing algorithms with tools for supervised decision-making in an iterative process sustained on the researchers’ judgement, so as to adapt, screen and tune the search output. The paper ends showing a working example that employs a set of developed scripts that implement the different stages of the proposed methodology."
187,8ad74017ac84c5025abfd20c74841813eaafb684,10.1016/j.envint.2020.105623,"Abstract Background In the screening phase of systematic review, researchers use detailed inclusion/exclusion criteria to decide whether each article in a set of candidate articles is relevant to the research question under consideration. A typical review may require screening thousands or tens of thousands of articles in and can utilize hundreds of person-hours of labor. Methods Here we introduce SWIFT-Active Screener, a web-based, collaborative systematic review software application, designed to reduce the overall screening burden required during this resource-intensive phase of the review process. To prioritize articles for review, SWIFT-Active Screener uses active learning, a type of machine learning that incorporates user feedback during screening. Meanwhile, a negative binomial model is employed to estimate the number of relevant articles remaining in the unscreened document list. Using a simulation involving 26 diverse systematic review datasets that were previously screened by reviewers, we evaluated both the document prioritization and recall estimation methods. Results On average, 95% of the relevant articles were identified after screening only 40% of the total reference list. In the 5 document sets with 5,000 or more references, 95% recall was achieved after screening only 34% of the available references, on average. Furthermore, the recall estimator we have proposed provides a useful, conservative estimate of the percentage of relevant documents identified during the screening process. Conclusion SWIFT-Active Screener can result in significant time savings compared to traditional screening and the savings are increased for larger project sizes. Moreover, the integration of explicit recall estimation during screening solves an important challenge faced by all machine learning systems for document screening: when to stop screening a prioritized reference list. The software is currently available in the form of a multi-user, collaborative, online web application."
189,9c45e1ad9f94a759c2f7ce268be80784520d24e8,10.1016/j.actbio.2020.11.011,"Clinical performance of osseointegrated implants could be compromised by the medications taken by patients. The effect of a specific medication on osseointegration can be easily investigated using traditional systematic reviews. However, assessment of all known medications requires the use of evidence mapping methods. These methods allow assessment of complex questions, but they are very resource intensive when done manually. The objective of this study was to develop a machine learning algorithm to automatically map the literature assessing the effect of medications on osseointegration. Datasets of articles classified manually were used to train a machine-learning algorithm based on Support Vector Machines. The algorithm was then validated and used to screen 599,604 articles identified with an extremely sensitive search strategy. The algorithm included 281 relevant articles that described the effect of 31 different drugs on osseointegration. This approach achieved an accuracy of 95%, and compared to manual screening, it reduced the workload by 93%. The systematic mapping revealed that the treatment outcomes of osseointegrated medical devices could be influenced by drugs affecting homeostasis, inflammation, cell proliferation and bone remodeling. The effect of all known medications on the performance of osseointegrated medical devices can be assessed using evidence mappings executed with highly accurate machine learning algorithms."
191,a1b349a21a390b0eb882e65dfc4f1caf9e385c39,10.1007/978-3-030-32489-6,No abstract found
192,a4ccbafbc0b7f2811d666d9c07e2962b82d19d51,10.1016/J.EMJ.2020.09.007,"Abstract A systematic literature review provides a comprehensive overview of literature related to a research question and synthesizes previous work to strengthen a particular topic’s foundation of knowledge, while adhering to the concepts of transparency and bias reduction. In the growing, complex, and dynamic, management research field, systematic literature reviews have value, yet there is relatively little work published describing how management researchers might apply this approach. In explaining the purpose of systematic reviews, we define a systematic review and describe its rationale. We then discuss how systematic literature reviews may enhance management research and address current management research shortcomings. We present a detailed systematic literature review execution guideline, outlining systematic literature review steps, and providing keys to effective implementation."
193,aaa8dd8c23c09759bff612888eeb6f5bffa62ddf,10.1007/978-3-030-32489-6_12,"Systematic literature reviews (SLRs) have become the foundation of evidence-based software engineering (EBSE). Conducting an SLR is largely a manual process. In the past decade, researchers have made major advances in automating the SLR process, aiming to reduce the workload and effort for conducting high-quality SLRs in software engineering (SE). The goal of this chapter is to provide an overview of strategies researchers have developed to automate the SLR process. We used a systematic search methodology to survey the literature about the strategies used to automate the SLR process in SE. Study selection is the most supported activity, while protocol definition, data extraction, and synthesis have only partial support. SE researchers have most frequently explored the visual text mining strategy. Visual text mining is useful from the beginning of the process (formulation of research questions) to the end of the process (extracting and summarizing data). Overall, we recommend that the SE community develop more automated strategies to reduce the manual effort required for SLRs in SE."
197,adff49d42bca8c91c33a353436c12210606c7e8e,10.1007/978-3-030-45439-5_26,"Searching literature for a systematic review begins with a manually constructed search strategy by an expert information specialist. The typical process of constructing search strategies is often undocumented, ad-hoc, and subject to individual expertise, which may introduce bias in the systematic review. A new method for objectively deriving search strategies has arisen from information specialists attempting to address these shortcomings. However, this proposed method still presents a number of manual, ad-hoc interventions, and trial-and-error processes, potentially still introducing bias into systematic reviews. Moreover, this method has not been rigorously evaluated on a large set of systematic review cases, thus its generalisability is unknown. In this work, we present a computational adaptation of this proposed objective method. Our adaptation removes the human-in-the-loop processes involved in the initial steps of creating a search strategy for a systematic review; reducing bias due to human factors and increasing the objectivity of the originally proposed method. Our proposed computational adaptation further enables a formal and rigorous evaluation over a large set of systematic reviews. We find that our computational adaptation of the original objective method provides an effective starting point for information specialists to continue refining. We also identify a number of avenues for extending and improving our adaptation to further promote supporting information specialists."
203,341db536f516e39312147d3704eb70fa6888c6b8,10.1186/s13643-016-0315-4,"Meta-research studies investigating methods, systems, and processes designed to improve the efficiency of systematic review workflows can contribute to building an evidence base that can help to increase value and reduce waste in research. This study demonstrates the use of an economic evaluation framework to compare the costs and effects of four variant approaches to identifying eligible studies for consideration in systematic reviews. A cost-effectiveness analysis was conducted using a basic decision-analytic model, to compare the relative efficiency of ‘safety first’, ‘double screening’, ‘single screening’ and ‘single screening with text mining’ approaches in the title-abstract screening stage of a ‘case study’ systematic review about undergraduate medical education in UK general practice settings. Incremental cost-effectiveness ratios (ICERs) were calculated as the ‘incremental cost per citation ‘saved’ from inappropriate exclusion’ from the review. Resource use and effect parameters were estimated based on retrospective analysis of ‘review process’ meta-data curated alongside the ‘case study’ review, in conjunction with retrospective simulation studies to model the integrated use of text mining. Unit cost parameters were estimated based on the ‘case study’ review’s project budget. A base case analysis was conducted, with deterministic sensitivity analyses to investigate the impact of variations in values of key parameters. Use of ‘single screening with text mining’ would have resulted in title-abstract screening workload reductions (base case analysis) of >60 % compared with other approaches. Across modelled scenarios, the ‘safety first’ approach was, consistently, equally effective and less costly than conventional ‘double screening’. Compared with ‘single screening with text mining’, estimated ICERs for the two non-dominated approaches (base case analyses) ranged from £1975 (‘single screening’ without a ‘provisionally included’ code) to £4427 (‘safety first’ with a ‘provisionally included’ code) per citation ‘saved’. Patterns of results were consistent between base case and sensitivity analyses. Alternatives to the conventional ‘double screening’ approach, integrating text mining, warrant further consideration as potentially more efficient approaches to identifying eligible studies for systematic reviews. Comparable economic evaluations conducted using other systematic review datasets are needed to determine the generalisability of these findings and to build an evidence base to inform guidance for review authors."
204,3bf4293957bc475818d39c3fbec16db330d3e08a,10.1186/2046-4053-4-5,"The large and growing number of published studies, and their increasing rate of publication, makes the task of identifying relevant studies in an unbiased way for inclusion in systematic reviews both complex and time consuming. Text mining has been offered as a potential solution: through automating some of the screening process, reviewer time can be saved. The evidence base around the use of text mining for screening has not yet been pulled together systematically; this systematic review fills that research gap. Focusing mainly on non-technical issues, the review aims to increase awareness of the potential of these technologies and promote further collaborative research between the computer science and systematic review communities. Five research questions led our review: what is the state of the evidence base; how has workload reduction been evaluated; what are the purposes of semi-automation and how effective are they; how have key contextual problems of applying text mining to the systematic review field been addressed; and what challenges to implementation have emerged? We answered these questions using standard systematic review methods: systematic and exhaustive searching, quality-assured data extraction and a narrative synthesis to synthesise findings. The evidence base is active and diverse; there is almost no replication between studies or collaboration between research teams and, whilst it is difficult to establish any overall conclusions about best approaches, it is clear that efficiencies and reductions in workload are potentially achievable. On the whole, most suggested that a saving in workload of between 30% and 70% might be possible, though sometimes the saving in workload is accompanied by the loss of 5% of relevant studies (i.e. a 95% recall). Using text mining to prioritise the order in which items are screened should be considered safe and ready for use in ‘live’ reviews. The use of text mining as a ‘second screener’ may also be used cautiously. The use of text mining to eliminate studies automatically should be considered promising, but not yet fully proven. In highly technical/clinical areas, it may be used with a high degree of confidence; but more developmental and evaluative work is needed in other disciplines."
207,6a894a18bf7e0845ccc1c2197f90d0fc132b15b6,10.1016/j.jclinepi.2015.12.002,"Abstract Objectives Locating overviews of systematic reviews is difficult because of an absence of appropriate indexing terms and inconsistent terminology used to describe overviews. Our objective was to develop a validated search strategy to retrieve overviews in MEDLINE. Study Design and Setting We derived a test set of overviews from the references of two method articles on overviews. Two population sets were used to identify discriminating terms, that is, terms that appear frequently in the test set but infrequently in two population sets of references found in MEDLINE. We used text mining to conduct a frequency analysis of terms appearing in the titles and abstracts. Candidate terms were combined and tested in MEDLINE in various permutations, and the performance of strategies measured using sensitivity and precision. Results Two search strategies were developed: a sensitivity-maximizing strategy, achieving 93% sensitivity (95% confidence interval [CI]: 87, 96) and 7% precision (95% CI: 6, 8), and a sensitivity-and-precision–maximizing strategy, achieving 66% sensitivity (95% CI: 58, 74) and 21% precision (95% CI: 17, 25). Conclusion The developed search strategies enable users to more efficiently identify overviews of reviews compared to current strategies. Consistent language in describing overviews would aid in their identification, as would a specific MEDLINE Publication Type."
208,75a0afc604f6227ac11c8ecaf9a1d550a3483883,,No DOI detected
209,795ab91272adaab848cb14de1dd9b105a93b5c67,,No DOI detected
211,7c9a387e06df923d0ccb8f215f693cc44a68bd6e,10.1016/j.jclinepi.2016.11.019,• The Agency for Healthcare Research and Quality Evidence-based Practice Center program recently published a methods white paper on the effectivehealthcare.ahrq.gov web site which focused on a preliminary exploration of the use of text-mining in evidence synthesis.
212,7e987ca358de33c1f011c0cbd38a81375e819eb8,10.1186/1471-2288-13-86,"PubMed translations of OvidSP Medline search filters offer searchers improved ease of access. They may also facilitate access to PubMed’s unique content, including citations for the most recently published biomedical evidence. Retrieving this content requires a search strategy comprising natural language terms (‘textwords’), rather than Medical Subject Headings (MeSH). We describe a reproducible methodology that uses a validated PubMed search filter translation to create a textword-only strategy to extend retrieval to PubMed’s unique heart failure literature. We translated an OvidSP Medline heart failure search filter for PubMed and established version equivalence in terms of indexed literature retrieval. The PubMed version was then run within PubMed to identify citations retrieved by the filter’s MeSH terms (Heart failure, Left ventricular dysfunction, and Cardiomyopathy). It was then rerun with the same MeSH terms restricted to searching on title and abstract fields (i.e. as ‘textwords’). Citations retrieved by the MeSH search but not the textword search were isolated. Frequency analysis of their titles/abstracts identified natural language alternatives for those MeSH terms that performed less effectively as textwords. These terms were tested in combination to determine the best performing search string for reclaiming this ‘lost set’. This string, restricted to searching on PubMed’s unique content, was then combined with the validated PubMed translation to extend the filter’s performance in this database. The PubMed heart failure filter retrieved 6829 citations. Of these, 834 (12%) failed to be retrieved when MeSH terms were converted to textwords. Frequency analysis of the 834 citations identified five high frequency natural language alternatives that could improve retrieval of this set (cardiac failure, cardiac resynchronization, left ventricular systolic dysfunction, left ventricular diastolic dysfunction, and LV dysfunction). Together these terms reclaimed 157/834 (18.8%) of lost citations. MeSH terms facilitate precise searching in PubMed’s indexed subset. They may, however, work less effectively as search terms prior to subject indexing. A validated PubMed search filter can be used to develop a supplementary textword-only search strategy to extend retrieval to PubMed’s unique content. A PubMed heart failure search filter is available on the CareSearch website (
 http://www.caresearch.com.au
 ) providing access to both indexed and non-indexed heart failure evidence."
215,8a84c6742b999f6549449ca60d928c4904130733,10.1016/j.jclinepi.2016.05.002,"Abstract Background In the development of search strategies for systematic reviews, “conceptual approaches” are generally recommended to identify appropriate search terms for those parts of the strategies for which no validated search filters exist. However, “objective approaches” based on search terms identified by text analysis are increasingly being applied. Objectives To prospectively compare an objective with a conceptual approach for the development of search strategies. Methods Two different MEDLINE search strategies were developed in parallel for five systematic reviews covering a range of topics and study designs. The Institute for Quality and Efficiency in Health Care (IQWiG) applied an objective approach, and external experts applied a conceptual approach for the same research questions. For each systematic review, the citations retrieved were combined and the overall pool of citations screened to determine sensitivity and precision. Results The objective approach yielded a weighted mean sensitivity and precision of 97% and 5%. The corresponding values for the conceptual approach were 75% and 4%. Conclusion Our findings indicate that the objective approach applied by IQWiG for search strategy development yields higher sensitivity than and similar precision to a conceptual approach. The main advantage of the objective approach is that it produces consistent results across searches."
216,92fb72ddef4fc718e97164363460d6bd98b15b37,10.1016/j.jclinepi.2014.09.016,"Abstract Background Different approaches can be adopted for the development of search strategies of systematic reviews. The objective approach draws on already established text analysis methods for developing search filters. Our aim was to determine whether the objective approach for the development of search strategies was noninferior to the conceptual approach commonly used in Cochrane reviews (CRs). Methods We conducted a search for CRs published in the Cochrane Library. The studies included in the CRs were searched for in MEDLINE and represented the total set. We then tested whether references previously removed could be identified via the objective approach. We also reconstructed the original search strategies from the CRs to determine why references could not be identified by the objective approach. As we performed the validation of the search strategies without study filters, we used only sensitivity as a quality measure and did not calculate precision. Results The objective approach yielded a mean sensitivity of 96% based on 13 searches. The noninferiority test showed that this approach was noninferior to the conceptual approach used in the CRs ( P Conclusion To the best of our knowledge, our findings indicate for the first time that the objective approach for the development of search strategies is noninferior to the conceptual approach."
219,a6a85b971c9273248cc1d93cb35da3c110488b06,10.1016/S0022-4375(01)00055-X,"Problem: Worldwide, over one million people die and about 10 million people sustain permanent disabilities each year in road traffic crashes. It is a matter of urgency that effective strategies, especially from existing controlled trials (CTs), are identified for crash prevention programs. Methods: We used word frequency analysis to develop a search strategy of known sensitivity and positive predictive value (PPV) to identify reports of controlled evaluation studies of road safety interventions in the TRANSPORT database. Results: 23,554 records were searched and 319 (1.4%) records of controlled evaluation studies were identified by handsearching. We were unable to devise search strategies that combined acceptable sensitivity and PPV. Impact on industry: Efforts to improve the identification of CTs of road safety interventions are urgently required. This would involve the documentation of the study design in the title, abstract, and methods section of the research report. The study methodology should be carefully and consistently indexed in road safety databases, and editors can encourage appropriate indexing by insisting on the use of structured abstracts that give details of study methodology."
221,b3d8f225f2a208ab594264289be6d28f8bfc8849,10.2307/j.ctvnp0jkr.6,No abstract found
222,b66827ed27c093b694e3d269808751b0ae5d733e,10.1186/1471-2288-10-76,"The identification of health services research in databases such as PubMed/Medline is a cumbersome task. This task becomes even more difficult if the field of interest involves the use of diverse methods and data sources, as is the case with staffing research. This type of research investigates the association between staffing parameters and nursing and patient outcomes. A comprehensively developed search strategy may help identify staffing research in PubMed/Medline. A set of relevant references in PubMed/Medline was identified by means of three systematic reviews. This development set was used to detect candidate free-text and MeSH terms. The frequency of these terms was compared to a random sample from PubMed/Medline in order to identify terms specific to staffing research, which were then used to develop a sensitive, precise and balanced search strategy. To determine their precision, the newly developed search strategies were tested against a) the pool of relevant references extracted from the systematic reviews, b) a reference set identified from an electronic journal screening, and c) a sample from PubMed/Medline. Finally, all newly developed strategies were compared to PubMed's Health Services Research Queries (PubMed's HSR Queries). The sensitivities of the newly developed search strategies were almost 100% in all of the three test sets applied; precision ranged from 6.1% to 32.0%. PubMed's HSR queries were less sensitive (83.3% to 88.2%) than the new search strategies. Only minor differences in precision were found (5.0% to 32.0%). As with other literature on health services research, staffing studies are difficult to identify in PubMed/Medline. Depending on the purpose of the search, researchers can choose between high sensitivity and retrieval of a large number of references or high precision, i.e. and an increased risk of missing relevant references, respectively. More standardized terminology (e.g. by consistent use of the term nurse staffing) could improve the precision of future searches in this field. Empirically selected search terms can help to develop effective search strategies. The high consistency between all test sets confirmed the validity of our approach."
225,cd7c88ccecf28a117f4cfd1647a37d155a1f3ce3,10.1186/2046-4053-1-19,"Over the past few years, information retrieval has become more and more professionalized, and information specialists are considered full members of a research team conducting systematic reviews. Research groups preparing systematic reviews and clinical practice guidelines have been the driving force in the development of search strategies, but open questions remain regarding the transparency of the development process and the available resources. An empirically guided approach to the development of a search strategy provides a way to increase transparency and efficiency. Our aim in this paper is to describe the empirically guided development process for search strategies as applied by the German Institute for Quality and Efficiency in Health Care (Institut fur Qualitat und Wirtschaftlichkeit im Gesundheitswesen, or IQWiG). This strategy consists of the following steps: generation of a test set, as well as the development, validation and standardized documentation of the search strategy. We illustrate our approach by means of an example, that is, a search for literature on brachytherapy in patients with prostate cancer. For this purpose, a test set was generated, including a total of 38 references from 3 systematic reviews. The development set for the generation of the strategy included 25 references. After application of textual analytic procedures, a strategy was developed that included all references in the development set. To test the search strategy on an independent set of references, the remaining 13 references in the test set (the validation set) were used. The validation set was also completely identified. Our conclusion is that an objectively derived approach similar to that used in search filter development is a feasible way to develop and validate reliable search strategies. Besides creating high-quality strategies, the widespread application of this approach will result in a substantial increase in the transparency of the development process of search strategies."
227,e74f28e66a2aa715fff2cf60158b177c45130fdc,10.1016/j.infsof.2010.12.010,"Context: Systematic literature review (SLR) has become an important research methodology in software engineering since the introduction of evidence-based software engineering (EBSE) in 2004. One critical step in applying this methodology is to design and execute appropriate and effective search strategy. This is a time-consuming and error-prone step, which needs to be carefully planned and implemented. There is an apparent need for a systematic approach to designing, executing, and evaluating a suitable search strategy for optimally retrieving the target literature from digital libraries. Objective: The main objective of the research reported in this paper is to improve the search step of undertaking SLRs in software engineering (SE) by devising and evaluating systematic and practical approaches to identifying relevant studies in SE. Method: We have systematically selected and analytically studied a large number of papers (SLRs) to understand the state-of-the-practice of search strategies in EBSE. Having identified the limitations of the current ad-hoc nature of search strategies used by SE researchers for SLRs, we have devised a systematic and evidence-based approach to developing and executing optimal search strategies in SLRs. The proposed approach incorporates the concept of 'quasi-gold standard' (QGS), which consists of collection of known studies, and corresponding 'quasi-sensitivity' into the search process for evaluating search performance. Results: We conducted two participant-observer case studies to demonstrate and evaluate the adoption of the proposed QGS-based systematic search approach in support of SLRs in SE research. Conclusion: We report their findings based on the case studies that the approach is able to improve the rigor of search process in an SLR, as well as it can serve as a supplement to the guidelines for SLRs in EBSE. We plan to further evaluate the proposed approach using a series of case studies on varying research topics in SE."
230,f8c71cc546ecf355fbc7f6efb2a57caf1c5c3cd9,10.1007/s007999900023,"Technical terms (henceforth called terms ), are important elements for digital libraries. In this paper we present a domain-independent method for the automatic extraction of multi-word terms, from machine-readable special language corpora. The method, (C-value/NC-value ), combines linguistic and statistical information. The first part, C-value, enhances the common statistical measure of frequency of occurrence for term extraction, making it sensitive to a particular type of multi-word terms, the nested terms. The second part, NC-value, gives: 1) a method for the extraction of term context words (words that tend to appear with terms); 2) the incorporation of information from term context words to the extraction of terms."
231,fa697fe188123580a6f915575911e1d0fdb1fd09,10.1016/J.JVAL.2016.03.1700,No abstract found
232,faf120b667f55145edc89c6ef2124d55d6d10ecd,,No DOI detected
258,b7e7cc5298262ac2e6659bead07a5ac45b13d1b3,10.1007/978-1-0716-1566-9_2,"Traditionally, literature identification for systematic reviews has relied on a two-step process: first, searching databases to identify potentially relevant citations, and then manually screening those citations. A number of tools have been developed to streamline and semi-automate this process, including tools to generate terms; to visualize and evaluate search queries; to trace citation linkages; to deduplicate, limit, or translate searches across databases; and to prioritize relevant abstracts for screening. Research is ongoing into tools that can unify searching and screening into a single step, and several protype tools have been developed. As this field grows, it is becoming increasingly important to develop and codify methods for evaluating the extent to which these tools fulfill their purpose."
263,482b5bd7bf350b2028df67922c4d42966f9f6242,10.1186/s13643-020-01542-z,"Literature searches underlie the foundations of systematic reviews and related review types. Yet, the literature searching component of systematic reviews and related review types is often poorly reported. Guidance for literature search reporting has been diverse, and, in many cases, does not offer enough detail to authors who need more specific information about reporting search methods and information sources in a clear, reproducible way. This document presents the PRISMA-S (Preferred Reporting Items for Systematic reviews and Meta-Analyses literature search extension) checklist, and explanation and elaboration. The checklist was developed using a 3-stage Delphi survey process, followed by a consensus conference and public review process. The final checklist includes 16 reporting items, each of which is detailed with exemplar reporting and rationale. The intent of PRISMA-S is to complement the PRISMA Statement and its extensions by providing a checklist that could be used by interdisciplinary authors, editors, and peer reviewers to verify that each component of a search is completely reported and therefore reproducible."
264,619075bd1516d5527d636645ae404f2b279129e3,10.1016/J.COMPIND.2021.103525,"This paper proposes a method of text mining to automatically retrieve knowledge from patents on how to recycle and reuse a waste. The main novelties are the introduction of a set of specific dependency patterns and the introduction of a partially revised TRIZ (Russian acronym for Theory ¨ of Inventive Problem Solving) ¨ ontology to classify the retrieved information. The proposed dependency patterns were manually extracted from a sample patents pool about waste recycling and reuse. The classification of the information is based on different classes: (1) what transformations can be carried out on the waste, (2) what technologies can be used to carry out these transformations, (3) what products can be obtained by transforming the waste, (4) what functions can be carried out by the waste, (5) with which technologies, and (6) on which entities. An automatic implementation of the proposed method, involving the manual check of the retrieved results, was tested through a case study about wood chip recycling and reuse. Compared to the dependency patterns from the literature, the proposed ones allowed to retrieve 28 % more pertinent information. This results mainly depends by better ability of the proposed patterns to better discriminate the relevant sentences from which to extract information, compared to the other patterns (i.e. + 40 %). The automatic classification of the information was also correctly performed: in almost each class, precision and recall were higher than 60 % and on average equal to 90 %."
270,1f8bebce6b3fab57c6cb121f56b9375f27bb48ce,10.1016/j.ijdrr.2020.101658,"Abstract This is a systematic review of emergency management publications using the bibliometric analysis. The authors addressed the questions of how emergency management research has evolved in connections to previous literature, what are the fundamental elements of effective strategies, and what topics emerged as new trends in adapting to changing environments. Findings reveal that topics of communication, collaboration, citizen participation, risk perception, and vulnerability have been addressed consistently over the last four decades. Findings also show that scholars have responded to emerging challenges by establishing emergency management systems, designing an international framework for climate change, and shifting focus from response to mitigation with special emphasis on community resilience and sustainability. The authors expect that this study lays a foundation for shaping future EM research."
271,52cb07ebebfac6ad6b2dbcf2309a17d03772fa22,10.1016/j.envint.2020.105739,"Background - The World Health Organization (WHO) and the International Labour Organization (ILO) are developing Joint Estimates of the work-related burden of disease and injury (WHO/ILO Joint Estimates), with contributions from a large network of experts. Evidence from mechanistic data suggests that exposure to long working hours may cause ischaemic heart disease (IHD). In this paper, we present a systematic review and meta-analysis of parameters for estimating the number of deaths and disability-adjusted life years from IHD that are attributable to exposure to long working hours, for the development of the WHO/ILO Joint Estimates. Objectives - We aimed to systematically review and meta-analyse estimates of the effect of exposure to long working hours (three categories: 41-48, 49-54 and ≥55 h/week), compared with exposure to standard working hours (35-40 h/week), on IHD (three outcomes: prevalence, incidence and mortality). Data sources - We developed and published a protocol, applying the Navigation Guide as an organizing systematic review framework where feasible. We searched electronic databases for potentially relevant records from published and unpublished studies, including MEDLINE, Scopus, Web of Science, CISDOC, PsycINFO, and WHO ICTRP. We also searched grey literature databases, Internet search engines and organizational websites; hand-searched reference lists of previous systematic reviews; and consulted additional experts. Study eligibility and criteria - We included working-age (≥15 years) workers in the formal and informal economy in any WHO and/or ILO Member State but excluded children (aged < 15 years) and unpaid domestic workers. We included randomized controlled trials, cohort studies, case-control studies and other non-randomized intervention studies which contained an estimate of the effect of exposure to long working hours (41-48, 49-54 and ≥55 h/week), compared with exposure to standard working hours (35-40 h/week), on IHD (prevalence, incidence or mortality). Study appraisal and synthesis methods - At least two review authors independently screened titles and abstracts against the eligibility criteria at a first stage and full texts of potentially eligible records at a second stage, followed by extraction of data from qualifying studies. Missing data were requested from principal study authors. We combined relative risks using random-effect meta-analysis. Two or more review authors assessed the risk of bias, quality of evidence and strength of evidence, using Navigation Guide and GRADE tools and approaches adapted to this project. Results - Thirty-seven studies (26 prospective cohort studies and 11 case-control studies) met the inclusion criteria, comprising a total of 768,751 participants (310,954 females) in 13 countries in three WHO regions (Americas, Europe and Western Pacific). The exposure was measured using self-reports in all studies, and the outcome was assessed with administrative health records (30 studies) or self-reported physician diagnosis (7 studies). The outcome was defined as incident non-fatal IHD event in 19 studies (8 cohort studies, 11 case-control studies), incident fatal IHD event in two studies (both cohort studies), and incident non-fatal or fatal (mixed) event in 16 studies (all cohort studies). Because we judged cohort studies to have a relatively lower risk of bias, we prioritized evidence from these studies and treated evidence from case-control studies as supporting evidence. For the bodies of evidence for both outcomes with any eligible studies (i.e. IHD incidence and mortality), we did not have serious concerns for risk of bias (at least for the cohort studies). No eligible study was found on the effect of long working hours on IHD prevalence. Compared with working 35-40 h/week, we are uncertain about the effect on acquiring (or incidence of) IHD of working 41-48 h/week (relative risk (RR) 0.98, 95% confidence interval (CI) 0.91 to 1.07, 20 studies, 312,209 participants, I 0%, quality of evidence) and 49-54 h/week (RR 1.05, 95% CI 0.94 to 1.17, 18 studies, 308,405 participants, I 0%, quality of evidence). Compared with working 35-40 h/week, working ≥55 h/week may have led to a moderately, clinically meaningful increase in the risk of acquiring IHD, when followed up between one year and 20 years (RR 1.13, 95% CI 1.02 to 1.26, 22 studies, 339,680 participants, I 5%, moderate quality of evidence). Compared with working 35-40 h/week, we are very uncertain about the effect on dying (mortality) from IHD of working 41-48 h/week (RR 0.99, 95% CI 0.88 to 1.12, 13 studies, 288,278 participants, I 8%, quality of evidence) and 49-54 h/week (RR 1.01, 95% CI 0.82 to 1.25, 11 studies, 284,474 participants, I 13%, quality of evidence). Compared with working 35-40 h/week, working ≥55 h/week may have led to a moderate, clinically meaningful increase in the risk of dying from IHD when followed up between eight and 30 years (RR 1.17, 95% CI 1.05 to 1.31, 16 studies, 726,803 participants, I 0%, moderate quality of evidence). Subgroup analyses found no evidence for differences by WHO region and sex, but RRs were higher among persons with lower SES. Sensitivity analyses found no differences by outcome definition (exclusively non-fatal or fatal versus mixed), outcome measurement (health records versus self-reports) and risk of bias (high/probably high ratings in any domain versus low/probably low in all domains). Conclusions - We judged the existing bodies of evidence for human evidence as inadequate evidence for harmfulness for the exposure categories 41-48 and 49-54 h/week for IHD prevalence, incidence and mortality, and for the exposure category ≥55 h/week for IHD prevalence. Evidence on exposure to working ≥55 h/week was judged as sufficient evidence of harmfulness for IHD incidence and mortality. Producing estimates for the burden of IHD attributable to exposure to working ≥55 h/week appears evidence-based, and the pooled effect estimates presented in this systematic review could be used as input data for the WHO/ILO Joint Estimates."
273,d692cc6f1a755d91a986f3e26f0691c0bab3e85c,10.1002/cl2.1129,"The volume of published academic research is growing rapidly and this new era of “big literature” poses new challenges to evidence synthesis, pushing traditional, manual methods of evidence synthesis to their limits. New technology developments, including machine learning, are likely to provide solutions to the problem of information overload and allow scaling of systematic maps to large and even vast literatures. In this paper, we outline how systematic maps lend themselves well to automation and computer‐assistance. We believe that it is a major priority to consolidate efforts to develop and validate efficient, rigorous and robust applications of these novel technologies, ensuring the challenges of big literature do not prevent the future production of systematic maps."
275,0c23ba3f1119d7e5d08fd90b6206a062e26699b1,10.1007/978-3-030-25719-4_8,This article provides an overview of modern natural language processing and understanding methods. All the monitored technologies are covered in the context of search engines. The authors do not consider any particular implementations of the search engines; however take in consideration some scientific research to show natural language processing techniques application prospects in the informational search industry.
278,52b3518aa9cea805a849dee7a47bd49c09f84e36,,No DOI detected
279,919f88bbe3ff2b3b86b6ce2c1014648dd92e6e68,,No DOI detected
283,cbc7bfb31880e2640535f661b878b5d390d3e2cf,10.1186/s12874-018-0545-3,"Systematic literature searching is recognised as a critical component of the systematic review process. It involves a systematic search for studies and aims for a transparent report of study identification, leaving readers clear about what was done to identify studies, and how the findings of the review are situated in the relevant evidence. Information specialists and review teams appear to work from a shared and tacit model of the literature search process. How this tacit model has developed and evolved is unclear, and it has not been explicitly examined before. The purpose of this review is to determine if a shared model of the literature searching process can be detected across systematic review guidance documents and, if so, how this process is reported in the guidance and supported by published studies. A literature review. Two types of literature were reviewed: guidance and published studies. Nine guidance documents were identified, including: The Cochrane and Campbell Handbooks. Published studies were identified through ‘pearl growing’, citation chasing, a search of PubMed using the systematic review methods filter, and the authors’ topic knowledge. The relevant sections within each guidance document were then read and re-read, with the aim of determining key methodological stages. Methodological stages were identified and defined. This data was reviewed to identify agreements and areas of unique guidance between guidance documents. Consensus across multiple guidance documents was used to inform selection of ‘key stages’ in the process of literature searching. Eight key stages were determined relating specifically to literature searching in systematic reviews. They were: who should literature search, aims and purpose of literature searching, preparation, the search strategy, searching databases, supplementary searching, managing references and reporting the search process. Eight key stages to the process of literature searching in systematic reviews were identified. These key stages are consistently reported in the nine guidance documents, suggesting consensus on the key stages of literature searching, and therefore the process of literature searching as a whole, in systematic reviews. Further research to determine the suitability of using the same process of literature searching for all types of systematic review is indicated."
286,0b49fa8774dec2d789c79f8fa0455185608d2847,10.1016/j.jbi.2015.11.010,"Display Omitted We present an approach for query-focused extractive summarisation of medical text.Our approach is data-centric and utilises a specialised annotated corpus.We introduce target-sentence-specific summarisation to improve summarisation.We incorporate query information and domain-specific knowledge in various ways.Our approach performs significantly better than existing benchmark approaches. BackgroundEvidence-based medicine practice requires medical practitioners to rely on the best available evidence, in addition to their expertise, when making clinical decisions. The medical domain boasts a large amount of published medical research data, indexed in various medical databases such as MEDLINE. As the size of this data grows, practitioners increasingly face the problem of information overload, and past research has established the time-associated obstacles faced by evidence-based medicine practitioners. In this paper, we focus on the problem of automatic text summarisation to help practitioners quickly find query-focused information from relevant documents. MethodsWe utilise an annotated corpus that is specialised for the task of evidence-based summarisation of text. In contrast to past summarisation approaches, which mostly rely on surface level features to identify salient pieces of texts that form the summaries, our approach focuses on the use of corpus-based statistics, and domain-specific lexical knowledge for the identification of summary contents. We also apply a target-sentence-specific summarisation technique that reduces the problem of underfitting that persists in generic summarisation models. ResultsIn automatic evaluations run over a large number of annotated summaries, our extractive summarisation technique statistically outperforms various baseline and benchmark summarisation models with a percentile rank of 96.8%. A manual evaluation shows that our extractive summarisation approach is capable of selecting content with high recall and precision, and may thus be used to generate bottom-line answers to practitioners' queries. ConclusionsOur research shows that the incorporation of specialised data and domain-specific knowledge can significantly improve text summarisation performance in the medical domain. Due to the vast amounts of medical text available, and the high growth of this form of data, we suspect that such summarisation techniques will address the time-related obstacles associated with evidence-based medicine."
288,1776fd3832162e91fb7088af0a2ae799e147c18c,10.1186/s13643-017-0667-4,"The second meeting of the International Collaboration for Automation of Systematic Reviews (ICASR) was held 3–4 October 2016 in Philadelphia, Pennsylvania, USA. ICASR is an interdisciplinary group whose aim is to maximize the use of technology for conducting rapid, accurate, and efficient systematic reviews of scientific evidence. Having automated tools for systematic review should enable more transparent and timely review, maximizing the potential for identifying and translating research findings to practical application. The meeting brought together multiple stakeholder groups including users of summarized research, methodologists who explore production processes and systematic review quality, and technologists such as software developers, statisticians, and vendors. This diversity of participants was intended to ensure effective communication with numerous stakeholders about progress toward automation of systematic reviews and stimulate discussion about potential solutions to identified challenges. The meeting highlighted challenges, both simple and complex, and raised awareness among participants about ongoing efforts by various stakeholders. An outcome of this forum was to identify several short-term projects that participants felt would advance the automation of tasks in the systematic review workflow including (1) fostering better understanding about available tools, (2) developing validated datasets for testing new tools, (3) determining a standard method to facilitate interoperability of tools such as through an application programming interface or API, and (4) establishing criteria to evaluate the quality of tools’ output. ICASR 2016 provided a beneficial forum to foster focused discussion about tool development and resources and reconfirm ICASR members’ commitment toward systematic reviews’ automation."
290,1d5972b32a9b5a455a6eef389de5b7fca25771ad,10.1007/978-3-319-58347-1_10,"We introduce a new representation learning approach for domain adaptation, in which data at training and test time come from similar but different distributions. Our approach is directly inspired by the theory on domain adaptation suggesting that, for effective domain transfer to be achieved, predictions must be made based on features that cannot discriminate between the training (source) and test (target) domains.

The approach implements this idea in the context of neural network architectures that are trained on labeled data from the source domain and unlabeled data from the target domain (no labeled target-domain data is necessary). As the training progresses, the approach promotes the emergence of features that are (i) discriminative for the main learning task on the source domain and (ii) indiscriminate with respect to the shift between the domains. We show that this adaptation behaviour can be achieved in almost any feed-forward model by augmenting it with few standard layers and a new gradient reversal layer. The resulting augmented architecture can be trained using standard backpropagation and stochastic gradient descent, and can thus be implemented with little effort using any of the deep learning packages.

We demonstrate the success of our approach for two distinct classification problems (document sentiment analysis and image classification), where state-of-the-art domain adaptation performance on standard benchmarks is achieved. We also validate the approach for descriptor learning task in the context of person re-identification application."
294,2571edb40c84552e094cd4d19f420bf44aeeb7b3,10.1016/J.JEMERMED.2007.11.022,No abstract found
297,34b9635d7779e219e9d60e0d3d33919ca9bc123c,10.1016/S1084-8045(02)00061-9,No abstract found
299,3a29aa4eff48624752c07059a44d3288a678c8ab,10.18653/v1/p16-1,No abstract found
303,40212e9474c3ddf3d8c6ffd13dd3211ec9406c49,10.1007/BFb0026683,"This paper explores the use of Support Vector Machines (SVMs) for learning text classifiers from examples. It analyzes the particular properties of learning with text data and identifies why SVMs are appropriate for this task. Empirical results support the theoretical findings. SVMs achieve substantial improvements over the currently best performing methods and behave robustly over a variety of different learning tasks. Furthermore they are fully automatic, eliminating the need for manual parameter tuning."
305,4666c17d770f0bc4c570754e17893fbb299e29d5,10.1186/s13643-015-0117-0,"Identifying relevant studies for inclusion in a systematic review (i.e. screening) is a complex, laborious and expensive task. Recently, a number of studies has shown that the use of machine learning and text mining methods to automatically identify relevant studies has the potential to drastically decrease the workload involved in the screening phase. The vast majority of these machine learning methods exploit the same underlying principle, i.e. a study is modelled as a bag-of-words (BOW). We explore the use of topic modelling methods to derive a more informative representation of studies. We apply Latent Dirichlet allocation (LDA), an unsupervised topic modelling approach, to automatically identify topics in a collection of studies. We then represent each study as a distribution of LDA topics. Additionally, we enrich topics derived using LDA with multi-word terms identified by using an automatic term recognition (ATR) tool. For evaluation purposes, we carry out automatic identification of relevant studies using support vector machine (SVM)-based classifiers that employ both our novel topic-based representation and the BOW representation. Our results show that the SVM classifier is able to identify a greater number of relevant studies when using the LDA representation than the BOW representation. These observations hold for two systematic reviews of the clinical domain and three reviews of the social science domain. A topic-based feature representation of documents outperforms the BOW representation when applied to the task of automatic citation screening. The proposed term-enriched topics are more informative and less ambiguous to systematic reviewers."
306,587d74d91d7279b7b8b7e578e14e5779ed04bae6,10.1016/B978-0-12-398476-0.00012-9,"The aim of evidence-based medicine (EBM) is to inform clinical practice in the light of all available evidence. Systematic review, in which one rigorously synthesizes all of the published literature relevant to a precisely formulated clinical question, is the foundation of EBM. Creating such syntheses involves identifying, extracting and combining reliable pieces of evidence from published articles. Systematic reviews are increasingly used to inform decisions at all levels of health care, from bedside individualized care to policy-making. They have gained wide acceptance as a practical way to provide reliable and comprehensive syntheses of the medical evidence base. But producing and maintaining such reviews is becoming increasingly onerous due to the exponential expansion of the biomedical literature: as the amount of published evidence increases, so too does the labor required to synthesize it. Moreover, higher standards for review methods have further increased the labor involved in conducting systematic reviews, exacerbating the problems caused by information overload. This chapter provides a basic introduction to systematic review and meta-analysis methods; here we focus on methods for providing the evidence base for decision support, and also the potential for medical informatics to facilitate and optimize the conduct and maintenance of evidence syntheses."
312,8fafdabcb7ee5512a617e31a3f6c0586536fcb38,10.1001/JAMA.282.7.634,No abstract found
315,a7b94b558c8430c24e913451e79ef81f4582c7ae,10.1186/s13326-017-0113-5,"Facing a growing workload and dwindling resources, the US National Library of Medicine (NLM) created the Indexing Initiative project in 1996. This cross-library team’s mission is to explore indexing methodologies for ensuring quality and currency of NLM document collections. The NLM Medical Text Indexer (MTI) is the main product of this project and has been providing automated indexing recommendations since 2002. After all of this time, the questions arise whether MTI is still useful and relevant. To answer the question about MTI usefulness, we track a wide variety of statistics related to how frequently MEDLINE indexers refer to MTI recommendations, how well MTI performs against human indexing, and how often MTI is used. To answer the question of MTI relevancy compared to other available tools, we have participated in the 2013 and 2014 BioASQ Challenges. The BioASQ Challenges have provided us with an unbiased comparison between the MTI system and other systems performing the same task. Indexers have continually increased their use of MTI recommendations over the years from 15.75% of the articles they index in 2002 to 62.44% in 2014 showing that the indexers find MTI to be increasingly useful. The MTI performance statistics show significant improvement in Precision (+0.2992) and F1 (+0.1997) with modest gains in Recall (+0.0454) over the years. MTI consistency is comparable to the available indexer consistency studies. MTI performed well in both of the BioASQ Challenges ranking within the top tier teams. Based on our findings, yes, MTI is still relevant and useful, and needs to be improved and expanded. The BioASQ Challenge results have shown that we need to incorporate more machine learning into MTI while still retaining the indexing rules that have earned MTI the indexers’ trust over the years. We also need to expand MTI through the use of full text, when and where it is available, to provide coverage of indexing terms that are typically only found in the full text. The role of MTI at NLM is also expanding into new areas, further reinforcing the idea that MTI is increasingly useful and relevant."
319,cf8d63a55d4cbd18debd1f23cd40d85f8a4fc0aa,10.1186/s13643-015-0066-7,"Automation of the parts of systematic review process, specifically the data extraction step, may be an important strategy to reduce the time necessary to complete a systematic review. However, the state of the science of automatically extracting data elements from full texts has not been well described. This paper performs a systematic review of published and unpublished methods to automate data extraction for systematic reviews. We systematically searched PubMed, IEEEXplore, and ACM Digital Library to identify potentially relevant articles. We included reports that met the following criteria: 1) methods or results section described what entities were or need to be extracted, and 2) at least one entity was automatically extracted with evaluation results that were presented for that entity. We also reviewed the citations from included reports. Out of a total of 1190 unique citations that met our search criteria, we found 26 published reports describing automatic extraction of at least one of more than 52 potential data elements used in systematic reviews. For 25 (48 %) of the data elements used in systematic reviews, there were attempts from various researchers to extract information automatically from the publication text. Out of these, 14 (27 %) data elements were completely extracted, but the highest number of data elements extracted automatically by a single study was 7. Most of the data elements were extracted with F-scores (a mean of sensitivity and positive predictive value) of over 70 %. We found no unified information extraction framework tailored to the systematic review process, and published reports focused on a limited (1–7) number of data elements. Biomedical natural language processing techniques have not been fully utilized to fully or even partially automate the data extraction step of systematic reviews."
326,f39a05625f500f6cea90681825680d063e28489f,10.1016/j.jclinepi.2017.08.011,"New approaches to evidence synthesis, which use human effort and machine automation in mutually reinforcing ways, can enhance the feasibility and sustainability of living systematic reviews. Human effort is a scarce and valuable resource, required when automation is impossible or undesirable, and includes contributions from online communities (crowds) as well as more conventional contributions from review authors and information specialists. Automation can assist with some systematic review tasks, including searching, eligibility assessment, identification and retrieval of full-text reports, extraction of data, and risk of bias assessment. Workflows can be developed in which human effort and machine automation can each enable the other to operate in more effective and efficient ways, offering substantial enhancement to the productivity of systematic reviews. This paper describes and discusses the potential-and limitations-of new ways of undertaking specific tasks in living systematic reviews, identifying areas where these human/machine are already in use, and where further research and development is needed. While the context is living systematic reviews, many of these enabling technologies apply equally to standard approaches to systematic reviewing."
332,0cccd7b049bfd2e63b10588c326a6ad6a1a68f58,10.1002/mar.21657,"Meta-analysis is a research method for systematically combining and synthesizing findings from multiple quantitative studies in a research domain. Despite its importance, most literature evaluating meta-analyses are based on data analysis and statistical discussions. This paper takes a holistic view, comparing meta-analyses to traditional systematic literature reviews. We described steps of the meta-analytic process including question definition, data collection, data analysis, and reporting results. For each step, we explain the primary purpose, the tasks required of the meta-analyst, and recommendations for best practice. Finally, we discuss recent developments in meta-analytic techniques, which increase its effectiveness in business research."
337,2ab1cde790dddb74d11625b37586ef085526cf83,10.1111/1365-2664.14154,"In Christie et al. (2019), we used simulations to quantitatively compare the bias of commonly used study designs in ecology and conservation. Based on these simulations, we proposed ‘accuracy weights’ as a potential way to account for study design validity in meta-analytic weighting methods. Pescott and Stewart (2022) raised concerns that these weights may not be generalisable and still lead to biased meta-estimates. Here we respond to their concerns and demonstrate why developing alternative weighting methods is key to the future of evidence synthesis. We acknowledge that our simple simulation unfairly penalised randomised controlled trial (RCT) relative to before-after control-impact (BACI) designs as we considered that the parallel trends assumption held for BACI designs. We point to an empirical follow-up study in which we more fairly quantify differences in biases between different study designs. However, we stand by our main findings that before-after (BA), control-impact (CI) and after designs are quantifiably more biased than BACI and RCT designs. We also emphasise that our ‘accuracy weighting’ method was preliminary and welcome future research to incorporate more dimensions of study quality. We further show that over a decade of advances in quality effect modelling, which Pescott and Stewart (2022) omit, highlights the importance of research such as ours in better understanding how to quantitatively integrate data on study quality directly into meta-analyses. We further argue that the traditional methods advocated for by Pescott and Stewart (2022; e.g. manual risk-of-bias assessments and inverse-variance weighting) are subjective, wasteful and potentially biased themselves. They also lack scalability for use in large syntheses that keep up-to-date with the rapidly growing scientific literature. Synthesis and applications. We suggest, contrary to Pescott and Stewart's narrative, that moving towards alternative weighting methods is key to future-proofing evidence synthesis through greater automation, flexibility and updating to respond to decision-makers' needs—particularly in crisis disciplines in conservation science where problematic biases and variability exist in study designs, contexts and metrics used. While we must be cautious to avoid misinforming decision-makers, this should not stop us investigating alternative weighting methods that integrate study quality data directly into meta-analyses. To reliably and pragmatically inform decision-makers with science, we need efficient, scalable, readily automated and feasible methods to appraise and weight studies to produce large-scale living syntheses of the future."
342,4e5b632acecccec97a6681c44bbe28a0664ac641,10.1007/s00228-022-03329-8,"PurposeTo assess the feasibility and acceptance of the semi-automated meta-analysis (SAMA). The objectives are twofold, namely (1) to compare expert opinion on the quality of protocols, methods, and results of one conventional meta-analysis (CMA) and one SAMA and (2) to compare the time to execute the CMA and the SAMA.MethodsExperts evaluated the protocols and manuscripts/reports of the CMA and SAMA conducted independently on the safety of metronidazole in pregnancy. Expert opinion was collected using AMSTAR 2 checklist. Time spent was recorded using case report forms.ResultsThe overall scores of the opinion of all experts for protocols, methods, and results for SAMA (6.75) and CMA (6.87) were not statistically different (p = 0.88). The experts’ confidence in the results of each MA was 7.89 ± 1.17 and 8.11 ± 0.92, respectively. The time to completion was 14 working days for SAMA and 24.7 for CMA. MA tasks such as calculation of effect estimates, subgroup/sensitivity analysis, and publication bias investigation required no investment in time for SAMA.ConclusionIn conclusion, our study demonstrated the feasibility of SAMA and suggests acceptance for risk assessment by an expert committee. Our results suggest that SAMA reduces the time required for a MA without altering expert confidence in the methodological and scientific rigor. As our study was limited to one example, the generalization of our results requires confirmation by other studies."
346,7f7cc403b2e318bb91a374f337b24ae71975cdcf,10.1007/s11095-022-03201-5,"Model-based meta-analysis (MBMA) is a quantitative approach that leverages published summary data along with internal data and can be applied to inform key drug development decisions, including the benefit-risk assessment of a treatment under investigation. These risk-benefit assessments may involve determining an optimal dose compared against historic external comparators of a particular disease indication. MBMA can provide a flexible framework for interpreting aggregated data from historic reference studies and therefore should be a standard tool for the model-informed drug development (MIDD) framework.In addition to pairwise and network meta-analyses, MBMA provides further contributions in the quantitative approaches with its ability to incorporate longitudinal data and the pharmacologic concept of dose-response relationship, as well as to combine individual- and summary-level data and routinely incorporate covariates in the analysis.A common application of MBMA is the selection of optimal dose and dosing regimen of the internal investigational molecule to evaluate external benchmarking and to support comparator selection. Two case studies provided examples in applications of MBMA in biologics (durvalumab + tremelimumab for safety) and small molecule (fenebrutinib for efficacy) to support drug development decision-making in two different but well-studied disease areas, i.e., oncology and rheumatoid arthritis, respectively.Important to the future directions of MBMA include additional recognition and engagement from drug development stakeholders for the MBMA approach, stronger collaboration between pharmacometrics and statistics, expanded data access, and the use of machine learning for database building. Timely, cost-effective, and successful application of MBMA should be part of providing an integrated view of MIDD."
348,8aef9a537ba227194a856fc2ef80deb7fd428ac9,10.1007/978-3-030-93715-7_22,"Literature reviews are essential parts of every academic paper, and there are many tools, which are trying to suggest relevant articles and make the process of working with citations easier. Many of them offer to focus on just specific recommended papers or provide a general picture of the area without explanations or hints on how particular papers can help. Co-citation networks serve as a foundation of multiple useful methods for citation recommendations, enabling the analysis of the structure of the scientific field. However, existing instruments using them have a steep learning curve. In this paper, we present the workflow prototype to elicit and evaluate a set of heuristics employing co-citation network analysis in the literature review process. We performed a step-by-step analysis, including analysis of bibliographic data visualization service VOSviewer patterns of use, which allowed us to synthesize Job Stories for the specification of possible user needs for citation recommendation. We produced a set of heuristics for the analysis of co-citation networks based on Job Stories. The heuristics are then evaluated on the set of papers from two Human-Computer Interaction conferences to reflect on their applicability and usability. Our results can be used to inform more straightforward navigation through co-citation networks, possible design improvements of services for literature management and bibliographic data visualization, as well as a foundation for learning designs for enhancing academic writing skills."
356,b52e681c39d31e6abf58ad1f5d9709e2dce12e1f,10.1186/s12911-022-01897-4,No DOI detected
368,0c800421ffa25836b74e48a0b3ddea8890e07e1b,10.1111/AEC.13052,"Research disciplines in science have historically developed in silos but are increasingly multidisciplinary. Here, we assessed how the insect ecology literature published in ecological and entomological journals has developed over the last 20 years and which topics have crossed discipline boundaries. We used structural topic modelling to assess research trends from 34 304 articles published in six ecology journals and six entomology journals between 2000 and 2020. We then identified and compared topics that emerged from the entire body of literature, or corpus, with topics that emerged from a subsection of articles that focused only on insects (insect corpus). We found that, within the entire corpus, topics on ‘Community ecology’, ‘Traits, life history & physiology’ and ‘Ecological methods & theory’ became more prevalent over time (hot topics), whereas ‘Population modelling’, ‘Insect development’, ‘Reproduction & ontogeny’ and ‘Plant growth’ declined in prevalence over the 20 years we surveyed (cold topics). In the insect corpus, we found that hot topics included ‘Thermal tolerance’ and ‘Disease vectors’, whereas cold topics included ‘Herbivore phenology’, ‘Insect-plant interactions’ and ‘Parasitoids and parasites’. ‘Landscape ecology’ was a growth topic area for both corpora. Our findings suggest that insect-related research is a major component of the broader ecological discipline, and there are topics in ecology where insect research aligns with general ecological trends. However, specific topics unique to the insect corpora – such as insect taxonomy – are fundamental to both insect and ecology research."
371,14a4c64ccdae3d7c2fb2a18ee8636d69ce956f16,10.1016/J.RECESP.2021.06.016,"Resumen La declaracion PRISMA (Preferred Reporting Items for Systematic reviews and Meta-Analyses), publicada en 2009, se diseno para ayudar a los autores de revisiones sistematicas a documentar de manera transparente el porque de la revision, que hicieron los autores y que encontraron. Durante la ultima decada, ha habido muchos avances en la metodologia y terminologia de las revisiones sistematicas, lo que ha requerido una actualizacion de esta guia. La declaracion prisma 2020 sustituye a la declaracion de 2009 e incluye una nueva guia de presentacion de las publicaciones que refleja los avances en los metodos para identificar, seleccionar, evaluar y sintetizar estudios. La estructura y la presentacion de los items ha sido modificada para facilitar su implementacion. En este articulo, presentamos la lista de verificacion PRISMA 2020 con 27 items, y una lista de verificacion ampliada que detalla las recomendaciones en la publicacion de cada item, la lista de verificacion del resumen estructurado PRISMA 2020 y el diagrama de flujo revisado para revisiones sistematicas."
372,1742ab13bfc96b2ac8a9f53318597649817abb65,10.1186/s12874-021-01463-y,"Background: There is an unmet need for review methods to support priority-setting, policy-making and strategic planning when a wide variety of interventions from differing disciplines may have the potential to impact a health outcome of interest. This article describes a Modular Literature Review, a novel systematic search and review method that employs systematic search strategies together with a hierarchy-based appraisal and synthesis of the resulting evidence. // Methods: We designed the Modular Review to examine the effects of 43 interventions on a health problem of global significance. Using the PICOS (Population, Intervention, Comparison, Outcome, Study design) framework, we developed a single four-module search template in which population, comparison and outcome modules were the same for each search and the intervention module was different for each of the 43 interventions. A series of literature searches were performed in five databases, followed by screening, extraction and analysis of data. “ES documents”, source documents for effect size (ES) estimates, were systematically identified based on a hierarchy of evidence. The evidence was categorised according to the likely effect on the outcome and presented in a standardised format with quantitative effect estimates, meta-analyses and narrative reporting. We compared the Modular Review to other review methods in health research for its strengths and limitations. // Results: The Modular Review method was used to review the impact of 46 antenatal interventions on four specified birth outcomes within 12 months. A total of 61,279 records were found; 35,244 were screened by title-abstract. Six thousand two hundred seventy-two full articles were reviewed against the inclusion criteria resulting in 365 eligible articles. // Conclusions: The Modular Review preserves principles that have traditionally been important to systematic reviews but can address multiple research questions simultaneously. The result is an accessible, reliable answer to the question of “what works?”. Thus, it is a well-suited literature review method to support prioritisation, decisions and planning to implement an agenda for health improvement."
373,2185b6b32a1339ab209ea083114227007352dfc8,10.1007/978-3-030-66147-2_10,"Systematic reviews are a firmly established method of ensuring that proposed research is based upon the best available scientific evidence. In this chapter, we provide a brief history of systematic reviews and discuss their adaptation to preclinical studies. The steps in conducting a systematic review are explained, with examples of best practice. Readers will learn how to critically evaluate the quality of systematic reviews in their own fields. Basic guidance on the parts of a systematic review and meta-analysis are explained. Critically appraised topics (or knowledge summaries) are also described, and their relevance for preclinical research is explained, including a worked example."
374,23a6f783190957fee7ec26f72a84738f64bac71a,10.1186/s12874-021-01451-2,"Systematic reviews are the cornerstone of evidence-based medicine. However, systematic reviews are time consuming and there is growing demand to produce evidence more quickly, while maintaining robust methods. In recent years, artificial intelligence and active-machine learning (AML) have been implemented into several SR software applications. As some of the barriers to adoption of new technologies are the challenges in set-up and how best to use these technologies, we have provided different situations and considerations for knowledge synthesis teams to consider when using artificial intelligence and AML for title and abstract screening.We retrospectively evaluated the implementation and performance of AML across a set of ten historically completed systematic reviews. Based upon the findings from this work and in consideration of the barriers we have encountered and navigated during the past 24 months in using these tools prospectively in our research, we discussed and developed a series of practical recommendations for research teams to consider in seeking to implement AML tools for citation screening into their workflow.We developed a seven-step framework and provide guidance for when and how to integrate artificial intelligence and AML into the title and abstract screening process. Steps include: (1) Consulting with Knowledge user/Expert Panel; (2) Developing the search strategy; (3) Preparing your review team; (4) Preparing your database; (5) Building the initial training set; (6) Ongoing screening; and (7) Truncating screening. During Step 6 and/or 7, you may also choose to optimize your team, by shifting some members to other review stages (e.g., full-text screening, data extraction).Artificial intelligence and, more specifically, AML are well-developed tools for title and abstract screening and can be integrated into the screening process in several ways. Regardless of the method chosen, transparent reporting of these methods is critical for future studies evaluating artificial intelligence and AML."
380,3a225feee7a0a6677077c5d1c4a203c1851d11f2,10.1057/s41267-020-00385-z,"Meta-analyses summarize a field’s research base and are therefore highly influential. Despite their value, the standards for an excellent meta-analysis, one that is potentially award-winning, have changed in the last decade. Each step of a meta-analysis is now more formalized, from the identification of relevant articles to coding, moderator analysis, and reporting of results. What was exemplary a decade ago can be somewhat dated today. Using the award-winning meta-analysis by Stahl et al. (Unraveling the effects of cultural diversity in teams: A meta-analysis of research on multicultural work groups. Journal of International Business Studies, 41(4):690–709, 2010) as an exemplar, we adopted a multi-disciplinary approach (e.g., management, psychology, health sciences) to summarize the anatomy (i.e., fundamental components) of a modern meta-analysis, focusing on: (1) data collection (i.e., literature search and screening, coding), (2) data preparation (i.e., treatment of multiple effect sizes, outlier identification and management, publication bias), (3) data analysis (i.e., average effect sizes, heterogeneity of effect sizes, moderator search), and (4) reporting (i.e., transparency and reproducibility, future research directions). In addition, we provide guidelines and a decision-making tree for when even foundational and highly cited meta-analyses should be updated. Based on the latest evidence, we summarize what journal editors and reviewers should expect, authors should provide, and readers (i.e., other researchers, practitioners, and policymakers) should consider about meta-analytic reviews."
382,3ad641921dbb490bc8c60866d55a6b0f20e9317b,10.1016/j.rec.2021.07.010,"The Preferred Reporting Items for Systematic reviews and Meta-Analyses (PRISMA) statement, published in 2009, was designed to help systematic reviewers transparently report why the review was done, what the authors did, and what they found. Over the past decade, advances in systematic review methodology and terminology have necessitated an update to the guideline. The PRISMA 2020 statement replaces the 2009 statement and includes new reporting guidance that reflects advances in methods to identify, select, appraise, and synthesise studies. The structure and presentation of the items have been modified to facilitate implementation. In this article, we present the PRISMA 2020 27-item checklist, an expanded checklist that details reporting recommendations for each item, the PRISMA 2020 abstract checklist, and the revised flow diagrams for original and updated reviews. Full English text available from:www.revespcardiol.org/en."
383,3b4c9d05f22fe36c311a824305d4e15d41d66816,10.1016/j.clinthera.2020.12.014,"Abstract Artificial intelligence (AI), a highly interdisciplinary science, is an increasing presence in pharmacovigilance (PV). A better understanding of the scope of artificial intelligence in pharmacovigilance (AIPV) may be advantageous to more sharply defining, for example, which terms, methods, tasks, and data sets are suitably subsumed under the application of AIPV. Accordingly, this article explores relevant points to consider regarding defining the scope of AIPV and offers a potential working definition of the scope of AIPV."
385,4a33de4b15922794bf24491fa39ead0f170f83b0,10.1016/j.jbi.2021.103970,"• In this work, we perform Named Entity Recognition (NER) on the 2018 TAC SRIE dataset, which contain entities related to experimental design. • We evaluate three algorithms finding those which use embedded representation outperform the statistical algorithm. • We discuss how our models underperform for lexically diverse entities, and how our system does not effectively address mentions which are non-contiguous. • We determine that given the large number of entities in the dataset, our models perform better when trained to sequence subsets of entities. Systematic reviews are labor-intensive processes to combine all knowledge about a given topic into a coherent summary. Despite the high labor investment, they are necessary to create an exhaustive overview of current evidence relevant to a research question. In this work, we evaluate three state-of-the-art supervised multi-label sequence classification systems to automatically identify 24 different experimental design factors for the categories of Animal , Dose , Exposure , and Endpoint from journal articles describing the experiments related to toxicity and health effects of environmental agents. We then present an in depth analysis of the results evaluating the lexical diversity of the design parameters with respect to model performance, evaluating the impact of tokenization and non-contiguous mentions, and finally evaluating the dependencies between entities within the category entities. We demonstrate that in general, algorithms that use embedded representations of the sequences out-perform statistical algorithms, but that even these algorithms struggle with lexically diverse entities."
391,66f88de8b5f0f6dbd8dcdf6a54c41d41f6296426,10.1186/s13643-020-01569-2,"The increasingly rapid rate of evidence publication has made it difficult for evidence synthesis—systematic reviews and health guidelines—to be continually kept up to date. One proposed solution for this is the use of automation in health evidence synthesis. Guideline developers are key gatekeepers in the acceptance and use of evidence, and therefore, their opinions on the potential use of automation are crucial. The objective of this study was to analyze the attitudes of guideline developers towards the use of automation in health evidence synthesis. The Diffusion of Innovations framework was chosen as an initial analytical framework because it encapsulates some of the core issues which are thought to affect the adoption of new innovations in practice. This well-established theory posits five dimensions which affect the adoption of novel technologies: Relative Advantage, Compatibility, Complexity, Trialability, and Observability. Eighteen interviews were conducted with individuals who were currently working, or had previously worked, in guideline development. After transcription, a multiphase mixed deductive and grounded approach was used to analyze the data. First, transcripts were coded with a deductive approach using Rogers’ Diffusion of Innovation as the top-level themes. Second, sub-themes within the framework were identified using a grounded approach. Participants were consistently most concerned with the extent to which an innovation is in line with current values and practices (i.e., Compatibility in the Diffusion of Innovations framework). Participants were also concerned with Relative Advantage and Observability, which were discussed in approximately equal amounts. For the latter, participants expressed a desire for transparency in the methodology of automation software. Participants were noticeably less interested in Complexity and Trialability, which were discussed infrequently. These results were reasonably consistent across all participants. If machine learning and other automation technologies are to be used more widely and to their full potential in systematic reviews and guideline development, it is crucial to ensure new technologies are in line with current values and practice. It will also be important to maximize the transparency of the methods of these technologies to address the concerns of guideline developers."
393,8301e8c86bd3e1791855a2370d804b111e818748,10.1038/S42256-020-00287-7,"To help researchers conduct a systematic review or meta-analysis as efficiently and transparently as possible, we designed a tool (ASReview) to accelerate the step of screening titles and abstracts. For many tasks - including but not limited to systematic reviews and meta-analyses - the scientific literature needs to be checked systematically. Currently, scholars and practitioners screen thousands of studies by hand to determine which studies to include in their review or meta-analysis. This is error prone and inefficient because of extremely imbalanced data: only a fraction of the screened studies is relevant. The future of systematic reviewing will be an interaction with machine learning algorithms to deal with the enormous increase of available text. We therefore developed an open source machine learning-aided pipeline applying active learning: ASReview. We demonstrate by means of simulation studies that ASReview can yield far more efficient reviewing than manual reviewing, while providing high quality. Furthermore, we describe the options of the free and open source research software and present the results from user experience tests. We invite the community to contribute to open source projects such as our own that provide measurable and reproducible improvements over current practice."
399,9019b14a5ff7a760e01725ed927b0018da7e7075,10.1186/s12911-021-01638-z,"Background An Informatics Consult has been proposed in which clinicians request novel evidence from large scale health data resources, tailored to the treatment of a specific patient. However, the availability of such consultations is lacking. We seek to provide an Informatics Consult for a situation where a treatment indication and contraindication coexist in the same patient, i.e., anti-coagulation use for stroke prevention in a patient with both atrial fibrillation (AF) and liver cirrhosis. Methods We examined four sources of evidence for the effect of warfarin on stroke risk or all-cause mortality from: (1) randomised controlled trials (RCTs), (2) meta-analysis of prior observational studies, (3) trial emulation (using population electronic health records (N = 3,854,710) and (4) genetic evidence (Mendelian randomisation). We developed prototype forms to request an Informatics Consult and return of results in electronic health record systems. Results We found 0 RCT reports and 0 trials recruiting for patients with AF and cirrhosis. We found broad concordance across the three new sources of evidence we generated. Meta-analysis of prior observational studies showed that warfarin use was associated with lower stroke risk (hazard ratio [HR] = 0.71, CI 0.39-1.29). In a target trial emulation, warfarin was associated with lower all-cause mortality (HR = 0.61, CI 0.49-0.76) and ischaemic stroke (HR = 0.27, CI 0.08-0.91). Mendelian randomisation served as a drug target validation where we found that lower levels of vitamin K1 (warfarin is a vitamin K1 antagonist) are associated with lower stroke risk. A pilot survey with an independent sample of 34 clinicians revealed that 85% of clinicians found information on prognosis useful and that 79% thought that they should have access to the Informatics Consult as a service within their healthcare systems. We identified candidate steps for automation to scale evidence generation and to accelerate the return of results. Conclusion We performed a proof-of-concept Informatics Consult for evidence generation, which may inform treatment decisions in situations where there is dearth of randomised trials. Patients are surprised to know that their clinicians are currently not able to learn in clinic from data on 'patients like me'. We identify the key challenges in offering such an Informatics Consult as a service."
400,9db3a6c0ae9651c4aee7974f1c8341a39aa85d80,10.5334/bc.67,"Energy use is of crucial importance for the global challenge of climate change, and also is an essential part of daily life. Hence, research on energy needs to be robust and valid. Other scientific disciplines have experienced a reproducibility crisis, i.e. existing findings could not be reproduced in new studies. The ‘TReQ’ approach is recommended to improve research practices in the energy field and arrive at greater transparency, reproducibility and quality. A highly adaptable suite of tools is presented that can be applied to energy research approaches across this multidisciplinary and fast-changing field. In particular, the following tools are introduced – preregistration of studies, making data and code publicly available, using preprints, and employing reporting guidelines – to heighten the standard of research practices within the energy field. The wider adoption of these tools can facilitate greater trust in the findings of research used to inform evidence-based policy and practice in the energy field. Practice relevance Concrete suggestions are provided for how and when to use preregistration, open data and code, preprints, and reporting guidelines, offering practical guidance for energy researchers for improving the TReQ of their research. The paper shows how employing tools around these concepts at appropriate stages of the research process can assure end-users of the research that good practices were followed. This will not only increase trust in research findings but also can deliver other co-benefits for researchers, e.g. more efficient processes and a more collaborative and open research culture. Increased TReQ can help remove barriers to accessing research both within and outside of academia, improving the visibility and impact of research findings. Finally, a checklist is presented that can be added to publications to show how the tools were used."
403,9ff53708c41e86f77623bbd592e29475f04447c7,10.1016/j.jclinepi.2021.12.005,"The objectives of this scoping review are to identify the reliability and validity of the available tools, their limitations and any recommendations to further improve the use of these tools.A scoping review methodology was followed to map the literature published on the challenges and solutions of conducting evidence synthesis using the JBI scoping review methodology.A total of 47 publications were included in the review. The current scoping review identified that LitSuggest, Rayyan, Abstractr, BIBOT, R software, RobotAnalyst, DistillerSR, ExaCT and NetMetaXL have potential to be used for the automation of systematic reviews. However, they are not without limitations. The review also identified other studies that employed algorithms that have not yet been developed into user friendly tools. Some of these algorithms showed high validity and reliability but their use is conditional on user knowledge of computer science and algorithms.Abstract screening has reached maturity; data extraction is still an active area. Developing methods to semi-automate different steps of evidence synthesis via machine learning remains an important research direction. Also, it is important to move from the research prototypes currently available to professionally maintained platforms."
410,b210eda80dbb0815f69ac7c269acc2f2354caa4c,10.1186/s13643-021-01626-4,"The Preferred Reporting Items for Systematic reviews and Meta-Analyses (PRISMA) statement, published in 2009, was designed to help systematic reviewers transparently report why the review was done, what the authors did, and what they found. Over the past decade, advances in systematic review methodology and terminology have necessitated an update to the guideline. The PRISMA 2020 statement replaces the 2009 statement and includes new reporting guidance that reflects advances in methods to identify, select, appraise, and synthesise studies. The structure and presentation of the items have been modified to facilitate implementation. In this article, we present the PRISMA 2020 27-item checklist, an expanded checklist that details reporting recommendations for each item, the PRISMA 2020 abstract checklist, and the revised flow diagrams for original and updated reviews."
416,c38b3d627d47f8d756507e90c44ce96493768e16,10.2196/33124,"Systematic reviews depend on time-consuming extraction of data from the PDFs of underlying studies. To date, automation efforts have focused on extracting data from the text, and no approach has yet succeeded in fully automating ingestion of quantitative evidence. However, the majority of relevant data is generally presented in tables, and the tabular structure is more amenable to automated extraction than free text.The purpose of this study was to classify the structure and format of descriptive statistics reported in tables in the comparative medical literature.We sampled 100 published randomized controlled trials from 2019 based on a search in PubMed; these results were imported to the AutoLit platform. Studies were excluded if they were nonclinical, noncomparative, not in English, protocols, or not available in full text. In AutoLit, tables reporting baseline or outcome data in all studies were characterized based on reporting practices. Measurement context, meaning the structure in which the interventions of interest, patient arm breakdown, measurement time points, and data element descriptions were presented, was classified based on the number of contextual pieces and metadata reported. The statistic formats for reported metrics (specific instances of reporting of data elements) were then classified by location and broken down into reporting strategies for continuous, dichotomous, and categorical metrics.We included 78 of 100 sampled studies, one of which (1.3%) did not report data elements in tables. The remaining 77 studies reported baseline and outcome data in 174 tables, and 96% (69/72) of these tables broke down reporting by patient arms. Fifteen structures were found for the reporting of measurement context, which were broadly grouped into: 1×1 contexts, where two pieces of context are reported in total (eg, arms in columns, data elements in rows); 2×1 contexts, where two pieces of context are given on row headers (eg, time points in columns, arms nested in data elements on rows); and 1×2 contexts, where two pieces of context are given on column headers. The 1×1 contexts were present in 57% of tables (99/174), compared to 20% (34/174) for 2×1 contexts and 15% (26/174) for 1×2 contexts; the remaining 8% (15/174) used unique/other stratification methods. Statistic formats were reported in the headers or descriptions of 84% (65/74) of studies.In this cross-sectional pilot review, we found a high density of information in tables, but with major heterogeneity in presentation of measurement context. The highest-density studies reported both baseline and outcome measures in tables, with arm-level breakout, intervention labels, and arm sizes present, and reported both the statistic formats and units. The measurement context formats presented here, broadly classified into three classes that cover 92% (71/78) of studies, form a basis for understanding the frequency of different reporting styles, supporting automated detection of the data format for extraction of metrics."
420,cafeb847539d5352baa8d71fc7b9660f0b6fa9eb,10.1007/978-3-030-90966-6_9,"Systematic reviews are an essential tool to help healthcare providers and medical practitioners stay up to date on the latest evidence and practices within their field. In recent years, developments have arisen in automating these systematic reviews. While automated systematic reviews are not currently in widespread use in the medical field, they present a way to increase the output of systematic reviews. This study is a literature review of how the automating of systematic reviews is currently being integrated into the healthcare system. This literature review will be relying on tools such as Web of Science, Harzing’s Publish or Perish, VOSviewer, CiteSpace, and MAXQDA to collect and analyze article data sets based on keywords “automating systematic reviews” and “healthcare systematic reviews”. Co-citation and content analyses were performed on data sets to determine which articles were most relevant. These analyses showed that automating systematic reviews is developing more within fields outside of the medical field. However, researchers are beginning to take an interest in the applications of an automated systematic review to use in the medical field. The healthcare sector has been more hesitant to adopt automation and often continues to rely on traditional methods. Automation technology is quickly advancing but currently lacks the ability to apply critical thinking of how literature is relevant to current practice, more easily understood by clinician experience, attitudes, and values."
426,e711aaaff9a058fee88ef08b1e495ee3cbf7ea63,10.1016/j.nedt.2021.104868,"Abstract Objectives Simulation-based learning is widely used in nurse education, including virtual reality (VR) methods which have experienced a major growth lately. Virtual reality offers risk free and contactless learning. Currently, little is known about what topics of nursing are adopted for VR simulations and how their design meets various educational goals. This review aims to scope existing articles on educational VR nursing simulations, and to analyse approaches from didactic and technical perspectives. Method A systematic mapping review following the PRISMA-ScR guideline and PICo search strategy was conducted. Peer reviewed articles in English and German were searched across Scopus, CINAHL, PsycINFO, PSYNDEX, PsycARTICLES, PubMed, ERIC and The Cochrane Library. Studies had to include at least one immersive head-mounted display VR simulation in the field of nursing education. Data extraction and analysis was performed in a narrative, graphical and tabular way. Results Twenty-two articles were identified. There is a large variety in the use and definition of VR simulation for educational purposes. Simulations were classified into four main educational objectives: procedural skills training to improve technical knowledge and proficiency; emergency response training that focusses on confidence; soft skills training that teaches empathy; and finally, psychomotor skills training. Various approaches and simulation designs were implemented to achieve these educational outcomes. A few of them were highly innovative in providing an immersive experience to learn complex tasks, e.g. auscultation, or foster empathy by mimicking life with dementia. Conclusions Despite an increase in the use of state-of-the-art VR nursing simulations, there is still a paucity of studies on immersive HMD based VR scenarios. Researchers designing educational VR packages need to be clear on terminology. In order to make full use of VR, designers should consider including haptic devices to practise psychomotor skills and include social interaction to teach soft skills."
429,f37de233cee350efdc5ecdd927f5d95906e628c8,10.1016/j.jclinepi.2021.03.001,"The Preferred Reporting Items for Systematic reviews and Meta-Analyses (PRISMA) statement, published in 2009, was designed to help systematic reviewers transparently report why the review was done, what the authors did, and what they found. Over the past decade, advances in systematic review methodology and terminology have necessitated an update to the guideline. The PRISMA 2020 statement replaces the 2009 statement and includes new reporting guidance that reflects advances in methods to identify, select, appraise, and synthesise studies. The structure and presentation of the items have been modified to facilitate implementation. In this article, we present the PRISMA 2020 27-item checklist, an expanded checklist that details reporting recommendations for each item, the PRISMA 2020 abstract checklist, and the revised flow diagrams for original and updated reviews."
432,1bfa5d3cd4088d3eb356ea5c4b82b7994759ef51,10.1186/s13643-016-0361-y,"While the debate regarding shared clinical trial data has shifted from whether such data should be shared to how this is best achieved, the sharing of data collected as part of systematic reviews has received little attention. In this commentary, we discuss the potential benefits of coordinated efforts to share data collected as part of systematic reviews. There are a number of potential benefits of systematic review data sharing. Shared information and data obtained as part of the systematic review process may reduce unnecessary duplication, reduce demand on trialist to service repeated requests from reviewers for data, and improve the quality and efficiency of future reviews. Sharing also facilitates research to improve clinical trial and systematic review methods and supports additional analyses to address secondary research questions. While concerns regarding appropriate use of data, costs, or the academic return for original review authors may impede more open access to information extracted as part of systematic reviews, many of these issues are being addressed, and infrastructure to enable greater access to such information is being developed. Embracing systems to enable more open access to systematic review data has considerable potential to maximise the benefits of research investment in undertaking systematic reviews."
433,5281ede65f81cf0dcbca11326888067b6c5f52d2,10.1186/s13643-019-1062-0,"Although many aspects of systematic reviews use computational tools, systematic reviewers have been reluctant to adopt machine learning tools. We discuss that the potential reason for the slow adoption of machine learning tools into systematic reviews is multifactorial. We focus on the current absence of trust in automation and set-up challenges as major barriers to adoption. It is important that reviews produced using automation tools are considered non-inferior or superior to current practice. However, this standard will likely not be sufficient to lead to widespread adoption. As with many technologies, it is important that reviewers see “others” in the review community using automation tools. Adoption will also be slow if the automation tools are not compatible with workflows and tasks currently used to produce reviews. Many automation tools being developed for systematic reviews mimic classification problems. Therefore, the evidence that these automation tools are non-inferior or superior can be presented using methods similar to diagnostic test evaluations, i.e., precision and recall compared to a human reviewer. However, the assessment of automation tools does present unique challenges for investigators and systematic reviewers, including the need to clarify which metrics are of interest to the systematic review community and the unique documentation challenges for reproducible software experiments. We discuss adoption barriers with the goal of providing tool developers with guidance as to how to design and report such evaluations and for end users to assess their validity. Further, we discuss approaches to formatting and announcing publicly available datasets suitable for assessment of automation technologies and tools. Making these resources available will increase trust that tools are non-inferior or superior to current practice. Finally, we identify that, even with evidence that automation tools are non-inferior or superior to current practice, substantial set-up challenges remain for main stream integration of automation into the systematic review process."
434,72f11f8fe10b867f7ccc7b26603912ee9e788bab,10.1097/01.ccm.0000550940.69610.da,No abstract found
435,7bbc1c1351981075379c34bc4fd16f922c48201c,10.1186/s13643-019-1074-9,"Technologies and methods to speed up the production of systematic reviews by reducing the manual labour involved have recently emerged. Automation has been proposed or used to expedite most steps of the systematic review process, including search, screening, and data extraction. However, how these technologies work in practice and when (and when not) to use them is often not clear to practitioners. In this practical guide, we provide an overview of current machine learning methods that have been proposed to expedite evidence synthesis. We also offer guidance on which of these are ready for use, their strengths and weaknesses, and how a systematic review team might go about using them in practice."
437,1f8b9700affc15680cc67f4e6ac4bc1c077c97f9,10.1007/978-3-319-52677-5_194-1,No abstract found
440,ebcf551e1a1b9a225d1580e1daba29b796424466,10.1016/j.jocn.2021.04.043,"Abstract Glioma is the most common primary intraparenchymal tumor of the brain and the 5-year survival rate of high-grade glioma is poor. Magnetic resonance imaging (MRI) is essential for detecting, characterizing and monitoring brain tumors but definitive diagnosis still relies on surgical pathology. Machine learning has been applied to the analysis of MRI data in glioma research and has the potential to change clinical practice and improve patient outcomes. This systematic review synthesizes and analyzes the current state of machine learning applications to glioma MRI data and explores the use of machine learning for systematic review automation. Various datapoints were extracted from the 153 studies that met inclusion criteria and analyzed. Natural language processing (NLP) analysis involved keyword extraction, topic modeling and document classification. Machine learning has been applied to tumor grading and diagnosis, tumor segmentation, non-invasive genomic biomarker identification, detection of progression and patient survival prediction. Model performance was generally strong (AUC = 0.87 ± 0.09; sensitivity = 0.87 ± 0.10; specificity = 0.0.86 ± 0.10; precision = 0.88 ± 0.11). Convolutional neural network, support vector machine and random forest algorithms were top performers. Deep learning document classifiers yielded acceptable performance (mean 5-fold cross-validation AUC = 0.71). Machine learning tools and data resources were synthesized and summarized to facilitate future research. Machine learning has been widely applied to the processing of MRI data in glioma research and has demonstrated substantial utility. NLP and transfer learning resources enabled the successful development of a replicable method for automating the systematic review article screening process, which has potential for shortening the time from discovery to clinical application in medicine."
443,418a17d3a1ae6036ac1776125c83932815987559,10.1038/s42256-020-00235-5,"The United Nations Sustainable Development Goal 2 (SDG 2) is to achieve zero hunger by 2030. We have designed Persephone, a machine learning model, to support a diverse volunteer network of 77 researchers from 23 countries engaged in creating interdisciplinary evidence syntheses in support of SDG 2. Such evidence syntheses, whatever the specific topic, assess original studies to determine the effectiveness of interventions. By gathering and summarizing current evidence and providing objective recommendations they can be valuable aids to decision-makers. However, they are time-consuming; estimates range from 18 months to three years to produce a single review. Persephone analysed 500,000 unstructured text summaries from prominent sources of agricultural research, determining with 90% accuracy the subset of studies that would eventually be selected by expert researchers. We demonstrate that machine learning models can be invaluable in placing evidence into the hands of policymakers. Evidence syntheses produced from the scientific literature are important tools for policymakers. Producing such evidence syntheses can be highly time- and labour-consuming but machine learning models can help as already demonstrated in the health and medical sciences. This Perspective describes a machine learning-based framework specifically designed to support evidence syntheses in the area of agricultural research, for tackling the UN Sustainable Development Goal 2: zero hunger by 2030."
446,935489033eb450aaf007cc89d974c431b27cdd74,10.1186/s13643-020-01324-7,"Improving the speed of systematic review (SR) development is key to supporting evidence-based medicine. Machine learning tools which semi-automate citation screening might improve efficiency. Few studies have assessed use of screening prioritization functionality or compared two tools head to head. In this project, we compared performance of two machine-learning tools for potential use in citation screening. Using 9 evidence reports previously completed by the ECRI Institute Evidence-based Practice Center team, we compared performance of Abstrackr and EPPI-Reviewer, two off-the-shelf citations screening tools, for identifying relevant citations. Screening prioritization functionality was tested for 3 large reports and 6 small reports on a range of clinical topics. Large report topics were imaging for pancreatic cancer, indoor allergen reduction, and inguinal hernia repair. We trained Abstrackr and EPPI-Reviewer and screened all citations in 10% increments. In Task 1, we inputted whether an abstract was ordered for full-text screening; in Task 2, we inputted whether an abstract was included in the final report. For both tasks, screening continued until all studies ordered and included for the actual reports were identified. We assessed potential reductions in hypothetical screening burden (proportion of citations screened to identify all included studies) offered by each tool for all 9 reports. For the 3 large reports, both EPPI-Reviewer and Abstrackr performed well with potential reductions in screening burden of 4 to 49% (Abstrackr) and 9 to 60% (EPPI-Reviewer). Both tools had markedly poorer performance for 1 large report (inguinal hernia), possibly due to its heterogeneous key questions. Based on McNemar’s test for paired proportions in the 3 large reports, EPPI-Reviewer outperformed Abstrackr for identifying articles ordered for full-text review, but Abstrackr performed better in 2 of 3 reports for identifying articles included in the final report. For small reports, both tools provided benefits but EPPI-Reviewer generally outperformed Abstrackr in both tasks, although these results were often not statistically significant. Abstrackr and EPPI-Reviewer performed well, but prioritization accuracy varied greatly across reports. Our work suggests screening prioritization functionality is a promising modality offering efficiency gains without giving up human involvement in the screening process."
448,0a8de6509c21b1502ac7559d2576bfdc0388db89,10.1186/s13643-019-1250-y,"Conducting systematic reviews (“reviews”) requires a great deal of effort and resources. Making data extracted during reviews available publicly could offer many benefits, including reducing unnecessary duplication of effort, standardizing data, supporting analyses to address secondary research questions, and facilitating methodologic research. Funded by the US Agency for Healthcare Research and Quality (AHRQ), the Systematic Review Data Repository (SRDR) is a free, web-based, open-source, data management and archival platform for reviews. Our specific objectives in this paper are to describe (1) the current extent of usage of SRDR and (2) the characteristics of all projects with publicly available data on the SRDR website. We examined all projects with data made publicly available through SRDR as of November 12, 2019. We extracted information about the characteristics of these projects. Two investigators extracted and verified the data. SRDR has had 2552 individual user accounts belonging to users from 80 countries. Since SRDR’s launch in 2012, data have been made available publicly for 152 of the 735 projects in SRDR (21%), at a rate of 24.5 projects per year, on average. Most projects are in clinical fields (144/152 projects; 95%); most have evaluated interventions (therapeutic or preventive) (109/152; 72%). The most frequent health areas addressed are mental and behavioral disorders (31/152; 20%) and diseases of the eye and ocular adnexa (23/152; 15%). Two-thirds of the projects (104/152; 67%) were funded by AHRQ, and one-sixth (23/152; 15%) are Cochrane reviews. The 152 projects each address a median of 3 research questions (IQR 1–5) and include a median of 70 studies (IQR 20–130). Until we arrive at a future in which the systematic review and broader research communities are comfortable with the accuracy of automated data extraction, re-use of data extracted by humans has the potential to help reduce redundancy and costs. The 152 projects with publicly available data through SRDR, and the more than 15,000 studies therein, are freely available to researchers and the general public who might be working on similar reviews or updates of reviews or who want access to the data for decision-making, meta-research, or other purposes."
454,1c32125cc9fb052b881a6dec812b62ed998915d7,10.1016/J.JSS.2006.07.009,"A consequence of the growing number of empirical studies in software engineering is the need to adopt systematic approaches to assessing and aggregating research outcomes in order to provide a balanced and objective summary of research evidence for a particular topic. The paper reports experiences with applying one such approach, the practice of systematic literature review, to the published studies relevant to topics within the software engineering domain. The systematic literature review process is summarised, a number of reviews being undertaken by the authors and others are described and some lessons about the applicability of this practice to software engineering are extracted.The basic systematic literature review process seems appropriate to software engineering and the preparation and validation of a review protocol in advance of a review activity is especially valuable. The paper highlights areas where some adaptation of the process to accommodate the domain-specific characteristics of software engineering is needed as well as areas where improvements to current software engineering infrastructure and practices would enhance its applicability. In particular, infrastructure support provided by software engineering indexing databases is inadequate. Also, the quality of abstracts is poor; it is usually not possible to judge the relevance of a study from a review of the abstract alone."
456,30f33bec81675a74e97c7976068629202407e1af,10.1007/s00204-017-1980-3,"Systematic reviews, pioneered in the clinical field, provide a transparent, methodologically rigorous and reproducible means of summarizing the available evidence on a precisely framed research question. Having matured to a well-established approach in many research fields, systematic reviews are receiving increasing attention as a potential tool for answering toxicological questions. In the larger framework of evidence-based toxicology, the advantages and obstacles of, as well as the approaches for, adapting and adopting systematic reviews to toxicology are still being explored. To provide the toxicology community with a starting point for conducting or understanding systematic reviews, we herein summarized available guidance documents from various fields of application. We have elaborated on the systematic review process by breaking it down into ten steps, starting with planning the project, framing the question, and writing and publishing the protocol, and concluding with interpretation and reporting. In addition, we have identified the specific methodological challenges of toxicological questions and have summarized how these can be addressed. Ultimately, this primer is intended to stimulate scientific discussions of the identified issues to fuel the development of toxicology-specific methodology and to encourage the application of systematic review methodology to toxicological issues."
462,3e729541cb6e6bb161e9cbff51593fb041a93ab6,10.1186/s12916-016-0596-4,"In a recent research article in BMC Medicine, Crequit and colleagues demonstrate how published systematic reviews in lung cancer provide a fragmented, out-of-date picture of the evidence for all treatments. The results and conclusions drawn from this study, based on cumulative network meta-analyses (NMA) of evidence from randomized clinical trials over time, are quite compelling. The inherent waste of research resulting from incomplete evidence synthesis has wide-reaching implications for a range of target groups including developers of systematic reviews and guidelines and their end-users, health care professionals and patients at the point of care. Building on emerging concepts for systematic reviews and NMA, the authors propose living cumulative as a potential solution and paradigmatic shift. Here we describe how recent innovations within authoring, dissemination, and updating of systematic reviews and trustworthy guidelines may greatly facilitate the production of NMA. Some additional challenges need to be solved for NMA in general, and for cumulative NMA in particular, before a paradigmatic shift for systematic reviews can become reality."
464,5f9e948be446694c925aad687973936cf18571a8,10.1186/2046-4053-3-74,"Systematic reviews, a cornerstone of evidence-based medicine, are not produced quickly enough to support clinical practice. The cost of production, availability of the requisite expertise and timeliness are often quoted as major contributors for the delay. This detailed survey of the state of the art of information systems designed to support or automate individual tasks in the systematic review, and in particular systematic reviews of randomized controlled clinical trials, reveals trends that see the convergence of several parallel research projects. We surveyed literature describing informatics systems that support or automate the processes of systematic review or each of the tasks of the systematic review. Several projects focus on automating, simplifying and/or streamlining specific tasks of the systematic review. Some tasks are already fully automated while others are still largely manual. In this review, we describe each task and the effect that its automation would have on the entire systematic review process, summarize the existing information system support for each task, and highlight where further research is needed for realizing automation for the task. Integration of the systems that automate systematic review tasks may lead to a revised systematic review workflow. We envisage the optimized workflow will lead to system in which each systematic review is described as a computer program that automatically retrieves relevant trials, appraises them, extracts and synthesizes data, evaluates the risk of bias, performs meta-analysis calculations, and produces a report in real time."
468,6ecdf07925073d26e1b6884a5761f3d5bb834fff,10.1186/s13643-015-0031-5,No abstract found
477,c67bc0b3f88f5f78ad662f4f9fe5d644ae63e9f9,10.1016/j.apergo.2015.09.013,"In this paper we present a literature review of the evolution of the levels of autonomy from the end of the 1950s up until now. The motivation of this study was primarily to gather and to compare the literature that exists, on taxonomies on levels of automation. Technical developments within both computer hardware and software have made it possible to introduce autonomy into virtually all aspects of human-machine systems. The current study, is focusing on how different authors treat the problem of different levels of automation. The outcome of this study is to present the differences between the proposed levels of automation and the various taxonomies, giving the potential users a number of choices in order to decide which taxonomy satisfies their needs better. In addition, this paper surveys deals with the term adaptive automation, which seems to be a new trend in the literature on autonomy."
480,d60598af7022eabac1854c654aa2f710a1cb3bb7,,No DOI detected
481,d72f3641738f685a56e2c54da798782c3d906cd6,10.1016/j.infsof.2004.08.005,"Recently, software engineering has witnessed a great increase in the amount of work with an empirical component; however, this work has often little or no established empirical framework within the topic to draw upon. Frequently, researchers use frameworks from other disciplines in an attempt to alleviate this deficiency. A common underpinning in these frameworks is that experimental replication is available as the cornerstone of knowledge discovery within the discipline. This paper investigates the issues involved in accepting this premise as a fundamental building block with empirical software engineering and recommends extending the traditional view of replication to improve the effectiveness of this essential process within our domain."
483,de1cd3e4aab96dc90dc6d6b9f5a90c6790f738c5,10.1016/j.infsof.2017.06.007,"Abstract Context Even with the increasing use of Systematic Literature Reviews (SLR) in software engineering (SE), there are still a number of barriers faced by SLR authors. These barriers increase the cost of conducting SLRs. Objective For many of these barriers, appropriate tool support could reduce their impact. In this paper, we use interactions with the SLR community in SE to identify and prioritize a set of requirements for SLR tooling infrastructure. Method This paper analyzes and combines the results from three studies on SLR process barriers and SLR tool requirements to produce a prioritized list of functional requirements for SLR tool support. Using this list of requirements, we perform a feature analysis of the current SLR support tools to identify requirements that are supported as well as identify the need for additional tooling infrastructure. Results The analysis resulted in a list 112 detailed requirements (consolidated into a set of composite requirements) that SE community desires in SLR support tools. The requirements span all the phases of the SLR process. The results show that, while recent tools cover more of the requirements, there are a number of high-priority requirements that are not yet fully covered by any of the existing tools. Conclusion The existing set of SLR tools do not cover all the requirements posed by the community. The list of requirements in this paper is useful for tool developers and researchers wishing to provide support to the SLR community with SE."
485,f0e3fc326cf84ec016c25e07016bd2305098a855,10.1016/j.jbi.2017.07.010,"Abstract Context Independent validation of published scientific results through study replication is a pre-condition for accepting the validity of such results. In computation research, full replication is often unrealistic for independent results validation, therefore, study reproduction has been justified as the minimum acceptable standard to evaluate the validity of scientific claims. The application of text mining techniques to citation screening in the context of systematic literature reviews is a relatively young and growing computational field with high relevance for software engineering, medical research and other fields. However, there is little work so far on reproduction studies in the field. Objective In this paper, we investigate the reproducibility of studies in this area based on information contained in published articles and we propose reporting guidelines that could improve reproducibility. Methods The study was approached in two ways. Initially we attempted to reproduce results from six studies, which were based on the same raw dataset. Then, based on this experience, we identified steps considered essential to successful reproduction of text mining experiments and characterized them to measure how reproducible is a study given the information provided on these steps. 33 articles were systematically assessed for reproducibility using this approach. Results Our work revealed that it is currently difficult if not impossible to independently reproduce the results published in any of the studies investigated. The lack of information about the datasets used limits reproducibility of about 80% of the studies assessed. Also, information about the machine learning algorithms is inadequate in about 27% of the papers. On the plus side, the third party software tools used are mostly free and available. Conclusions The reproducibility potential of most of the studies can be significantly improved if more attention is paid to information provided on the datasets used, how they were partitioned and utilized, and how any randomization was controlled. We introduce a checklist of information that needs to be provided in order to ensure that a published study can be reproduced."
490,087da4667cfd162f615511277f04bd5f172a9ad8,10.1016/j.jisa.2022.103121,"While Machine Learning (ML) technologies are widely adopted in many mission critical fields to support intelligent decision-making, concerns remain about system resilience against ML-specific security attacks and privacy breaches as well as the trust that users have in these systems. In this article, we present our recent systematic and comprehensive survey on the state-of-the-art ML robustness and trustworthiness from a security engineering perspective, focusing on the problems in system threat analysis, design and evaluation faced in developing practical machine learning applications, in terms of robustness and user trust. Accordingly, we organize the presentation of this survey intended to facilitate the convey of the body of knowledge from this angle. We then describe a metamodel we created that represents the body of knowledge in a standard and visualized way. We further illustrate how to leverage the metamodel to guide a systematic threat analysis and security design process which extends and scales up the classic process. Finally, we propose the future research directions motivated by our findings. Our work differs itself from the existing surveys by (i) exploring the fundamental principles and best practices to support robust and trustworthy ML system development, and (ii) studying the interplay of robustness and user trust in the context of ML systems. We expect this survey provides a big picture for machine learning security practitioners."
491,4b64b789c4016cd76415c5d31e7d1f2253bc40fe,10.1136/bmjebm-2021-111892,No abstract found
493,66a34160908439b07eb727800f01251e204bbb59,10.1016/j.envint.2021.107025,"• Dextr is a semi-automated data extraction tool that can capture complex data. • Dextr connects extracted entities to support hierarchical data extraction. • Dextr supports development of annotated datasets within a standard review workflow. • Dextr’s user verification option assures user-driven semi-automated data extraction. There has been limited development and uptake of machine-learning methods to automate data extraction for literature-based assessments. Although advanced extraction approaches have been applied to some clinical research reviews, existing methods are not well suited for addressing toxicology or environmental health questions due to unique data needs to support reviews in these fields. To develop and evaluate a flexible, web-based tool for semi-automated data extraction that: 1) makes data extraction predictions with user verification, 2) integrates token-level annotations, and 3) connects extracted entities to support hierarchical data extraction. Dextr was developed with Agile software methodology using a two-team approach. The development team outlined proposed features and coded the software. The advisory team guided developers and evaluated Dextr’s performance on precision, recall, and extraction time by comparing a manual extraction workflow to a semi-automated extraction workflow using a dataset of 51 environmental health animal studies. The semi-automated workflow did not appear to affect precision rate (96.0% vs. 95.4% manual, p = 0.38), resulted in a small reduction in recall rate (91.8% vs. 97.0% manual, p < 0.01), and substantially reduced the median extraction time (436 s vs. 933 s per study manual, p < 0.01) compared to a manual workflow. Dextr provides similar performance to manual extraction in terms of recall and precision and greatly reduces data extraction time. Unlike other tools, Dextr provides the ability to extract complex concepts (e.g., multiple experiments with various exposures and doses within a single study), properly connect the extracted elements within a study, and effectively limit the work required by researchers to generate machine-readable, annotated exports. The Dextr tool addresses data-extraction challenges associated with environmental health sciences literature with a simple user interface, incorporates the key capabilities of user verification and entity connecting, provides a platform for further automation developments, and has the potential to improve data extraction for literature reviews in this and other fields."
502,45cd0aa193063ec3e674393f520a8a34e192a062,10.1002/9781119636113.ch12,No abstract found
503,7353853d46eccf8482547605efeed3aeaa9cfa1e,10.1186/s12911-021-01444-7,"Over the last decades, the face of health care has changed dramatically, with big improvements in what is technically feasible. However, there are indicators that the current approach to evaluating evidence in health care is not holistic and hence in the long run, health care will not be sustainable. New conceptual and normative frameworks for the evaluation of health care need to be developed and investigated. The current paper presents a novel framework of justifiable health care and explores how the use of artificial intelligence and big data can contribute to achieving the goals of this framework."
504,79cddcee2c54ee98ceaf7bfe621d6246cba9e031,10.1186/s13643-021-01665-x,"Living systematic reviews (LSRs) can expedite evidence synthesis by incorporating new evidence in real time. However, the methods needed to identify new studies in a timely manner are not well established. To explore the value of complementary search approaches in terms of search performance, impact on results and conclusions, screening workload, and feasibility compared to the reference standard. We developed three complementary search approaches for a systematic review on treatments for bronchiolitis: Automated Full Search, PubMed Similar Articles, and Scopus Citing References. These were automated to retrieve results monthly; pairs of reviewers screened the records and commented on feasibility. After 1 year, we conducted a full update search (reference standard). For each complementary approach, we compared search performance (proportion missed, number needed to read [NNR]) and reviewer workload (number of records screened, time required) to the reference standard. We investigated the impact of the new trials on the effect estimate and certainty of evidence for the primary outcomes. We summarized comments about feasibility. Via the reference standard, reviewers screened 505 titles/abstracts, 24 full texts, and identified four new trials (NNR 127; 12.4 h). Of the complementary approaches, only the Automated Full Search located all four trials; these were located 6 to 12 months sooner than via the reference standard but did not alter the results nor certainty in the evidence. The Automated Full Search was the most resource-intensive approach (816 records screened; NNR 204; 17.1 h). The PubMed Similar Articles and Scopus Citing References approaches located far fewer records (452 and 244, respectively), thereby requiring less screening time (9.4 and 5.2 h); however, each approach located only one of the four new trials. Reviewers found it feasible and convenient to conduct monthly screening for searches of this yield (median 15–65 records/month). The Automated Full Search was the most resource-intensive approach, but also the only to locate all of the newly published trials. Although the monthly screening time for the PubMed Similar Articles and Scopus Citing Articles was far less, most relevant records were missed. These approaches were feasible to integrate into reviewer work processes. Open Science Framework. https://doi.org/10.17605/OSF.IO/6M28H
 ."
509,bbde2b2f40d901b39dd5a6f68f988c191281914b,10.1186/s12874-021-01335-5,"Standard practice for conducting systematic reviews (SRs) is time consuming and involves the study team screening hundreds or thousands of citations. As the volume of medical literature grows, the citation set sizes and corresponding screening efforts increase. While larger team size and alternate screening methods have the potential to reduce workload and decrease SR completion times, it is unknown whether investigators adapt team size or methods in response to citation set sizes. Using a cross-sectional design, we sought to understand how citation set size impacts (1) the total number of authors or individuals contributing to screening and (2) screening methods. MEDLINE was searched in April 2019 for SRs on any health topic. A total of 1880 unique publications were identified and sorted into five citation set size categories (after deduplication):   10,000. A random sample of 259 SRs were selected (~ 50 per category) for data extraction and analysis. With the exception of the pairwise t test comparing the under 1000 and over 10,000 categories (median 5 vs. 6, p = 0.049) no statistically significant relationship was evident between author number and citation set size. While visual inspection was suggestive, statistical testing did not consistently identify a relationship between citation set size and number of screeners (title-abstract, full text) or data extractors. However, logistic regression identified investigators were significantly more likely to deviate from gold-standard screening methods (i.e. independent duplicate screening) with larger citation sets. For every doubling of citation size, the odds of using gold-standard screening decreased by 15 and 20% at title-abstract and full text review, respectively. Finally, few SRs reported using crowdsourcing (n = 2) or computer-assisted screening (n = 1). Large citation set sizes present a challenge to SR teams, especially when faced with time-sensitive health policy questions. Our study suggests that with increasing citation set size, authors are less likely to adhere to gold-standard screening methods. It is possible that adjunct screening methods, such as crowdsourcing (large team) and computer-assisted technologies, may provide a viable solution for authors to complete their SRs in a timely manner."
514,242d11b84abb8dee817fadb622c1935aa013be6e,10.1016/j.jhqr.2019.07.012,No abstract found
517,3ea255334ad869477fab667db8d9c6f1e3a6bc11,10.1186/s13643-020-01351-4,"The fourth meeting of the International Collaboration for Automation of Systematic Reviews (ICASR) was held 5–6 November 2019 in The Hague, the Netherlands. ICASR is an interdisciplinary group whose goal is to maximize the use of technology for conducting rapid, accurate, and efficient systematic reviews of scientific evidence. The group seeks to facilitate the development and acceptance of automated techniques for systematic reviews. In 2018, the major themes discussed were the transferability of automation tools (i.e., tools developed for other purposes that might be used by systematic reviewers), the automated recognition of study design in multiple disciplines and applications, and approaches for the evaluation of automation tools."
518,4f7b25600bd3908692a15041d5f179b3fa01e299,10.1186/s12874-020-01143-3,"Data extraction forms link systematic reviews with primary research and provide the foundation for appraising, analysing, summarising and interpreting a body of evidence. This makes their development, pilot testing and use a crucial part of the systematic reviews process. Several studies have shown that data extraction errors are frequent in systematic reviews, especially regarding outcome data. We reviewed guidance on the development and pilot testing of data extraction forms and the data extraction process. We reviewed four types of sources: 1) methodological handbooks of systematic review organisations (SRO); 2) textbooks on conducting systematic reviews; 3) method documents from health technology assessment (HTA) agencies and 4) journal articles. HTA documents were retrieved in February 2019 and database searches conducted in December 2019. One author extracted the recommendations and a second author checked them for accuracy. Results are presented descriptively. Our analysis includes recommendations from 25 documents: 4 SRO handbooks, 11 textbooks, 5 HTA method documents and 5 journal articles. Across these sources the most common recommendations on form development are to use customized or adapted standardised extraction forms (14/25); provide detailed instructions on their use (10/25); ensure clear and consistent coding and response options (9/25); plan in advance which data are needed (9/25); obtain additional data if required (8/25); and link multiple reports of the same study (8/25). The most frequent recommendations on piloting extractions forms are that forms should be piloted on a sample of studies (18/25); and that data extractors should be trained in the use of the forms (7/25). The most frequent recommendations on data extraction are that extraction should be conducted by at least two people (17/25); that independent parallel extraction should be used (11/25); and that procedures to resolve disagreements between data extractors should be in place (14/25). Overall, our results suggest a lack of comprehensiveness of recommendations. This may be particularly problematic for less experienced reviewers. Limitations of our method are the scoping nature of the review and that we did not analyse internal documents of health technology agencies."
519,5294a094208b6862175bd206d7f9083320e0b525,10.1007/s13278-020-00635-w,"In recent decades, researchers have realized that social networks are important sources for adhering to the evolution of many aspects of Information Retrieval (IR). These social networks have produced vast amounts of important information that are not covered by traditional IR systems. This improvement, which has become one of the main applications of IR, offers several social features such as the conversational exchange and the share of opinions by users, and the association of users with the same interests. This work introduces a model of Social Information Research that takes into account social information on users and exploits them as a second source of information for a given query. In the proposed IR system, the quality of results improved by the analysis of user’s needs and by comparing other users’ social data."
523,aa2870d01d1894f2511ae3f18e9b280ae69143f3,10.1186/s12874-020-01031-w,"We investigated the feasibility of using a machine learning tool’s relevance predictions to expedite title and abstract screening. We subjected 11 systematic reviews and six rapid reviews to four retrospective screening simulations (automated and semi-automated approaches to single-reviewer and dual independent screening) in Abstrackr, a freely-available machine learning software. We calculated the proportion missed, workload savings, and time savings compared to single-reviewer and dual independent screening by human reviewers. We performed cited reference searches to determine if missed studies would be identified via reference list scanning. For systematic reviews, the semi-automated, dual independent screening approach provided the best balance of time savings (median (range) 20 (3–82) hours) and reliability (median (range) proportion missed records, 1 (0–14)%). The cited references search identified 59% (n = 10/17) of the records missed. For the rapid reviews, the fully and semi-automated approaches saved time (median (range) 9 (2–18) hours and 3 (1–10) hours, respectively), but less so than for the systematic reviews. The median (range) proportion missed records for both approaches was 6 (0–22)%. Using Abstrackr to assist one of two reviewers in systematic reviews saves time with little risk of missing relevant records. Many missed records would be identified via other means."
526,f107248324e09a3abee45e31199e5c9b69c85bdc,10.1186/s12874-020-01129-1,"Systematic reviews often require substantial resources, partially due to the large number of records identified during searching. Although artificial intelligence may not be ready to fully replace human reviewers, it may accelerate and reduce the screening burden. Using DistillerSR (May 2020 release), we evaluated the performance of the prioritization simulation tool to determine the reduction in screening burden and time savings. Using a true recall @ 95%, response sets from 10 completed systematic reviews were used to evaluate: (i) the reduction of screening burden; (ii) the accuracy of the prioritization algorithm; and (iii) the hours saved when a modified screening approach was implemented. To account for variation in the simulations, and to introduce randomness (through shuffling the references), 10 simulations were run for each review. Means, standard deviations, medians and interquartile ranges (IQR) are presented. Among the 10 systematic reviews, using true recall @ 95% there was a median reduction in screening burden of 47.1% (IQR: 37.5 to 58.0%). A median of 41.2% (IQR: 33.4 to 46.9%) of the excluded records needed to be screened to achieve true recall @ 95%. The median title/abstract screening hours saved using a modified screening approach at a true recall @ 95% was 29.8 h (IQR: 28.1 to 74.7 h). This was increased to a median of 36 h (IQR: 32.2 to 79.7 h) when considering the time saved not retrieving and screening full texts of the remaining 5% of records not yet identified as included at title/abstract. Among the 100 simulations (10 simulations per review), none of these 5% of records were a final included study in the systematic review. The reduction in screening burden to achieve true recall @ 95% compared to @ 100% resulted in a reduced screening burden median of 40.6% (IQR: 38.3 to 54.2%). The prioritization tool in DistillerSR can reduce screening burden. A modified or stop screening approach once a true recall @ 95% is achieved appears to be a valid method for rapid reviews, and perhaps systematic reviews. This needs to be further evaluated in prospective reviews using the estimated recall."
527,fb63b21b7c8bfbe7333981d85575d4185062839b,10.1186/s12874-020-01004-z,"Rapid reviews (RRs) have emerged as an efficient alternative to time-consuming systematic reviews—they can help meet the demand for accelerated evidence synthesis to inform decision-making in healthcare. The synthesis of diagnostic evidence has important methodological challenges. Here, we performed an international survey to identify the current practice of producing RRs for diagnostic tests. We developed and administered an online survey inviting institutions that perform RRs of diagnostic tests from all over the world. All participants (N = 25) reported the implementation of one or more methods to define the scope of the RR; however, only one strategy (defining a structured question) was used by ≥90% of participants. All participants used at least one methodological shortcut including the use of a previous review as a starting point (92%) and the use of limits on the search (96%). Parallelization and automation of review tasks were not extensively used (48 and 20%, respectively). Our survey indicates a greater use of shortcuts and limits for conducting diagnostic test RRs versus the results of a recent scoping review analyzing published RRs. Several shortcuts are used without knowing how their implementation affects the results of the evidence synthesis in the setting of diagnostic test reviews. Thus, a structured evaluation of the challenges and implications of the adoption of these RR methods is warranted."
528,273c392b7a59b5d5aabdff11acd35e3762c4c62d,10.1186/s13643-019-1221-3,"Web applications that employ natural language processing technologies to support systematic reviewers during abstract screening have become more common. The goal of our project was to conduct a case study to explore a screening approach that temporarily replaces a human screener with a semi-automated screening tool. We evaluated the accuracy of the approach using DistillerAI as a semi-automated screening tool. A published comparative effectiveness review served as the reference standard. Five teams of professional systematic reviewers screened the same 2472 abstracts in parallel. Each team trained DistillerAI with 300 randomly selected abstracts that the team screened dually. For all remaining abstracts, DistillerAI replaced one human screener and provided predictions about the relevance of records. A single reviewer also screened all remaining abstracts. A second human screener resolved conflicts between the single reviewer and DistillerAI. We compared the decisions of the machine-assisted approach, single-reviewer screening, and screening with DistillerAI alone against the reference standard. The combined sensitivity of the machine-assisted screening approach across the five screening teams was 78% (95% confidence interval [CI], 66 to 90%), and the combined specificity was 95% (95% CI, 92 to 97%). By comparison, the sensitivity of single-reviewer screening was similar (78%; 95% CI, 66 to 89%); however, the sensitivity of DistillerAI alone was substantially worse (14%; 95% CI, 0 to 31%) than that of the machine-assisted screening approach. Specificities for single-reviewer screening and DistillerAI were 94% (95% CI, 91 to 97%) and 98% (95% CI, 97 to 100%), respectively. Machine-assisted screening and single-reviewer screening had similar areas under the curve (0.87 and 0.86, respectively); by contrast, the area under the curve for DistillerAI alone was just slightly better than chance (0.56). The interrater agreement between human screeners and DistillerAI with a prevalence-adjusted kappa was 0.85 (95% CI, 0.84 to 0.86%). The accuracy of DistillerAI is not yet adequate to replace a human screener temporarily during abstract screening for systematic reviews. Rapid reviews, which do not require detecting the totality of the relevant evidence, may find semi-automation tools to have greater utility than traditional systematic reviews."
529,84ea330b06f1ac83e6d389b7f25d0fca3280ca27,10.1186/s13643-019-0974-z,No abstract found
530,9716a25cb7bb88ca5113f0e9d80376c6a00f6652,,No DOI detected
531,cbd019af5ae3b8f8d2ed645461a4cf084a2aadfc,10.1186/s13643-019-1222-2,"We explored the performance of three machine learning tools designed to facilitate title and abstract screening in systematic reviews (SRs) when used to (a) eliminate irrelevant records (automated simulation) and (b) complement the work of a single reviewer (semi-automated simulation). We evaluated user experiences for each tool. We subjected three SRs to two retrospective screening simulations. In each tool (Abstrackr, DistillerSR, RobotAnalyst), we screened a 200-record training set and downloaded the predicted relevance of the remaining records. We calculated the proportion missed and workload and time savings compared to dual independent screening. To test user experiences, eight research staff tried each tool and completed a survey. Using Abstrackr, DistillerSR, and RobotAnalyst, respectively, the median (range) proportion missed was 5 (0 to 28) percent, 97 (96 to 100) percent, and 70 (23 to 100) percent for the automated simulation and 1 (0 to 2) percent, 2 (0 to 7) percent, and 2 (0 to 4) percent for the semi-automated simulation. The median (range) workload savings was 90 (82 to 93) percent, 99 (98 to 99) percent, and 85 (85 to 88) percent for the automated simulation and 40 (32 to 43) percent, 49 (48 to 49) percent, and 35 (34 to 38) percent for the semi-automated simulation. The median (range) time savings was 154 (91 to 183), 185 (95 to 201), and 157 (86 to 172) hours for the automated simulation and 61 (42 to 82), 92 (46 to 100), and 64 (37 to 71) hours for the semi-automated simulation. Abstrackr identified 33–90% of records missed by a single reviewer. RobotAnalyst performed less well and DistillerSR provided no relative advantage. User experiences depended on user friendliness, qualities of the user interface, features and functions, trustworthiness, ease and speed of obtaining predictions, and practicality of the export file(s). The workload savings afforded in the automated simulation came with increased risk of missing relevant records. Supplementing a single reviewer’s decisions with relevance predictions (semi-automated simulation) sometimes reduced the proportion missed, but performance varied by tool and SR. Designing tools based on reviewers’ self-identified preferences may improve their compatibility with present workflows. Not applicable."
533,090a6772a1d69f07bfe7e89f99934294a0dac1b9,10.1109/tsmc.1976.4309452,No abstract found
538,122ba8c1be8c382e0c3f026e9a45e0cec4950646,10.1016/j.artmed.2010.10.005,"Objective: To determine whether the automatic classification of documents can be useful in systematic reviews on medical topics, and specifically if the performance of the automatic classification can be enhanced by using the particular protocol of questions employed by the human reviewers to create multiple classifiers. Methods and materials: The test collection is the data used in large-scale systematic review on the topic of the dissemination strategy of health care services for elderly people. From a group of 47,274 abstracts marked by human reviewers to be included in or excluded from further screening, we randomly selected 20,000 as a training set, with the remaining 27,274 becoming a separate test set. As a machine learning algorithm we used complement naive Bayes. We tested both a global classification method, where a single classifier is trained on instances of abstracts and their classification (i.e., included or excluded), and a novel per-question classification method that trains multiple classifiers for each abstract, exploiting the specific protocol (questions) of the systematic review. For the per-question method we tested four ways of combining the results of the classifiers trained for the individual questions. As evaluation measures, we calculated precision and recall for several settings of the two methods. It is most important not to exclude any relevant documents (i.e., to attain high recall for the class of interest) but also desirable to exclude most of the non-relevant documents (i.e., to attain high precision on the class of interest) in order to reduce human workload. Results: For the global method, the highest recall was 67.8% and the highest precision was 37.9%. For the per-question method, the highest recall was 99.2%, and the highest precision was 63%. The human-machine workflow proposed in this paper achieved a recall value of 99.6%, and a precision value of 17.8%. Conclusion: The per-question method that combines classifiers following the specific protocol of the review leads to better results than the global method in terms of recall. Because neither method is efficient enough to classify abstracts reliably by itself, the technology should be applied in a semi-automatic way, with a human expert still involved. When the workflow includes one human expert and the trained automatic classifier, recall improves to an acceptable level, showing that automatic classification techniques can reduce the human workload in the process of building a systematic review."
539,1375acf8ab165034648564137c2ded9401fd9044,10.1186/2046-4053-1-28,"This paper argues that the current proliferation of types of systematic reviews creates challenges for the terminology for describing such reviews. Terminology is necessary for planning, describing, appraising, and using reviews, building infrastructure to enable the conduct and use of reviews, and for further developing review methodology. There is insufficient consensus on terminology for a typology of reviews to be produced and any such attempt is likely to be limited by the overlapping nature of the dimensions along which reviews vary. It is therefore proposed that the most useful strategy for the field is to develop terminology for the main dimensions of variation. Three such main dimensions are proposed: (1) aims and approaches (including what the review is aiming to achieve, the theoretical and ideological assumptions, and the use of theory and logics of aggregation and configuration in synthesis); (2) structure and components (including the number and type of mapping and synthesis components and how they relate); and (3) breadth and depth and the extent of ‘work done’ in addressing a research issue (including the breadth of review questions, the detail with which they are addressed, and the amount the review progresses a research agenda). This then provides an overarching strategy to encompass more detailed descriptions of methodology and may lead in time to a more overarching system of terminology for systematic reviews."
540,17a4cf66d4b87ac4a8e768b9f5e6c8b13acaff6b,10.1186/1471-2105-11-492,"An increase in work on the full text of journal articles and the growth of PubMedCentral have the opportunity to create a major paradigm shift in how biomedical text mining is done. However, until now there has been no comprehensive characterization of how the bodies of full text journal articles differ from the abstracts that until now have been the subject of most biomedical text mining research. We examined the structural and linguistic aspects of abstracts and bodies of full text articles, the performance of text mining tools on both, and the distribution of a variety of semantic classes of named entities between them. We found marked structural differences, with longer sentences in the article bodies and much heavier use of parenthesized material in the bodies than in the abstracts. We found content differences with respect to linguistic features. Three out of four of the linguistic features that we examined were statistically significantly differently distributed between the two genres. We also found content differences with respect to the distribution of semantic features. There were significantly different densities per thousand words for three out of four semantic classes, and clear differences in the extent to which they appeared in the two genres. With respect to the performance of text mining tools, we found that a mutation finder performed equally well in both genres, but that a wide variety of gene mention systems performed much worse on article bodies than they did on abstracts. POS tagging was also more accurate in abstracts than in article bodies. Aspects of structure and content differ markedly between article abstracts and article bodies. A number of these differences may pose problems as the text mining field moves more into the area of processing full-text articles. However, these differences also present a number of opportunities for the extraction of data types, particularly that found in parenthesized text, that is present in article bodies but not in article abstracts."
547,28f64a2eb8f720a54fd82417430fe5fe6a342412,10.1186/1471-2288-6-7,"Most electronic search efforts directed at identifying primary studies for inclusion in systematic reviews rely on the optimal Boolean search features of search interfaces such as DIALOG® and Ovid™. Our objective is to test the ability of an Ultraseek® search engine to rank MEDLINE® records of the included studies of Cochrane reviews within the top half of all the records retrieved by the Boolean MEDLINE search used by the reviewers. Collections were created using the MEDLINE bibliographic records of included and excluded studies listed in the review and all records retrieved by the MEDLINE search. Records were converted to individual HTML files. Collections of records were indexed and searched through a statistical search engine, Ultraseek, using review-specific search terms. Our data sources, systematic reviews published in the Cochrane library, were included if they reported using at least one phase of the Cochrane Highly Sensitive Search Strategy (HSSS), provided citations for both included and excluded studies and conducted a meta-analysis using a binary outcome measure. Reviews were selected if they yielded between 1000–6000 records when the MEDLINE search strategy was replicated. Nine Cochrane reviews were included. Included studies within the Cochrane reviews were found within the first 500 retrieved studies more often than would be expected by chance. Across all reviews, recall of included studies into the top 500 was 0.70. There was no statistically significant difference in ranking when comparing included studies with just the subset of excluded studies listed as excluded in the published review. The relevance ranking provided by the search engine was better than expected by chance and shows promise for the preliminary evaluation of large results from Boolean searches. A statistical search engine does not appear to be able to make fine discriminations concerning the relevance of bibliographic records that have been pre-screened by systematic reviewers."
559,3cd74ba47376ddef505672d255a99917048ec1a5,10.1038/gim.2012.7,Toward modernizing the systematic review pipeline in genetics: efficient updating via data mining
561,488a3f9092093d4f52b25df4342ec79f0327a325,,No DOI detected
570,61372320baa219ab53b28bdc489cb4e063f57e52,10.1186/1471-2105-9-205,"Synthesis of data from published human genetic association studies is a critical step in the translation of human genome discoveries into health applications. Although genetic association studies account for a substantial proportion of the abstracts in PubMed, identifying them with standard queries is not always accurate or efficient. Further automating the literature-screening process can reduce the burden of a labor-intensive and time-consuming traditional literature search. The Support Vector Machine (SVM), a well-established machine learning technique, has been successful in classifying text, including biomedical literature. The GAPscreener, a free SVM-based software tool, can be used to assist in screening PubMed abstracts for human genetic association studies. The data source for this research was the HuGE Navigator, formerly known as the HuGE Pub Lit database. Weighted SVM feature selection based on a keyword list obtained by the two-way z score method demonstrated the best screening performance, achieving 97.5% recall, 98.3% specificity and 31.9% precision in performance testing. Compared with the traditional screening process based on a complex PubMed query, the SVM tool reduced by about 90% the number of abstracts requiring individual review by the database curator. The tool also ascertained 47 articles that were missed by the traditional literature screening process during the 4-week test period. We examined the literature on genetic associations with preterm birth as an example. Compared with the traditional, manual process, the GAPscreener both reduced effort and improved accuracy. GAPscreener is the first free SVM-based application available for screening the human genetic association literature in PubMed with high recall and specificity. The user-friendly graphical user interface makes this a practical, stand-alone application. The software can be downloaded at no charge."
571,683c7cdc698804bf5c37dd61bace5e6b2c408068,10.1007/978-3-642-13059-5_33,The purpose of this work is to improve on the selection of algorithms for classifier committees applied to reducing the workload of human experts in building systematic reviews used in evidence-based medicine We focus on clustering pre-selected classifiers based on a multi-measure prediction performance evaluation expressed in terms of a projection from a high-dimensional space to a visualizable two-dimensional one The best classifier was selected from each cluster and included in the committee We applied the committee of classifiers to rank biomedical abstracts based on the predicted relevance to the topic under review We identified a subset of abstracts that represents the bottom of the ranked list (predicted as irrelevant) We used False Negatives (relevant articles mistakenly ranked at the bottom) as a final performance measure Our early experiments demonstrate that the classifier committee built using our new approach outperformed committees of classifiers arbitrary created from the same list of pre-selected classifiers.
575,74b8583fe4f590ccdef82df42d8a0341b54f1dad,10.1016/j.ins.2012.05.027,"Evidence-based medicine has recently received a large amount of attention in medical research. To help clinical practices use evidence-based medicine, it should be easy to find the best current evidence that is relevant to the clinical question and has high methodological quality. However, searching for relevant articles and appraising their validity is demanding work for most clinicians. We hypothesize that, through an effective design that addresses the two major aspects - relevance and quality - together with a ranking algorithm, search engines can automatically retrieve articles that are relevant to clinical questions and are based on valid evidence. The contribution of this study has two parts. First, we approach this problem by combining methodologies. After designing a suitable document query-relevance score and methodological quality score, we combined them using various fusion methods. The result was a twofold increase in the mean average precision. Second, for correct evaluation, we built a test collection using a preexisting reliable database, the Cochrane Reviews, which allowed robust and comprehensive evaluation."
576,7749604f60fbc4dcc070d0359eb7bb32f6c6d681,10.1016/j.jbi.2014.06.005,"Display Omitted Active learning is promising in the areas with complex topics in systematic reviews.Certainty criteria is promising to accelerate screening regardless of the topic.Certainty criteria performs as well as uncertainty criteria in classification.Weighting positive instances is promising to overcome the data imbalance.Unsupervised methods enhance the classification performance. In systematic reviews, the growing number of published studies imposes a significant screening workload on reviewers. Active learning is a promising approach to reduce the workload by automating some of the screening decisions, but it has been evaluated for a limited number of disciplines. The suitability of applying active learning to complex topics in disciplines such as social science has not been studied, and the selection of useful criteria and enhancements to address the data imbalance problem in systematic reviews remains an open problem. We applied active learning with two criteria (certainty and uncertainty) and several enhancements in both clinical medicine and social science (specifically, public health) areas, and compared the results in both. The results show that the certainty criterion is useful for finding relevant documents, and weighting positive instances is promising to overcome the data imbalance problem in both data sets. Latent dirichlet allocation (LDA) is also shown to be promising when little manually-assigned information is available. Active learning is effective in complex topics, although its efficiency is limited due to the difficulties in text classification. The most promising criterion and weighting method are the same regardless of the review topic, and unsupervised techniques like LDA have a possibility to boost the performance of active learning without manual annotation."
584,8fc8201dbd1d6ef274352a578e1ada637894bc30,10.1007/978-3-642-37456-2_24,"Class imbalance is one of the challenging problems for machine learning in many real-world applications. Cost-sensitive learning has attracted significant attention in recent years to solve the problem, but it is difficult to determine the precise misclassification costs in practice. There are also other factors that influence the performance of the classification including the input feature subset and the intrinsic parameters of the classifier. This paper presents an effective wrapper framework incorporating the evaluation measure (AUC and G-mean) into the objective function of cost sensitive SVM directly to improve the performance of classification by simultaneously optimizing the best pair of feature subset, intrinsic parameters and misclassification cost parameters. Experimental results on various standard benchmark datasets and real-world data with different ratios of imbalance show that the proposed method is effective in comparison with commonly used sampling techniques."
593,ae3eff1b20ec8b4b4a8e06237a32275fa3bee72c,10.1016/J.CLSR.2014.01.009,"Abstract “Text mining” covers a range of techniques that allow software to extract information from text documents. It is not a new technology, but it has recently received spotlight attention due to the emergence of Big Data. The applications of text mining are very diverse and span multiple disciplines, ranging from biomedicine to legal, business intelligence and security. From a legal perspective, text mining touches upon several areas of law, including contract law, copyright law and database law. This contribution discusses the legal issues encountered during the assembly of texts into so-called “corpora”, as well as the use of such corpora."
599,ba5e070e797dd2c09061e023758d76878a1cc4b7,,No DOI detected
607,d3077a3461b53c129abe73ba2aa1694966be6bfc,10.1007/978-3-642-01818-3_29,"The purpose of this work is to reduce the workload of human experts in building systematic reviews from published articles, used in evidence-based medicine. We propose to use a committee of classifiers to rank biomedical abstracts based on the predicted relevance to the topic under review. In our approach, we identify two subsets of abstracts: one that represents the top, and another that represents the bottom of the ranked list. These subsets, identified using machine learning (ML) techniques, are considered zones where abstracts are labeled with high confidence as relevant or irrelevant to the topic of the review. Early experiments with this approach using different classifiers and different representation techniques show significant workload reduction."
609,d468d1b8861186de09b0bfb4f9755678c21bad59,10.1016/j.artmed.2012.05.002,"Objectives: To investigate whether (1) machine learning classifiers can help identify nonrandomized studies eligible for full-text screening by systematic reviewers; (2) classifier performance varies with optimization; and (3) the number of citations to screen can be reduced. Methods: We used an open-source, data-mining suite to process and classify biomedical citations that point to mostly nonrandomized studies from 2 systematic reviews. We built training and test sets for citation portions and compared classifier performance by considering the value of indexing, various feature sets, and optimization. We conducted our experiments in 2 phases. The design of phase I with no optimization was: 4 classifiersx3 feature setsx3 citation portions. Classifiers included k-nearest neighbor, naive Bayes, complement naive Bayes, and evolutionary support vector machine. Feature sets included bag of words, and 2- and 3-term n-grams. Citation portions included titles, titles and abstracts, and full citations with metadata. Phase II with optimization involved a subset of the classifiers, as well as features extracted from full citations, and full citations with overweighted titles. We optimized features and classifier parameters by manually setting information gain thresholds outside of a process for iterative grid optimization with 10-fold cross-validations. We independently tested models on data reserved for that purpose and statistically compared classifier performance on 2 types of feature sets. We estimated the number of citations needed to screen by reviewers during a second pass through a reduced set of citations. Results: In phase I, the evolutionary support vector machine returned the best recall for bag of words extracted from full citations; the best classifier with respect to overall performance was k-nearest neighbor. No classifier attained good enough recall for this task without optimization. In phase II, we boosted performance with optimization for evolutionary support vector machine and complement naive Bayes classifiers. Generalization performance was better for the latter in the independent tests. For evolutionary support vector machine and complement naive Bayes classifiers, the initial retrieval set was reduced by 46% and 35%, respectively. Conclusions: Machine learning classifiers can help identify nonrandomized studies eligible for full-text screening by systematic reviewers. Optimization can markedly improve performance of classifiers. However, generalizability varies with the classifier. The number of citations to screen during a second independent pass through the citations can be substantially reduced."
614,de186a9bb0ccb6a88a83061a27ce73a53b6604c7,,No DOI detected
622,f99642b853f500f63e3c92f8c04911051cea97e7,10.1016/j.jbi.2009.11.001,"Massive increases in electronically available text have spurred a variety of natural language processing methods to automatically identify relationships from text; however, existing annotated collections comprise only bioinformatics (gene-protein) or clinical informatics (treatment-disease) relationships. This paper introduces the Claim Framework that reflects how authors across biomedical spectrum communicate findings in empirical studies. The Framework captures different levels of evidence by differentiating between explicit and implicit claims, and by capturing under-specified claims such as correlations, comparisons, and observations. The results from 29 full-text articles show that authors report fewer than 7.84% of scientific claims in an abstract, thus revealing the urgent need for text mining systems to consider the full-text of an article rather than just the abstract. The results also show that authors typically report explicit claims (77.12%) rather than an observations (9.23%), correlations (5.39%), comparisons (5.11%) or implicit claims (2.7%). Informed by the initial manual annotations, we introduce an automated approach that uses syntax and semantics to identify explicit claims automatically and measure the degree to which each feature contributes to the overall precision and recall. Results show that a combination of semantics and syntax is required to achieve the best system performance."
624,fee2640df2347212418eeb19d8a19851ec0dd9b0,10.1016/j.knosys.2014.01.021,"This paper introduces a framework that allows to mitigate the impact of class imbalance on most scalar performance measures when used to evaluate the behavior of classifiers. Formally, a correction function is defined with the aim of highlighting those classification results that present moderately higher prediction rates on the minority class. Besides, this function punishes those scenarios that are biased towards the majority class, but also those that are strongly biased to favor the minority class. This strategy assumes a typical imbalance task, in which the minority class contains the most relevant samples to the research purposes. A novel experimental framework is designed to show the advantages of our approach when compared to the standard use of well-established measures, demonstrating its consistency and validity."
626,ffcf75cd8e5ec6cbbb7057994cf2e1300a7f2b35,10.1016/j.infsof.2012.04.003,"Context: Systematic Literature Reviews (SLRs) are an important component to identify and aggregate research evidence from different empirical studies. Despite its relevance, most of the process is conducted manually, implying additional effort when the Selection Review task is performed and leading to reading all studies under analysis more than once. Objective: We propose an approach based on Visual Text Mining (VTM) techniques to assist the Selection Review task in SLR. It is implemented into a VTM tool (Revis), which is freely available for use. Method: We have selected and implemented appropriate visualization techniques into our approach and validated and demonstrated its usefulness in performing real SLRs. Results: The results have shown that employment of VTM techniques can successfully assist in the Selection Review task, speeding up the entire SLR process in comparison to the conventional approach. Conclusion: VTM techniques are valuable tools to be used in the context of selecting studies in the SLR process, prone to speed up some stages of SLRs."
632,099f8bf42b95a4c239bc338b05821b592c60f44b,10.1038/s41746-022-00589-7,"Abstract Mental illness is highly prevalent nowadays, constituting a major cause of distress in people’s life with impact on society’s health and well-being. Mental illness is a complex multi-factorial disease associated with individual risk factors and a variety of socioeconomic, clinical associations. In order to capture these complex associations expressed in a wide variety of textual data, including social media posts, interviews, and clinical notes, natural language processing (NLP) methods demonstrate promising improvements to empower proactive mental healthcare and assist early diagnosis. We provide a narrative review of mental illness detection using NLP in the past decade, to understand methods, trends, challenges and future directions. A total of 399 studies from 10,467 records were included. The review reveals that there is an upward trend in mental illness detection NLP research. Deep learning methods receive more attention and perform better than traditional machine learning methods. We also provide some recommendations for future studies, including the development of novel detection methods, deep learning paradigms and interpretable models."
637,49983f7fa5eaf068aa6c48f85a53dae29f7009e3,10.1002/cl2.1214,"Background Across the globe, gender disparities still exist with regard to equitable access to resources, participation in decision-making processes, and gender and sexual-based violence. This is particularly true in fragile and conflict-affected settings, where women and girls are affected by both fragility and conflict in unique ways. While women have been acknowledged as key actors in peace processes and post-conflict reconstruction (e.g., through the United Nations Security Council Resolution 1325 and the Women, Peace and Security Agenda) evidence on the effectiveness of gender-specific and gender-transformative interventions to improve women's empowerment in fragile and conflict-affected states and situations (FCAS) remains understudied. Objectives The purpose of this review was to synthesize the body of evidence around gender-specific and gender-transformative interventions aimed at improving women's empowerment in fragile and conflict-affected settings with high levels of gender inequality. We also aimed to identify barriers and facilitators that could affect the effectiveness of these interventions and to provide implications for policy, practice and research designs within the field of transitional aid. Methods We searched for and screened over 100,000 experimental and quasi-experimental studies focused on FCAS at the individual and community levels. We used standard methodological procedures outlined by the Campbell Collaboration for the data collection and analysis, including quantitative and qualitative analyses, and completed the Grading of Recommendations, Assessment, Development and Evaluations (GRADE) methodology to assess the certainty around each body of evidence. Results We identified 104 impact evaluations (75% randomised controlled trials) assessing the effects of 14 different types of interventions in FCAS. About 28% of included studies were assessed as having a high risk of bias (45% among quasi-experimental designs). Interventions supporting women's empowerment and gender equality in FCAS produced positive effects on the outcomes related to the primary focus of the intervention. There are no significant negative effects of any included interventions. However, we observe smaller effects on behavioural outcomes further along the causal chain of empowerment. Qualitative syntheses indicated that gender norms and practices are potential barriers to intervention effectiveness, while working with local powers and institutions can facilitate the uptake and legitimacy of interventions. Conclusions We observe gaps of rigorous evidence in certain regions (notably MENA and Latin America) and in interventions specifically targeting women as actors of peacebuilding. Gender norms and practices are important elements to consider in programme design and implementation to maximise potential benefits: focusing on empowerment only might not be enough in the absence of targeting the restrictive gender norms and practices that may undermine intervention effectiveness. Lastly, programme designers and implementation should consider explicitly targeting specific empowerment outcomes, promoting social capital and exchange, and tailoring the intervention components to the desired empowerment-related outcomes."
639,515804f633dfecf2d4dd34f5d44aa64f99236d62,10.1186/s12874-022-01649-y,No DOI detected
641,5c407cef8315847c0abaa2ae43db481f4ef99929,10.1186/s13750-022-00268-w,"Abstract Background Natural climate solutions (NCS)—actions to conserve, restore, and modify natural and modified ecosystems to increase carbon storage or avoid greenhouse gas (GHG) emissions—are increasingly regarded as important pathways for climate change mitigation, while contributing to our global conservation efforts, overall planetary resilience, and sustainable development goals. Recently, projections posit that terrestrial-based NCS can potentially capture or avoid the emission of at least 11 Gt (gigatons) of carbon dioxide equivalent a year, or roughly encompassing one third of the emissions reductions needed to meet the Paris Climate Agreement goals by 2030. NCS interventions also purport to provide co-benefits such as improved productivity and livelihoods from sustainable natural resource management, protection of locally and culturally important natural areas, and downstream climate adaptation benefits. Attention on implementing NCS to address climate change across global and national agendas has grown—however, clear understanding of which types of NCS interventions have undergone substantial study versus those that require additional evidence is still lacking. This study aims to conduct a systematic map to collate and describe the current state, distribution, and methods used for evidence on the links between NCS interventions and climate change mitigation outcomes within tropical and sub-tropical terrestrial ecosystems. Results of this study can be used to inform program and policy design and highlight critical knowledge gaps where future evaluation, research, and syntheses are needed. Methods To develop this systematic map, we will search two bibliographic databases (including 11 indices) and 67 organization websites, backward citation chase from 39 existing evidence syntheses, and solicit information from key informants. All searches will be conducted in English and encompass subtropical and tropical terrestrial ecosystems (forests, grasslands, mangroves, agricultural areas). Search results will be screened at title and abstract, and full text levels, recording both the number of excluded articles and reasons for exclusion. Key meta-data from included articles will be coded and reported in a narrative review that will summarize trends in the evidence base, assess gaps in knowledge, and provide insights for policy, practice, and research. The data from this systematic map will be made open access."
645,981d613b0b3a8a87c9f33d700d125a74fbf835d6,10.1016/j.jbi.2022.104013,"The paper presents a conceptual framework for building practically applicable clinical decision support systems (CDSSs) using data-driven (DD) predictive modelling. With the proposed framework we have tried to fill the gap between experimental CDSS implementations widely covered in the literature and solutions acceptable by physicians in daily practice. The framework is based on a three-stage approach where DD model definition is accomplished with practical norms referencing (scales, clinical recommendations, etc.) and explanation of the prediction results and recommendations. The approach is aimed at increasing the applicability of CDSSs based on DD models through better integration into decision context and higher explainability. The approach has been implemented in software solutions and tested within a case study in type 2 diabetes mellitus (T2DM) prediction, enabling us to improve known clinical scales (such as FINDRISK) while keeping the problem-specific reasoning interface similar to existing applications. A survey was performed to assess and investigate the acceptance level and provide insights on the influences of the introduced framework's element on physicians' behavior."
647,9f55c0b090d0de3521b8af6cfe3f36e43c311784,10.1016/S2666-5247(21)00249-4,"<h2>Summary</h2><h3>Background</h3> The increase in artemisinin resistance threatens malaria elimination in Asia by the target date of 2030 and could derail control efforts in other endemic regions. This study aimed to develop up-to-date spatial distribution visualisations of the <i>kelch13</i> (<i>K13</i>) gene markers of artemisinin resistance in <i>Plasmodium falciparum</i> for policy makers. <h3>Methods</h3> In this systematic review and spatiotemporal analysis we used the WorldWide Antimalarial Resistance Network (WWARN) surveyor molecular markers of artemisinin resistance database. We updated the database by searching PubMed and SCOPUS for studies published between Jan 1, 1990, and March 31, 2021. Articles were included if they contained data on <i>K13</i> markers of artemisinin resistance from patients' samples in Asia and articles already included in the WWARN database were excluded. Data were extracted from the published articles and authors were contacted when information was missing. We used the lowest administrative unit levels for the sampling locations of all the <i>K13</i> data to describe the spatiotemporal distribution. The numbers of samples tested and those with each molecular marker in each administrative unit level were aggregated by year to calculate the marker prevalence over time. <h3>Findings</h3> Data were collated from 72 studies comprising <i>K13</i> markers from 16 613 blood samples collected from 1991 to 2020 from 18 countries. Most samples were from Myanmar (3842 [23·1%]), Cambodia (3804 [22·9%]), and Vietnam (2663 [16·0%]). The median time between data collection and publication was 3·6 years (range 0·9–25·0, IQR 2·7 [2·5–5·2]). There was a steady increase in the prevalence of WHO-validated <i>K13</i> markers, with the lowest of 4·3% in 2005 (n=47) and the highest of 62·9% in 2018 (n=264). Overall, the prevalence of Cys580Tyr mutation increased from 48·9% in 2002 to 84·9% in 2018. <h3>Interpretation</h3> From 2002 to 2018, there has been a steady increase in geographical locations and the proportion of infected people with validated artemisinin resistance markers. More consistent data collection, over more extended periods in the same areas with the rapid sharing of data are needed to map the spread and evolution of resistance to better inform policy decisions. Data in the literature are reported in a heterogeneous way leading to difficulties in pooling and interpretation. We propose here a tool with a set of minimum criteria for reporting future studies. <h3>Funding</h3> This research was funded in part by the Wellcome Trust."
652,c4fd386ab3a32732bdf867ea60ec5547005b1658,10.1186/s13643-021-01880-6,"This study developed, calibrated and evaluated a machine learning (ML) classifier designed to reduce study identification workload in maintaining the Cochrane COVID-19 Study Register (CCSR), a continuously updated register of COVID-19 research studies.A ML classifier for retrieving COVID-19 research studies (the 'Cochrane COVID-19 Study Classifier') was developed using a data set of title-abstract records 'included' in, or 'excluded' from, the CCSR up to 18th October 2020, manually labelled by information and data curation specialists or the Cochrane Crowd. The classifier was then calibrated using a second data set of similar records 'included' in, or 'excluded' from, the CCSR between October 19 and December 2, 2020, aiming for 99% recall. Finally, the calibrated classifier was evaluated using a third data set of similar records 'included' in, or 'excluded' from, the CCSR between the 4th and 19th of January 2021.The Cochrane COVID-19 Study Classifier was trained using 59,513 records (20,878 of which were 'included' in the CCSR). A classification threshold was set using 16,123 calibration records (6005 of which were 'included' in the CCSR) and the classifier had a precision of 0.52 in this data set at the target threshold recall >0.99. The final, calibrated COVID-19 classifier correctly retrieved 2285 (98.9%) of 2310 eligible records but missed 25 (1%), with a precision of 0.638 and a net screening workload reduction of 24.1% (1113 records correctly excluded).The Cochrane COVID-19 Study Classifier reduces manual screening workload for identifying COVID-19 research studies, with a very low and acceptable risk of missing eligible studies. It is now deployed in the live study identification workflow for the Cochrane COVID-19 Study Register."
653,d5a9e8ada224e51d907bcc26a1cb077f559bf33e,10.35713/aic.v3.i1.1,No abstract found
657,075298ab1fcf923d7b81d73727ede4e60ab28a8c,10.1016/j.jcpo.2021.100313,"Abstract Background The rapid growth in cancer research continues expanding the literature. Text mining approaches help make sense of large bodies of scientific literature and integrate the mounting data into the health care delivery system. Our objective is to generate a comprehensive understanding of the themes and trends in cancer research. Methods We employed a three-step text mining process of corpus generation and term-list creation and analysis, including latent semantic analysis for dimension reduction and factor analysis for topic identification to analyze 93,423 abstracts from the top 20 cancer/oncology journals for the period between 1999 and 2020. Results We identified 14 distinct topics in cancer literature. The results revealed the uptrend topics - including cell signaling (T-2), immunotherapy (T-3), clinical trials (T-5), disparities and epidemiology (T-7), cancer practice and policy (T-8), outcome research (T-9), and molecular therapeutics (T-10). - and downtrend topics such as cell death (T-1), early phase clinical trials (T-4), angiogenesis (T-6), cancer screening (T-12), and transplant (T-13). The topics of biomarkers(T-11) and cancer genetics(T-16) remained relatively stable. While the topics of angiogenesis (n = 10,490) and cell death (n = 10,258) included the highest number of abstracts, biomarkers (n = 3,203), and cancer genetic (n = 4,322) themes included the least number of articles. These findings suggest that despite having the lowest numbers of publications, certain topics such as cancer genetic (T-16) and biomarkers (T-11) have been exhibiting a stable trend and drawing a steady amount of attention from cancer researchers over the past 20 years. Conclusion Findings of this study contribute explanatory insight about themes and trends in cancer research, which can help researchers and stakeholders to identify areas for future studies. Policy Summary Statement The findings indicate the increasing efforts to improve cancer practice and cancer care through policy efforts. Therefore, policymakers and other stakeholders may use the findings in prioritization and funding of specific topics."
667,2f648e8abf14255ba10818b361f94c8a0668c22f,10.1186/s13643-021-01700-x,"Current text mining tools supporting abstract screening in systematic reviews are not widely used, in part because they lack sensitivity and precision. We set out to develop an accessible, semi-automated “workflow” to conduct abstract screening for systematic reviews and other knowledge synthesis methods. We adopt widely recommended text-mining and machine-learning methods to (1) process title-abstracts into numerical training data; and (2) train a classification model to predict eligible abstracts. The predicted abstracts are screened by human reviewers for (“true”) eligibility, and the newly eligible abstracts are used to identify similar abstracts, using near-neighbor methods, which are also screened. These abstracts, as well as their eligibility results, are used to update the classification model, and the above steps are iterated until no new eligible abstracts are identified. The workflow was implemented in R and evaluated using a systematic review of insulin formulations for type-1 diabetes (14,314 abstracts) and a scoping review of knowledge-synthesis methods (17,200 abstracts). Workflow performance was evaluated against the recommended practice of screening abstracts by 2 reviewers, independently. Standard measures were examined: sensitivity (inclusion of all truly eligible abstracts), specificity (exclusion of all truly ineligible abstracts), precision (inclusion of all truly eligible abstracts among all abstracts screened as eligible), F1-score (harmonic average of sensitivity and precision), and accuracy (correctly predicted eligible or ineligible abstracts). Workload reduction was measured as the hours the workflow saved, given only a subset of abstracts needed human screening. With respect to the systematic and scoping reviews respectively, the workflow attained 88%/89% sensitivity, 99%/99% specificity, 71%/72% precision, an F1-score of 79%/79%, 98%/97% accuracy, 63%/55% workload reduction, with 12%/11% fewer abstracts for full-text retrieval and screening, and 0%/1.5% missed studies in the completed reviews. The workflow was a sensitive, precise, and efficient alternative to the recommended practice of screening abstracts with 2 reviewers. All eligible studies were identified in the first case, while 6 studies (1.5%) were missed in the second that would likely not impact the review’s conclusions. We have described the workflow in language accessible to reviewers with limited exposure to natural language processing and machine learning, and have made the code available to reviewers."
668,342d20741b5ab448eb9098c4e72c95b7aebc9a60,10.1016/j.intimp.2021.107526,"Parkinson's disease is a progressive neurodegenerative disease associated with a loss of dopaminergic neurons in the substantia nigra of the brain. Neuroinflammation, another hallmark of the disease, is thought to play an important role in the neurodegenerative process. While mitigating neuroinflammation could prove beneficial for Parkinson's disease, identifying the most relevant biological processes and pharmacological targets as well as drugs to modulate them remains highly challenging. The present study aimed to better understand the protein network behind neuroinflammation in Parkinson's disease and to prioritize possible targets for its pharmacological modulation. We first used text-mining to systematically collect the proteins significantly associated to Parkinson's disease neuroinflammation over the scientific literature. The functional interaction network formed by these proteins was then analyzed by integrating functional enrichment, network topology analysis and drug-protein interaction analysis. We identified 57 proteins significantly associated to neuroinflammation in Parkinson's disease. Toll-like Receptor Cascades as well as Interleukin 4, Interleukin 10 and Interleukin 13 signaling appeared as the most significantly enriched biological processes. Protein network analysis using STRING and CentiScaPe identified 8 proteins with the highest ability to control these biological processes underlying neuroinflammation, namely caspase 1, heme oxygenase 1, interleukin 1beta, interleukin 4, interleukin 6, interleukin 10, tumor necrosis factor alpha and toll-like receptor 4. These key proteins were indexed to be targetable by a total of 38 drugs including 27 small compounds 11 protein-based therapies. In conclusion, our study highlights key proteins in Parkinson's disease neuroinflammation as well as pharmacological compounds acting on them. As such, it may facilitate the prioritization of biomarkers for the development of diagnostic, target-engagement assessment and therapeutic tools against Parkinson's disease."
674,47b9648d267b49a741556f20805584b188c0acd7,10.1002/cl2.1188,No abstract found
678,51ae84062647aae1038c11596fc76d50c6aac6dd,10.3233/efi-211567,No abstract found
685,70ea4b6df6e64dd39dd354e426442d6a7ee4a990,10.1016/J.IJEPES.2021.107176,"Abstract The exponential growth of solar power has been witnessed in the past decade and is projected by the ambitious policy targets. Nevertheless, the proliferation of solar energy poses challenges to power system operations, mostly due to its uncertainty, locational specificity, and variability. The prevalence of smart grids enables artificial intelligence (AI) techniques to mitigate solar integration problems with massive amounts of solar energy data. Different AI subfields (e.g., machine learning, deep learning, ensemble learning, and metaheuristic learning) have brought breakthroughs in solar energy, especially in its grid integration. However, AI research in solar integration is still at the preliminary stage, and is lagging behind the AI mainstream. Aiming to inspire deep AI involvement in the solar energy domain, this paper presents a taxonomical overview of AI applications in solar photovoltaic (PV) systems. Text mining techniques are first used as an assistive tool to collect, analyze, and categorize a large volume of literature in this field. Then, based on the constructed literature infrastructure, recent advancements in AI applications to solar forecasting, PV array detection, PV system fault detection, design optimization, and maximum power point tracking control problems are comprehensively reviewed. Current challenges and future trends of AI applications in solar integration are also discussed for each application theme."
687,74c7684a36df898fd1fecd4ca4082ddc34ad362b,10.1186/s12874-021-01485-6,"Clinical trial registries can be used as sources of clinical evidence for systematic review synthesis and updating. Our aim was to evaluate methods for identifying clinical trial registrations that should be screened for inclusion in updates of published systematic reviews.A set of 4644 clinical trial registrations (ClinicalTrials.gov) included in 1089 systematic reviews (PubMed) were used to evaluate two methods (document similarity and hierarchical clustering) and representations (L2-normalised TF-IDF, Latent Dirichlet Allocation, and Doc2Vec) for ranking 163,501 completed clinical trials by relevance. Clinical trial registrations were ranked for each systematic review using seeding clinical trials, simulating how new relevant clinical trials could be automatically identified for an update. Performance was measured by the number of clinical trials that need to be screened to identify all relevant clinical trials.Using the document similarity method with TF-IDF feature representation and Euclidean distance metric, all relevant clinical trials for half of the systematic reviews were identified after screening 99 trials (IQR 19 to 491). The best-performing hierarchical clustering was using Ward agglomerative clustering (with TF-IDF representation and Euclidean distance) and needed to screen 501 clinical trials (IQR 43 to 4363) to achieve the same result.An evaluation using a large set of mined links between published systematic reviews and clinical trial registrations showed that document similarity outperformed hierarchical clustering for identifying relevant clinical trials to include in systematic review updates."
691,8659ba88beb299e4ecc428befabb945c86921304,10.1186/s12859-021-04396-x,"BACKGROUND The Living Evidence Map Project at the Norwegian Institute of Public Health (NIPH) gives an updated overview of research results and publications. As part of NIPH's mandate to inform evidence-based infection prevention, control and treatment, a large group of experts are continously monitoring, assessing, coding and summarising new COVID-19 publications. Screening tools, coding practice and workflow are incrementally improved, but remain largely manual. RESULTS This paper describes how deep learning methods have been employed to learn classification and coding from the steadily growing NIPH COVID-19 dashboard data, so as to aid manual classification, screening and preprocessing of the rapidly growing influx of new papers on the subject. Our main objective is to make manual screening scalable through semi-automation, while ensuring high-quality Evidence Map content. CONCLUSIONS We report early results on classifying publication topic and type from titles and abstracts, showing that even simple neural network architectures and text representations can yield acceptable performance."
715,c3bc3c0e0c02d1e83653038f8a9b02fae6bf57fd,10.1002/cl2.1180,"This review builds on 3ie's (international initiative for impact evaluation) evidence gap map (EGM) of the impact evaluation and systematic review (SR) evidence base of interventions aiming to promote peaceful and inclusive societies in fragile contexts. The EGM identified a cluster of studies evaluating gender equality‐focused behaviour change communication programmes and raised interest in investigating the evidence base for understanding the role of women more broadly as agents of change in developing peaceful and inclusive societies. Building on the cluster of evidence identified in the EGM, our review will increase generalisability of findings from single studies and focus on interventions across a broad range of geographical locations, settings and populations, types of implementations and outcomes. We will also address (when possible) the identified gaps in literature regarding metaanalysis in conflict‐affected contexts. As such, we propose the following objectives: (1) The primary objective of this review is to identify, assess and synthesise evidence on the effect of gender specific and gender transformative interventions within the context of the four pillars of United Nations Security Council Resolution (UNSCR) 1325 on women's empowerment and gender equality in Fragile and Conflict Affected States/Situations (FCAS). The SR will facilitate the use of evidence in informing policy and practice decisions within the field of transition aid, particularly as it relates to gender focused programming. (2) Our second objective is to assess how these interventions contribute to inclusive and sustainable peace in conflict affected situations. We will compare the effectiveness of these different types of interventions through the lenses of their ecological level, types of impact on women's empowerment, local context of gender inequality and conflict. To achieve these objectives we aim to answer the following questions: (1) What are the impacts of gender transformative and specific interventions on women's empowerment and gender equality in FCAS? (2) What are the effects of these interventions on sustainable peace? (3) To what extent do effects vary by population group, ecological level and types of interventions? (4) What are contextual barriers to and facilitators of intervention effectiveness?"
722,db53306c105d19b4d9c4916b3461a443cd8a0943,10.1007/s10669-021-09830-2,"Various industrial and development projects have the potential to adversely affect threatened and endangered species and their habitats. The federal Endangered Species Act (ESA) requires preparation of a biological assessment or habitat conservation plan before federal agencies can authorize, through decision documents and permits, unintentional and otherwise prohibited “take” (i.e., harm) of listed species. These documents describe the potential effects of proposed projects on listed species and include measures to mitigate those effects. Collectively, these assessments, plans, decision documents, and permits—termed ESA documents in our study—are valuable for identifying approved mitigation options that could apply to future projects. However, owing to the volume, length, and complexity of these documents, manual review would be time- and labor-intensive. In this study, we apply three supervised machine learning algorithms, including two based on state-of-the-art transfer learning, to develop and evaluate predictive models capable of extracting mitigation-related text from ESA documents. The machine learning models were developed based on a training dataset that was created as part of this study. The best performing model showed an estimated ROC-AUC score of 0.98 and a precision recall AUC score of 0.86 during cross-validation, indicating great potential for effectively extracting mitigation-related content from existing documents. To illustrate the utility of this technology, we present a simulated case study application in which the use of pretrained machine learning models capable of recognizing mitigation measures, coupled with a large historical corpus of ESA documents and keyword filters, provided a means to rapidly assess the commonly used mitigation measures for a given species. While this technology did not eliminate the requirement for biological expertise, it did allow for rapid scoping assessments and could serve as a supporting resource even for experienced biologists."
724,dce5855c7661a8b73d73a2946f4f603fb6aa24b4,10.1007/978-3-030-72113-8_36,"Professional search queries are often formulated in a structured manner, where multiple aspects are combined in a logical form. The information need is often fulfilled by an initial retrieval stage followed by a complex reranking algorithm. In this paper, we analyze a simple, explainable reranking model that follows the structured search criterion. Different aspects of the criterion are predicted by machine learning classifiers, which are then combined through the logical form to predict document relevance. On three years of data from the TREC Precision Medicine literature search track (2017–2019), we show that the simple model consistently performs as well as LambdaMART rerankers. Furthermore, many black-box rerankers developed by top-ranked TREC teams can be replaced by this simple model without statistically significant performance change. Finally, we find that the model can achieve remarkably high performance even when manually labeled documents are very limited. Together, these findings suggest that leveraging the structure in professional search queries is a promising direction towards building explainable, label-efficient, and high-performance retrieval models for professional search tasks."
733,063afd192657281a6d43a1513344f0e3d6d47912,10.1007/978-3-540-68947-8_8,No abstract found
736,0bb3a566d5cc49a2f2ecbd5739d6c7ef760e2f80,,No DOI detected
739,1646ffbe4086944ef495004a5c07af1409641c52,10.1186/1472-6947-10-29,"Formulating a clinical information need in terms of the four atomic parts which are Population/Problem, Intervention, Comparison and Outcome (known as PICO elements) facilitates searching for a precise answer within a large medical citation database. However, using PICO defined items in the information retrieval process requires a search engine to be able to detect and index PICO elements in the collection in order for the system to retrieve relevant documents. In this study, we tested multiple supervised classification algorithms and their combinations for detecting PICO elements within medical abstracts. Using the structural descriptors that are embedded in some medical abstracts, we have automatically gathered large training/testing data sets for each PICO element. Combining multiple classifiers using a weighted linear combination of their prediction scores achieves promising results with an f-measure score of 86.3% for P, 67% for I and 56.6% for O. Our experiments on the identification of PICO elements showed that the task is very challenging. Nevertheless, the performance achieved by our identification method is competitive with previously published results and shows that this task can be achieved with a high accuracy for the P element but lower ones for I and O elements."
749,2e49816e154db2c7e81f5bb5d8acc01a24626b00,10.1016/S0959-440X(96)80056-X,"'Profiles' of protein structures and sequence alignments can detect subtle homologies. Profile analysis has been put on firmer mathematical ground by the introduction of hidden Markov model (HMM) methods. During the past year, applications of these powerful new HMM-based profiles have begun to appear in the fields of protein-structure prediction and large-scale genome-sequence analysis."
750,2f564216056d38425c470d98b478dbef06663a32,10.1016/j.jbi.2008.12.011,"Background: Large numbers of reports of randomized controlled trials (RCTs) are published each year, and it is becoming increasingly difficult for clinicians practicing evidence-based medicine to find answers to clinical questions. The automatic machine extraction of RCT experimental details, including design methodology and outcomes, could help clinicians and reviewers locate relevant studies more rapidly and easily. Aim: This paper investigates how the comparison of interventions is documented in the abstracts of published RCTs. The ultimate goal is to use automated text mining to locate each intervention arm of a trial. This preliminary work aims to identify coordinating constructions, which are prevalent in the expression of intervention comparisons. Methods and results: An analysis of the types of constructs that describe the allocation of intervention arms is conducted, revealing that the compared interventions are predominantly embedded in coordinating constructions. A method is developed for identifying the descriptions of the assignment of treatment arms in clinical trials, using a full sentence parser to locate coordinating constructions and a statistical classifier for labeling positive examples. Predicate-argument structures are used along with other linguistic features with a maximum entropy classifier. An F-score of 0.78 is obtained for labeling relevant coordinating constructions in an independent test set. Conclusions: The intervention arms of a randomized controlled trials can be identified by machine extraction incorporating syntactic features derived from full sentence parsing."
752,38dd6bbd251e4af79515a8ffd62dfba77fe24d35,10.1016/J.IJMEDINF.2004.10.003,"Summary Background: Clinicians have many unanswered questions during clinical encounters which may impact on the quality and outcomes of decisions made. Provision of online evidence at the point of care is one strategy that provides clinicians with easy access to up-to-date evidence in clinical settings to support evidence-based decision-making. Aim: To determine if and when general practitioners use an online evidence system in routine clinical practice, the type of questions for which clinicians seek evidence and the extent to which the system provides clinically useful answers. Design of study: A prospective cohort study which involved a 4-week clinical trial of Quick Clinical, an online evidence system specifically designed around the needs of general practitioners. Setting: Two hundred and twenty-seven clinicians who had a computer with Internet access in their consulting rooms. Methods: Computer logs and survey analysis. Results: One hundred and ninety-three general practitioners used the online evidence system to conduct on average 8.7 searches/month. The majority of these (81%) were conducted from consulting rooms and carried out between 9 a.m. and 7 p.m. (83%). The most frequent searches conducted related to diagnosis (40%) and treatment (35%). 83% of clinicians believed that Quick Clinical (QC) had the potential to improve patient care, and one in four users reported direct experience of improvements in care. In 73% of queries with clinician feedback participants reported that they were able to find clinically useful information during their routine work. Conclusion: General practitioners will use an online evidence retrieval system in routine practice, and report that its use improves the quality of patient care."
758,46774b2fc6ddc7e11685f748eda8d9ed2c8669e1,10.1145/2649387.2649406,No abstract found
762,586ea3c3797996d6351db961def288f7c92328e6,10.1016/j.jbi.2014.02.006,"Display Omitted Classification of sentences in Evidence Based Medicine abstracts, using a standard abstract structure.Supervised sentence-oriented classification using the PIBOSO scheme.Lexical, statistical and sequential features, independent of external sources.Increased efficiency of around 25 percentage points in F-score when compared to state of the art. Evidence Based Medicine (EBM) provides a framework that makes use of the current best evidence in the domain to support clinicians in the decision making process. In most cases, the underlying foundational knowledge is captured in scientific publications that detail specific clinical studies or randomised controlled trials. Over the course of the last two decades, research has been performed on modelling key aspects described within publications (e.g., aims, methods, results), to enable the successful realisation of the goals of EBM. A significant outcome of this research has been the PICO (Population/Problem-Intervention-Comparison-Outcome) structure, and its refined version PIBOSO (Population-Intervention-Background-Outcome-Study Design-Other), both of which provide a formalisation of these scientific artefacts. Subsequently, using these schemes, diverse automatic extraction techniques have been proposed to streamline the knowledge discovery and exploration process in EBM. In this paper, we present a Machine Learning approach that aims to classify sentences according to the PIBOSO scheme. We use a discriminative set of features that do not rely on any external resources to achieve results comparable to the state of the art. A corpus of 1000 structured and unstructured abstracts - i.e., the NICTA-PIBOSO corpus - is used for training and testing. Our best CRF classifier achieves a micro-average F-score of 90.74% and 87.21%, respectively, over structured and unstructured abstracts, which represents an increase of 25.48 percentage points and 26.6 percentage points in F-score when compared to the best existing approaches."
763,5bf9aa50975c87853bf991b0d2eda40c56b29e70,10.1016/j.jbi.2013.07.009,"Graphical AbstractDisplay Omitted Two sets of naive Bayes classifiers were developed for PICO detection.We trained one set with first sentences and the other with all sentences.The first-sentence classifier performs slightly better for patient (P) elements.The all-sentence classifier performs better for intervention (I) elements.The performances are about the same for outcome (O) elements. Efficient identification of patient, intervention, comparison, and outcome (PICO) components in medical articles is helpful in evidence-based medicine. The purpose of this study is to clarify whether first sentences of these components are good enough to train naive Bayes classifiers for sentence-level PICO element detection. We extracted 19,854 structured abstracts of randomized controlled trials with any P/I/O label from PubMed for naive Bayes classifiers training. Performances of classifiers trained by first sentences of each section ( CF ) and those trained by all sentences ( CA ) were compared using all sentences by ten-fold cross-validation. The results measured by recall, precision, and F-measures show that there are no significant differences in performance between CF and CA for detection of O-element (F-measure=0.731?0.009 vs. 0.738?0.010, p=0.123). However, CA perform better for I-elements, in terms of recall (0.752?0.012 vs. 0.620?0.007, p<0.001) and F-measures (0.728?0.006 vs. 0.662?0.007, p<0.001). For P-elements, CF have higher precision (0.714?0.009 vs. 0.665?0.010, p<0.001), but lower recall (0.766?0.013 vs. 0.811?0.012, p<0.001). CF are not always better than CA in sentence-level PICO element detection. Their performance varies in detecting different elements."
766,67e54dd64aa72b639c4e8b3fe9332896d5970084,10.1007/s00354-007-0017-5,"Evidence-based medicine (EBM) requires medical practitioners to select appropriate treatments for individual patients based on the current best evidence, and the results of phase III clinical trials are the major source of such evidence. In this paper, we report results of experiment in extracting important information for EBM from the abstracts of phase III clinical trials, in an effort to investigate how far the existing natural language processing (NLP) techniques could support EBM using MEDLINE database."
771,982a27d974e88f1e23b89ae76bbf5b6fd22a4c52,10.1186/1471-2288-3-25,"Background: In the era of evidence based medicine, with systematic reviews as its cornerstone, adequate quality assessment tools should be available. There is currently a lack of a systematically developed and evaluated tool for the assessment of diagnostic accuracy studies. The aim of this project was to combine empirical evidence and expert opinion in a formal consensus method to develop a tool to be used in systematic reviews to assess the quality of primary studies of diagnostic accuracy. Methods: We conducted a Delphi procedure to develop the quality assessment tool by refining an initial list of items. Members of the Delphi panel were experts in the area of diagnostic research. The results of three previously conducted reviews of the diagnostic literature were used to generate a list of potential items for inclusion in the tool and to provide an evidence base upon which to develop the tool. Results: A total of nine experts in the field of diagnostics took part in the Delphi procedure. The Delphi procedure consisted of four rounds, after which agreement was reached on the items to be included in the tool which we have called QUADAS. The initial list of 28 items was reduced to fourteen items in the final tool. Items included covered patient spectrum, reference standard, disease progression bias, verification bias, review bias, clinical review bias, incorporation bias, test execution, study withdrawals, and indeterminate results. The QUADAS tool is presented together with guidelines for scoring each of the items included in the tool. Conclusions: This project has produced an evidence based quality assessment tool to be used in systematic reviews of diagnostic accuracy studies. Further work to determine the usability and validity of the tool continues."
783,c772abd194e30459334924f6a76496e3fef2abd2,10.1016/S0002-9394(14)70422-2,"THE RANDOMIZED controlled trial (RCT), more than any other methodology, can have a powerful and immediate impact on patient care. Ideally, the report of such an evaluation needs to convey to the reader relevant information concerning the design, conduct, analysis, and generalizability of the trial. This information should provide the reader with the ability to make informed judgments regarding the internal and external validity of the trial. Accurate and complete reporting also benefits editors and reviewers in their deliberations regarding submitted manuscripts. For RCTs to ultimately benefit patients, the published report should be of the highest possible standard. For editorial comment see p 649. Evidence produced repeatedly over the last 30 years indicates a wide chasm between what a trial should report and what is actually published in the literature. In a review of 71 RCTs with negative results published between 1960 and 1975, the authors reported that the vast"
799,f198043a866e9187925a8d8db9a55e3bfdd47f2c,10.1016/B978-0-12-411519-4.00006-9,"Abstract Topic analysis is a powerful tool that extracts “topics” from document collections. Unlike manual tagging, which is effort intensive and requires expertise in the documents’ subject matter, topic analysis (in its simplest form) is an automated process. Relying on the assumption that each document in a collection refers to a small number of topics, it extracts bags of words attributable to these topics. These topics can be used to support document retrieval or to relate documents to each other through their associated topics. Given the variety and amount of textual information included in software repositories, in issue reports, in commit and source-code comments, and in other forms of documentation, this method has found many applications in the software-engineering field of mining software repositories. This chapter provides an overview of the theory underlying latent Dirichlet allocation (LDA), the most popular topic-analysis method today. Next, it illustrates, with a brief tutorial introduction, how to employ LDA on a textual data set. Third, it reviews the software-engineering literature for uses of LDA for analyzing textual software-development assets, in order to support developers’ activities. Finally, we discuss the interpretability of the automatically extracted topics, and their correlation with tags provided by subject-matter experts."
801,fc9a6e30fc9fe0c99679daeb15677b00b62e0641,10.1016/j.ijmedinf.2005.05.004,"Summary Purpose: This paper appraises empirical studies examining the impact of clinical information-retrieval technology on physicians and medical students. Methods: The world literature was reviewed up to February 2004. Two reviewers independently identified studies by scrutinising 3368 and 3249 references from bibliographic databases. Additional studies were retrieved by hand searches, and by searching ISI Web of Science for citations of articles. Six hundred and five paper-based articles were assessed for relevance. Of those, 40 (6.6%) were independently appraised by two reviewers for relevance and methodological quality. These articles were quantitative, qualitative or of mixed methods, and 26 (4.3%) were retained for further analysis. For each retained article, two teams used content analysis to review extracted textual material (quantitative results and qualitative findings). Results: Observational studies suggest that nearly one-third of searches using information-retrieval technology may have a positive impact on physicians. Two experimental and three laboratory studies do not reach consensus in support of a greater impact of this technology compared with other sources of information, notably printed educational material. Clinical information-retrieval technology may affect physicians, and further research is needed to examine its impact in everyday practice."
828,ed19019b82088a6e560ebdeadb19b72a915bb2c4,10.1186/s13643-021-01881-5,"Systematic review is an indispensable tool for optimal evidence collection and evaluation in evidence-based medicine. However, the explosive increase of the original literatures makes it difficult to accomplish critical appraisal and regular update. Artificial intelligence (AI) algorithms have been applied to automate the literature screening procedure in medical systematic reviews. In these studies, different algorithms were used and results with great variance were reported. It is therefore imperative to systematically review and analyse the developed automatic methods for literature screening and their effectiveness reported in current studies.An electronic search will be conducted using PubMed, Embase, ACM Digital Library, and IEEE Xplore Digital Library databases, as well as literatures found through supplementary search in Google scholar, on automatic methods for literature screening in systematic reviews. Two reviewers will independently conduct the primary screening of the articles and data extraction, in which nonconformities will be solved by discussion with a methodologist. Data will be extracted from eligible studies, including the basic characteristics of study, the information of training set and validation set, and the function and performance of AI algorithms, and summarised in a table. The risk of bias and applicability of the eligible studies will be assessed by the two reviewers independently based on Quality Assessment of Diagnostic Accuracy Studies (QUADAS-2). Quantitative analyses, if appropriate, will also be performed.Automating systematic review process is of great help in reducing workload in evidence-based practice. Results from this systematic review will provide essential summary of the current development of AI algorithms for automatic literature screening in medical evidence synthesis and help to inspire further studies in this field.PROSPERO CRD42020170815 (28 April 2020)."
845,752bed0f7134fa2ff06be13b6db32238b08d83e8,10.1007/978-3-030-71921-0_6,"Accurate data extraction and their synthesis form the basis of appropriate conclusions of a systematic review. Systematic reviewers should extract ALL data relevant to the review question, not just the outcome data. Data to be extracted include baseline characteristics of study participants, information related to study methodology and outcomes and other relevant information. If published articles have given the results using figures instead of actual numbers, specialised software that convert images to pixel values may be utilised to obtain the actual data values. Tools such as Plot Digitizer, WebPlotDigitizer, Engauge, Dexter, Ycasd and GetData Graph Digitizer can be used for this purpose. When unable to extract data from available reports or to seek clarifications, the reviewers could contact the original investigators. Data extraction should be performed using pre-piloted forms independently by at least two reviewers to ensure accuracy. A high level of diligence is required to minimise errors during the stage of data extraction."
863,2d08a4dbc5ffbcacbd2fbf2d299eca71229e9db6,10.1007/s10669-020-09763-2,"Machine learning technology has been widely adopted as a cost-saving document prioritization approach in systematic literature reviews related to human health risk assessments. Supervised approaches use a training dataset, a relatively small set of documents with human-annotated labels indicating the topic of each document, to build models that automatically predict the labels of a much larger set of unlabelled documents. Deep learning algorithms form a branch of machine learning that relies on complex neural network architectures to learn the features of the object to be classified. Although deep learning algorithms have till recently mainly been applied for image, video, and audio classification, they are increasingly being deployed on text classification problems. To explore the potential advantages and practicalities of using deep learning algorithms in the document prioritization step of systematic literature reviews, we compare the performance of the most commonly used deep learning architectures with more traditional machine learning models using a dataset of approximately 7000 abstracts from the scientific literature related to the chemical arsenic. The dataset was previously annotated by subject matter experts with regard to relevance to toxicological mode of action. We examine the relative performance of each algorithm type at alternative levels of training by sequentially expanding the training dataset to generate a learning curve. We find that deep learning offers increased performance in some instances but also requires more data to train algorithms, increased model training time, increased computational power, and more labor-intensive algorithm tuning compared to baseline traditional machine learning algorithms."
874,f8e1899afbe2d93f518f9d0564dc97ba0e61cc1b,10.1007/978-3-030-46216-1_27,"Research synthesis and meta-analysis (RS/MA) comprise a set of procedures for conducting comprehensive and empirically-grounded reviews of previous research. In contrast to traditional literature reviews, RS/MA provide enhanced objectivity and systematicity. It is not surprising, therefore, that researchers have turned in recent years to RS/MA as a means to bring together and examine previous research in a number of individual subdomains. Corpus researchers, however, have applied RS/MA only sparsely and in a very small number of subdomains. Considering the broad range of questions addressed in corpus linguistics, we believe there is massive potential for meta-analysis as a means to systematically synthesize findings in the field of corpus linguistics. In order to enable such efforts, the chapter provides both a conceptual and procedural introduction to RS/MA, addressing all major steps including (a) defining the domain and searching for primary literature, (b) developing and implementing a coding scheme, (c) calculating and aggregating effect sizes (using the langtest.jp web app and the metafor package in R), and (d) interpreting results. The discussion considers each step with respect to the challenges unique to meta-analyzing research in corpus linguistics. We also propose a novel use of corpus linguistic techniques as a means to enhance the efficiency and perhaps accuracy of extracting synthetic data from primary studies."
876,000ee1a4ac67d0001c937200562f8a913d9b2ab8,10.1016/j.jclinepi.2019.07.005,"Abstract Objectives Data Abstraction Assistant (DAA) is a software for linking items abstracted into a data collection form for a systematic review to their locations in a study report. We conducted a randomized cross-over trial that compared DAA-facilitated single-data abstraction plus verification (“DAA verification”), single data abstraction plus verification (“regular verification”), and independent dual data abstraction plus adjudication (“independent abstraction”). Study Design and Setting This study is an online randomized cross-over trial with 26 pairs of data abstractors. Each pair abstracted data from six articles, two per approach. Outcomes were the proportion of errors and time taken. Results Overall proportion of errors was 17% for DAA verification, 16% for regular verification, and 15% for independent abstraction. DAA verification was associated with higher odds of errors when compared with regular verification (adjusted odds ratio [OR] = 1.08; 95% confidence interval [CI]: 0.99–1.17) or independent abstraction (adjusted OR = 1.12; 95% CI: 1.03–1.22). For each article, DAA verification took 20 minutes (95% CI: 1–40) longer than regular verification, but 46 minutes (95% CI: 26 to 66) shorter than independent abstraction. Conclusion Independent abstraction may only be necessary for complex data items. DAA provides an audit trail that is crucial for reproducible research."
877,046b3c365e1b24443847db20b45a886e1d5f556c,10.1002/9781119536604.ch5,No abstract found
879,18fdedc5d614ba0429314ebe824edbfc63ddb05a,10.1016/j.infsof.2018.09.006,"Abstract Context A Multivocal Literature Review (MLR) is a form of a Systematic Literature Review (SLR) which includes the grey literature (e.g., blog posts, videos and white papers) in addition to the published (formal) literature (e.g., journal and conference papers). MLRs are useful for both researchers and practitioners since they provide summaries both the state-of-the art and –practice in a given area. MLRs are popular in other fields and have recently started to appear in software engineering (SE). As more MLR studies are conducted and reported, it is important to have a set of guidelines to ensure high quality of MLR processes and their results. Objective There are several guidelines to conduct SLR studies in SE. However, several phases of MLRs differ from those of traditional SLRs, for instance with respect to the search process and source quality assessment. Therefore, SLR guidelines are only partially useful for conducting MLR studies. Our goal in this paper is to present guidelines on how to conduct MLR studies in SE. Method To develop the MLR guidelines, we benefit from several inputs: (1) existing SLR guidelines in SE, (2), a literature survey of MLR guidelines and experience papers in other fields, and (3) our own experiences in conducting several MLRs in SE. We took the popular SLR guidelines of Kitchenham and Charters as the baseline and extended/adopted them to conduct MLR studies in SE. All derived guidelines are discussed in the context of an already-published MLR in SE as the running example. Results The resulting guidelines cover all phases of conducting and reporting MLRs in SE from the planning phase, over conducting the review to the final reporting of the review. In particular, we believe that incorporating and adopting a vast set of experience-based recommendations from MLR guidelines and experience papers in other fields have enabled us to propose a set of guidelines with solid foundations. Conclusion Having been developed on the basis of several types of experience and evidence, the provided MLR guidelines will support researchers to effectively and efficiently conduct new MLRs in any area of SE. The authors recommend the researchers to utilize these guidelines in their MLR studies and then share their lessons learned and experiences."
880,1ab82e5ff6ef2210d396bf70ecabb46b3562c956,10.1016/j.jbi.2019.103321,"Abstract Objective Published clinical trials and high quality peer reviewed medical publications are considered as the main sources of evidence used for synthesizing systematic reviews or practicing Evidence Based Medicine (EBM). Finding all relevant published evidence for a particular medical case is a time and labour intensive task, given the breadth of the biomedical literature. Automatic quantification of conceptual relationships between key clinical evidence within and across publications, despite variations in the expression of clinically-relevant concepts, can help to facilitate synthesis of evidence. In this study, we aim to provide an approach towards expediting evidence synthesis by quantifying semantic similarity of key evidence as expressed in the form of individual sentences. Such semantic textual similarity can be applied as a key approach for supporting selection of related studies. Material and methods We propose a generalisable approach for quantifying semantic similarity of clinical evidence in the biomedical literature, specifically considering the similarity of sentences corresponding to a given type of evidence, such as clinical interventions, population information, clinical findings, etc. We develop three sets of generic, ontology-based, and vector-space models of similarity measures that make use of a variety of lexical, conceptual, and contextual information to quantify the similarity of full sentences containing clinical evidence. To understand the impact of different similarity measures on the overall evidence semantic similarity quantification, we provide a comparative analysis of these measures when used as input to an unsupervised linear interpolation and a supervised regression ensemble. In order to provide a reliable test-bed for this experiment, we generate a dataset of 1000 pairs of sentences from biomedical publications that are annotated by ten human experts. We also extend the experiments on an external dataset for further generalisability testing. Results The combination of all diverse similarity measures showed stronger correlations with the gold standard similarity scores in the dataset than any individual kind of measure. Our approach reached near 0.80 average Pearson correlation across different clinical evidence types using the devised similarity measures. Although they were more effective when combined together, individual generic and vector-space measures also resulted in strong similarity quantification when used in both unsupervised and supervised models. On the external dataset, our similarity measures were highly competitive with the state-of-the-art approaches developed and trained specifically on that dataset for predicting semantic similarity. Conclusion Experimental results showed that the proposed semantic similarity quantification approach can effectively identify related clinical evidence that is reported in the literature. The comparison with a state-of-the-art method demonstrated the effectiveness of the approach, and experiments with an external dataset support its generalisability."
882,1eed8b41d7c9d0196bf633ea98e74f9fd36ce4e7,10.1016/j.aap.2019.105333,"Abstract Attributions of fault are often associated with worse injury outcomes; however, the consistency and magnitude of these impacts is not known. This review examined the prognostic role of fault on health, mental health, pain and work outcomes after transport injury. A systematic search of five electronic databases (Medline, Embase, CINAHL, PsycINFO, Cochrane Library) yielded 16,324 records published between 2000 and January 2018. Eligibility criteria were: adult transport injury survivors; prospective design; multivariable analysis; fault-related factor analysed; pain, mental health, general health or work-related outcome. Citations (n = 10,558, excluding duplicates) and full text articles (n = 555) were screened manually (Reviewer 1), and using concurrent machine learning and text mining (Reviewer 2; using Abstrackr, WordStat and QDA miner). Data from 55 papers that met all inclusion criteria were extracted, papers were evaluated for risk of bias using the QUIPS tool, and overall level of evidence was assessed using the GRADE tool. There were six main fault-related factors classified as: fault or responsibility, fault-based compensation, lawyer involvement or litigation, blame or guilt, road user or position in vehicle, and impact direction. Overall there were inconsistent associations between fault and transport injury outcomes, and 60% of papers had high risk of bias. There was moderate evidence that fault-based compensation claims were associated with poorer health-related outcomes, and that lawyer involvement was associated with poorer work outcomes beyond 12 months post-injury. However, the evidence of negative associations between fault-based compensation claims and work-related outcomes was limited. Lawyer involvement and fault-based compensation claims were associated with adverse mental health outcomes six months post-injury, but not beyond 12 months. The most consistent associations between fault and negative outcomes were not for fault attributions, per se, but were related to fault-related procedures (e.g., lawyer engagement, fault-based compensation claims)."
883,2087470008e121b3e2006f2870b1bbdcc7f6fbe8,10.1016/J.IJMEDINF.2019.06.020,"Abstract Background and objective Hypoglycemia is a common safety event when attempting to optimize glycemic control in diabetes (DM). While electronic medical records provide a natural ground for detecting and analyzing hypoglycemia, ICD codes used in the databases may be invalid, insensitive or non-specific in detecting new hypoglycemic events. We developed text preprocessing methods to improve automatic detection of hypoglycemia from analysis of clinical encounter text notes. Methods We set out to improve hypoglycemia detection from clinical notes by introducing three preprocessing methods: stop word filtering, medication signaling, and ICD narrative enrichment. To test the proposed methods, we selected clinical notes from VA Maryland Healthcare System, based on various combinations of three criteria that are suggestive of hypoglycemia, including ICD-9 code of diabetes and hypoglycemia, laboratory glucose values Results Each of the proposed preprocessing methods contributed to the performance of hypoglycemia detection by significantly increasing the F1 score in the range of 5.3∼7.4% on one dataset (p  Conclusion The proposed text preprocessing methods improved the performance of hypoglycemia detection from clinical text notes. Stop word filtering achieved the most performance improvement. ICD narrative enrichment boosted the recall of detection. Combining the three preprocessing methods led to additional performance gains."
885,363111a248a4065d93b7957d09b75ebed3b3d996,10.1007/978-3-030-33607-3_29,"The overwhelming volume of data generated online continuous to grow at an exponential and unprecedented rate. Over 80% of such data is unstructured. Scientific research publications constitute a significant portion of such unstructured data. Systematic literature review (SLR) activity is a rigorous and challenging process. The key challenge in SLR is the automatic extraction of the relevant data from the sheer volume of research publications. Lack of a unified framework has been identified as the key problem. A canonical model, based on the structure of the papers was proposed as the framework for data extraction purposes in SLR. Implemented as a classification problem, traditional machine learning models were used to realise the canonical model. A good accuracy was reported in these traditional models. However, there is room for improvement. This paper presents the result of the work on the same problem using convolutional neural network (CNN), which is more sophisticated (deeper). The results show an improvement over the traditional machine learning models with an accuracy of 85%. Unlike the previous CNN NLP works, this work also demonstrates the application of CNN on a bigger NLP dataset such as the data from the scientific research publications. The result also shows that the CNN performs even better in NLP tasks with bigger datasets."
886,3b00f5cbdcf854992e43b5d31d7d17dd6d317aaa,10.1007/s10669-019-09717-3,"Machine learning has emerged as a cost-effective innovation to support systematic literature reviews in human health risk assessments and other contexts. Supervised machine learning approaches rely on a training dataset, a relatively small set of documents with human-annotated labels indicating their topic, to build models that automatically classify a larger set of unclassified documents. “Active” machine learning has been proposed as an approach that limits the cost of creating a training dataset by interactively and sequentially focussing on training only the most informative documents. We simulate active learning using a dataset of approximately 7000 abstracts from the scientific literature related to the chemical arsenic. The dataset was previously annotated by subject matter experts with regard to relevance to two topics relating to toxicology and risk assessment. We examine the performance of alternative sampling approaches to sequentially expanding the training dataset, specifically looking at uncertainty-based sampling and probability-based sampling. We discover that while such active learning methods can potentially reduce training dataset size compared to random sampling, predictions of model performance in active learning are likely to suffer from statistical bias that negates the method’s potential benefits. We discuss approaches and the extent to which the bias resulting from skewed sampling can be compensated. We propose a useful role for active learning in contexts in which the accuracy of model performance metrics is not critical and/or where it is beneficial to rapidly create a class-balanced training dataset."
890,5a9cff4b92adc0ad8f9dea5cfcdc4843ed757c6e,10.1007/978-3-030-20485-3_19,"The process of developing systematic reviews is a well established method of collecting evidence from publications, where it follows a predefined and explicit protocol design to promote rigour, transparency and repeatability. The process is manual and involves lot of time and needs expertise. The aim of this work is to build an effective framework using machine learning techniques to partially automate the process of systematic literature review by extracting required data elements of anxiety outcome measures. A framework is thus proposed that initially builds a training corpus by extracting different data elements related to anxiety outcome measures from relevant publications. The publications are retrieved from Medline, EMBASE, CINAHL, AHMED and Pyscinfo following a given set of rules defined by a research group in the United Kingdom reviewing comfort interventions in health care. Subsequently, the method trains a machine learning classifier using this training corpus to extract the desired data elements from new publications. The experiments are conducted on 48 publications containing anxiety outcome measures with an aim to automatically extract the sentences stating the mean and standard deviation of the measures of outcomes of different types of interventions to lessen anxiety. The experimental results show that the recall and precision of the proposed method using random forest classifier are respectively 100% and 83%, which indicates that the method is able to extract all required data elements."
894,86269e1307900fdcb12c104c06f95ce5923e48ae,10.1186/s13643-019-1035-3,"Maintained study-based registers (SBRs) have, at their core, study records linked to, potentially, multiple other records such as references, data sets, standard texts and full-text reports. Such registers can minimise and refine searching, de-duplicating, screening and acquisition of full texts. SBRs can facilitate new review titles/updates and, within seconds, inform the team about the potential workload of each task. We discuss the advantages/disadvantages of SBRs and report a case of how such a register was used to develop a successful grant application and deliver results—reducing considerable redundancy of effort. SBRs saved time in question-setting and scoping and made rapid production of nine Cochrane systematic reviews possible. Whilst helping prioritise and conduct systematic reviews, SBRs improve quality. Those funding information specialists for literature reviewing could reasonably stipulate the resulting SBR to be delivered for dissemination and use beyond the life of the project."
896,a1e02ff59e27beb3f77b0c74b7576605cccccd92,10.1016/j.jbi.2019.103275,"Abstract Background With the substantial growth in the biomedical research literature, a larger number of claims are published daily, some of which seemingly disagree with or contradict prior claims on the same topics. Resolving such contradictions is critical to advancing our understanding of human disease and developing effective treatments. Automated text analysis techniques can facilitate such analysis by extracting claims from the literature, flagging those that are potentially contradictory, and identifying any study characteristics that may explain such contradictions. Methods Using SemMedDB, our own PubMed-scale repository of semantic predications (subject-relation-object triples), we identified apparent contradictions in the biomedical research literature and developed a categorization of contextual characteristics that explain such contradictions. Clinically relevant semantic predications relating to 20 diseases and involving opposing predicate pairs (e.g., an intervention treats or causes a disease) were retrieved from SemMedDB. After addressing inference, uncertainty, generic concepts, and NLP errors through automatic and manual filtering steps, a set of apparent contradictions were identified and characterized. Results We retrieved 117,676 predication instances from 62,360 PubMed abstracts (Jan 1980-Dec 2016). From these instances, automatic filtering steps generated 2236 candidate contradictory pairs. Through manual analysis, we determined that 58 of these pairs (2.6%) were apparent contradictions. We identified five main categories of contextual characteristics that explain these contradictions: (a) internal to the patient, (b) external to the patient, (c) endogenous/exogenous, (d) known controversy, and (e) contradictions in literature. Categories (a) and (b) were subcategorized further (e.g., species, dosage) and accounted for the bulk of the contradictory information. Conclusions Semantic predications, by accounting for lexical variability, and SemMedDB, owing to its literature scale, can support identification and elucidation of potentially contradictory claims across the biomedical domain. Further filtering and classification steps are needed to distinguish among them the true contradictory claims. The ability to detect contradictions automatically can facilitate important biomedical knowledge management tasks, such as tracking and verifying scientific claims, summarizing research on a given topic, identifying knowledge gaps, and assessing evidence for systematic reviews, with potential benefits to the scientific community. Future work will focus on automating these steps for fully automatic recognition of contradictions from the biomedical research literature."
897,b4c124ba7638c3effaaf7e61f07689aae1265af9,10.1186/s12911-019-0814-z,"Assessing risks of bias in randomized controlled trials (RCTs) is an important but laborious task when conducting systematic reviews. RobotReviewer (RR), an open-source machine learning (ML) system, semi-automates bias assessments. We conducted a user study of RobotReviewer, evaluating time saved and usability of the tool. Systematic reviewers applied the Cochrane Risk of Bias tool to four randomly selected RCT articles. Reviewers judged: whether an RCT was at low, or high/unclear risk of bias for each bias domain in the Cochrane tool (Version 1); and highlighted article text justifying their decision. For a random two of the four articles, the process was semi-automated: users were provided with ML-suggested bias judgments and text highlights. Participants could amend the suggestions if necessary. We measured time taken for the task, ML suggestions, usability via the System Usability Scale (SUS) and collected qualitative feedback. For 41 volunteers, semi-automation was quicker than manual assessment (mean 755 vs. 824 s; relative time 0.75, 95% CI 0.62–0.92). Reviewers accepted 301/328 (91%) of the ML Risk of Bias (RoB) judgments, and 202/328 (62%) of text highlights without change. Overall, ML suggested text highlights had a recall of 0.90 (SD 0.14) and precision of 0.87 (SD 0.21) with respect to the users’ final versions. Reviewers assigned the system a mean 77.7 SUS score, corresponding to a rating between “good” and “excellent”. Semi-automation (where humans validate machine learning suggestions) can improve the efficiency of evidence synthesis. Our system was rated highly usable, and expedited bias assessment of RCTs."
898,b4ff5dd502041edcfeab98e4ef33014ae51c40bd,10.1016/j.jclinepi.2018.12.014,"Abstract Systematic reviewers are simultaneously unable to produce systematic reviews fast enough to keep up with the availability of new trial evidence while overproducing systematic reviews that are unlikely to change practice because they are redundant or biased. Although the transparency and completeness of trial reporting has improved with changes in policy and new technologies, systematic reviews have not yet benefited from the same level of effort. We found that new methods and tools used to automate aspects of systematic review processes have focused on improving the efficiency of individual systematic reviews rather than the efficiency of the entire ecosystem of systematic review production. We use software engineering principles to review challenges and opportunities for improving the interoperability, integrity, efficiency, and maintainability. We conclude by recommending ways to improve access to structured systematic review results. Major opportunities for improving systematic reviews will come from new tools and changes in policy focused on doing the right systematic reviews rather than just doing more of them faster."
901,d3504d67daa3da163dcbc168a5b75dac43e43055,10.1016/J.YJBINX.2019.100005,"Abstract Population, intervention, comparison and outcome (PICO) facets of clinical studies are required both for physicians in a clinical setting and for reviewers as they compare the effectiveness of different treatment strategies. Automated methods developed for the first three of these facets identify entities, but outcome detection has been limited to identifying the entire sentence. We frame outcome detection as a noun phrase prediction task and use semi-supervised learning to detect new outcomes (aka endpoints) from the method section of 88 K MEDLINE abstracts. A manual analysis showed that 96.7% of all outcomes can be captured using a noun phrase representation. With respect to the machine learning classifiers, the Support Vector Machine produced higher precision, F1-score, and accuracy than the General Linear Model when evaluated with respect to the initial gold standard of survivorship seed terms and a manual gold standard that considered all outcomes. However, the best model does not employ machine learning, but rather leverages list structure and resulted in 90.14 precision, 60.69 recall, 75.41 F1-score, and 92.60 accuracy with respect to the manual gold standard of all outcomes. Finally we developed a silver standard with a precision of 89.28 and recall of 86.77 compared to the manual gold standard and used the silver standard to identify all outcomes reported for five breast cancer treatments. The increased precision afforded by this approach reveals that in contrast to chemotherapy and targeted therapy, the surrogate outcome disease free survival (DFS) is reported more frequently than the clinically relevant outcome overall survival (OS) for hormone therapies, which is consistent with findings that DFS translates into firm OS improvements in a hormone therapy setting."
902,d5d9f56ea43fb6de0cfcd1109ebc0ed1430f5706,10.1186/s12911-019-0992-8,"Machine learning can assist with multiple tasks during systematic reviews to facilitate the rapid retrieval of relevant references during screening and to identify and extract information relevant to the study characteristics, which include the PICO elements of patient/population, intervention, comparator, and outcomes. The latter requires techniques for identifying and categorising fragments of text, known as named entity recognition. A publicly available corpus of PICO annotations on biomedical abstracts is used to train a named entity recognition model, which is implemented as a recurrent neural network. This model is then applied to a separate collection of abstracts for references from systematic reviews within biomedical and health domains. The occurrences of words tagged in the context of specific PICO contexts are used as additional features for a relevancy classification model. Simulations of the machine learning-assisted screening are used to evaluate the work saved by the relevancy model with and without the PICO features. Chi-squared and statistical significance of positive predicted values are used to identify words that are more indicative of relevancy within PICO contexts. Inclusion of PICO features improves the performance metric on 15 of the 20 collections, with substantial gains on certain systematic reviews. Examples of words whose PICO context are more precise can explain this increase. Words within PICO tagged segments in abstracts are predictive features for determining inclusion. Combining PICO annotation model into the relevancy classification pipeline is a promising approach. The annotations may be useful on their own to aid users in pinpointing necessary information for data extraction, or to facilitate semantic search."
903,d9cfdd8d7fd678e7705459ede406c24d93752199,,No DOI detected
904,e956027633a5b063c0a97f441be6c3fe496c4a12,10.1016/j.jclinepi.2018.08.023,"Abstract Objectives Systematic reviews and meta-analyses are labor-intensive and time-consuming. Automated extraction of quantitative data from primary studies can accelerate this process. ClinicalTrials.gov, launched in 2000, is the world's largest trial repository of results data from clinical trials; it has been used as a source instead of journal articles. We have developed a Web application called EXACT (EXtracting Accurate efficacy and safety information from ClinicalTrials.gov) that allows users without advanced programming skills to automatically extract data from ClinicalTrials.gov in analysis-ready format. We have also used the automatically extracted data to examine the reproducibility of meta-analyses in three published systematic reviews. Study Design and Setting We developed a Python-based software application (EXACT) that automatically extracts data required for meta-analysis from the ClinicalTrials.gov database in a spreadsheet format. We confirmed the accuracy of the extracted data and then used those data to repeat meta-analyses in three published systematic reviews. To ensure that we used the same statistical methods and outcomes as the published systematic reviews, we repeated the meta-analyses using data manually extracted from the relevant journal articles. For the outcomes whose results we were able to reproduce using those journal article data, we examined the usability of ClinicalTrials.gov data. Results EXACT extracted data at ClincalTrials.gov with 100% accuracy, and it required 60% less time than the usual practice of manually extracting data from journal articles. We found that 87% of the data elements extracted using EXACT matched those extracted manually from the journal articles. We were able to reproduce 24 of 28 outcomes using the journal article data. Of these 24 outcomes, we were able to reproduce 83.3% of the published estimates using data at ClinicalTrials.gov . Conclusion EXACT ( http://bio-nlp.org/EXACT ) automatically and accurately extracted data elements from ClinicalTrials.gov and thus reduced time in data extraction. The ClinicalTrials.gov data reproduced most meta-analysis results in our study, but this conclusion needs further validation."
908,1c3823649e4bfb81c0f6cc2b7089383b72017a80,10.1016/j.jclinepi.2018.06.011,"Abstract Objectives To illustrate the use of process mining concepts, techniques, and tools to improve the systematic review process. Study Design and Setting We simulated review activities and step-specific methods in the process for systematic reviews conducted by one research team over 1 year to generate an event log of activities, with start/end dates, reviewer assignment by expertise, and person-hours worked. Process mining techniques were applied to the event log to “discover” process models, which allowed visual display, animation, or replay of the simulated review activities. Summary statistics were calculated for person-time and timelines. We also analyzed the social networks of team interactions. Results The 12 simulated reviews included an average of 3,831 titles/abstracts (range: 1,565–6,368) and 20 studies (6–42). The average review completion time was 463 days (range: 289–629) (881 person-hours [range: 243–1,752]). The average person-hours per activity were study selection 26%, data collection 24%, report preparation 23%, and meta-analysis 17%. Social network analyses showed the organizational interaction of team members, including how they worked together to complete review tasks and to hand over tasks upon completion. Conclusion Event log and process mining can be valuable tools for research teams interested in improving and modernizing the systematic review process."
917,077326a79e2a3e95aab41e8d2df3fc44ae47d8e0,10.1016/j.jbi.2016.10.014,"Abstract Objectives Extracting data from publication reports is a standard process in systematic review (SR) development. However, the data extraction process still relies too much on manual effort which is slow, costly, and subject to human error. In this study, we developed a text summarization system aimed at enhancing productivity and reducing errors in the traditional data extraction process. Methods We developed a computer system that used machine learning and natural language processing approaches to automatically generate summaries of full-text scientific publications. The summaries at the sentence and fragment levels were evaluated in finding common clinical SR data elements such as sample size, group size, and PICO values. We compared the computer-generated summaries with human written summaries (title and abstract) in terms of the presence of necessary information for the data extraction as presented in the Cochrane review’s study characteristics tables. Results At the sentence level, the computer-generated summaries covered more information than humans do for systematic reviews (recall 91.2% vs. 83.8%, p  Conclusion Computer-generated summaries are potential alternative information sources for data extraction in systematic review development. Machine learning and natural language processing are promising approaches to the development of such an extractive summarization system."
918,0999e7808d7947858f24d71a0f99d54a3c3eb3dd,10.1016/j.ajp.2011.03.003,"Abstract Background Transcranial magnetic stimulation (TMS) has played an important role in the fields of psychiatry, neurology and neuroscience, since its emergence in the mid-1980s; and several high quality reviews have been produced since then. Most high quality reviews serve as powerful tools in the evaluation of predefined tendencies, but they cannot actually uncover new trends within the literature. However, special statistical procedures to ‘mine’ the literature have been developed which aid in achieving such a goal. Objectives This paper aims to uncover patterns within the literature on TMS as a whole, as well as specific trends in the recent literature on TMS for the treatment of depression. Methods Data mining and text mining. Results Currently there are 7299 publications, which can be clustered in four essential themes. Considering the frequency of the core psychiatric concepts within the indexed literature, the main results are: depression is present in 13.5% of the publications; Parkinson's disease in 2.94%; schizophrenia in 2.76%; bipolar disorder in 0.158%; and anxiety disorder in 0.142% of all the publications indexed in PubMed. Several other perspectives are discussed in the article."
919,09b4df8f77024b7b1b6c74cf410aacbe596079e7,10.1016/j.envint.2019.105228,"Abstract Background Systematic reviews involve mining literature databases to identify relevant studies. Identifying potentially relevant studies can be informed by computational tools comparing text similarity between candidate studies and selected key (i.e., seed) references. Challenge Using computational approaches to identify relevant studies for risk assessments is challenging, as these assessments examine multiple chemical effects across lifestages (e.g., human health risk assessments) or specific effects of multiple chemicals (e.g., cumulative risk). The broad scope of potentially relevant literature can make selection of seed references difficult. Approach We developed a generalized computational scoping strategy to identify human health relevant studies for multiple chemicals and multiple effects. We used semi-supervised machine learning to prioritize studies to review manually with training data derived from references cited in the hazard identification sections of several US EPA Integrated Risk Information System (IRIS) assessments. These generic training data or seed studies were clustered with the unclassified corpus to group studies based on text similarity. Clusters containing a high proportion of seed studies were prioritized for manual review. Chemical names were removed from seed studies prior to clustering resulting in a generic, chemical-independent method for identifying potentially human health relevant studies. We developed a case study that focused on identifying the array of chemicals that have been studied with respect to in utero exposure to test the recall of this novel literature searching strategy. We then evaluated the general strategy of using generic, chemical-independent training data with two previous IRIS assessments by comparing studies predicted relevant to those used in the assessments (i.e., total relevant). Outcome A keyword search designed to retrieve studies that examined the in utero effects of environmental chemicals identified over 54,000 candidate references. Clustering algorithms were applied using 1456 studies from multiple IRIS assessments with chemical names removed as training data or seeds (i.e., semi-supervised learning). Using a six-algorithm ensemble approach 2602 articles, or approximately 5% of candidate references, were “voted” relevant by four or more clustering algorithms and manual review confirmed nearly 50% of these studies were relevant. Further evaluations on two IRIS assessments, using a nine-algorithm ensemble approach and a set of generic, chemical-independent, externally-derived seed studies correctly identified 77–83% of hazard identification studies published in the assessments and eliminated the need to manually screen more than 75% of search results on average. Limitations The chemical-independent approach used to build the training literature set provides a broad and unbiased picture across a variety of endpoints and environmental exposures but does not systematically identify all available data. Variance between actual and predicted relevant studies will be greater because of the external and non-random origin of seed study selection. This approach depends on access to readily available generic training data that can be used to locate relevant references in an unclassified corpus. Impact A generic approach to identifying human health relevant studies could be an important first step in literature evaluation for risk assessments. This initial scoping approach could facilitate faster literature evaluation by focusing reviewer efforts, as well as potentially minimize reviewer bias in selection of key studies. Using externally-derived training data has applicability particularly for databases with very low search precision where identifying training data may be cost-prohibitive."
920,0d73a6cbd8c469829049abe5412bec5e06b200ec,10.1016/j.jbi.2019.103202,"Abstract Context Citation screening (also called study selection) is a phase of systematic review process that has attracted a growing interest on the use of text mining (TM) methods to support it to reduce time and effort. Search results are usually imbalanced between the relevant and the irrelevant classes of returned citations. Class imbalance among other factors has been a persistent problem that impairs the performance of TM models, particularly in the context of automatic citation screening for systematic reviews. This has often caused the performance of classification models using the basic title and abstract data to ordinarily fall short of expectations. Objective In this study, we explore the effects of using full bibliography data in addition to title and abstract on text classification performance for automatic citation screening. Methods We experiment with binary and Word2vec feature representations and SVM models using 4 software engineering (SE) and 15 medical review datasets. We build and compare 3 types of models (binary-non-linear, Word2vec-linear and Word2vec-non-linear kernels) with each dataset using the two feature sets. Results The bibliography enriched data exhibited consistent improved performance in terms of recall, work saved over sampling (WSS) and Matthews correlation coefficient (MCC) in 3 of the 4 SE datasets that are fairly large in size. For the medical datasets, the results vary, however in the majority of cases the performance is the same or better. Conclusion Inclusion of the bibliography data provides the potential of improving the performance of the models but to date results are inconclusive."
921,10a442f275db09db8ed7281f0821f70dca9d26f4,10.1186/s13643-019-1161-y,"Experimental designs for evaluating knowledge translation (KT) interventions can provide strong estimates of effectiveness but offer limited insight into how the intervention worked. Consequently, process evaluations have been used to explore the causal mechanisms at work; however, there are limited standards to guide this work. This study synthesizes current evidence of KT process evaluations to provide future methodological recommendations. Peer-reviewed search strategies were developed by a health research librarian. Studies had to be in English, published since 1996, and were not excluded based on design. Studies had to (1) be a process evaluation of a KT intervention study in primary health, (2) be a primary research study, and (3) include a licensed healthcare professional delivering or receiving the intervention. A two-step, two-person hybrid screening approach was used for study inclusion with inter-rater reliability ranging from 94 to 95%. Data on study design, data collection, theoretical influences, and approaches used to evaluate the KT intervention, analysis, and outcomes were extracted by two reviewers. Methodological quality was assessed with the Mixed Methods Appraisal Tool (MMAT). Of the 20,968 articles screened, 226 studies fit our inclusion criteria. The majority of process evaluations used qualitative forms of data collection (43.4%) and individual interviews as the predominant data collection method. 72.1% of studies evaluated barriers and/or facilitators to implementation. 59.7% of process evaluations were stand-alone evaluations. The timing of data collection varied widely with post-intervention data collection being the most frequent (46.0%). Only 38.1% of the studies were informed by theory. Furthermore, 38.9% of studies had MMAT scores of 50 or less indicating poor methodological quality. There is widespread acceptance that the generalizability of quantitative trials of KT interventions would be significantly enhanced through complementary process evaluations. However, this systematic review found that process evaluations are of mixed quality and lack theoretical guidance. Most process evaluation data collection occurred post-intervention undermining the ability to evaluate the process of implementation. Strong science and methodological guidance is needed to underpin and guide the design and execution of process evaluations in KT science. This study is not registered with PROSPERO."
926,18519cf7671d0f4defdb5f5119ed56d417dfc7a0,10.1016/j.compag.2018.12.044,"Various Farm Management Information Systems (FMISs) have been developed to support the management of the farm businesses. These FMISs typically support the different domains of the agricultural sector, such as arable and dairy farming; and include different set of features, such as crop, field, and financial management. These FMISs also have to deal with diverse obstacles during their development and adoption, such as lack of standardized data, cost and usability. Though several papers have been published in the past several years on this topic, there has been no explicit attempt to systematically review these papers to identify and characterize the features and obstacles. The objective of this study is to identify and describe the state-of-the-art of FMISs and as such pave the way for further research and development of FMISs. We applied a systematic literature review protocol in which we included the literature published from 2008 to 2018. We found 1048 papers of which 38 papers were selected as primary studies that we analyzed further in detail. From the detailed analysis, we identified 81 unique FMIS features and 51 unique obstacles of FMISs. We have systematically ranked the identified features and obstacles and describe the key associated aspects. These aspects include the agricultural domains, modeling approaches, delivery models, and identified stakeholders."
929,30a627f06cca149fdc1e9b2d90f75caf7dbd7112,10.1007/978-3-030-10752-9_1,"With the increasing number of scientific publications, the analysis of the trends and the state-of-the-art in a certain scientific field is becoming very time-consuming and tedious task. In response to urgent needs of information, for which the existing systematic review model does not well, several other review types have emerged, namely the rapid review and scoping reviews. In this paper, we propose an NLP powered tool that automates most of the review process by automatic analysis of articles indexed in the IEEE Xplore, PubMed, and Springer digital libraries. We demonstrate the applicability of the toolkit by analyzing articles related to Enhanced Living Environments and Ambient Assisted Living, in accordance with the PRISMA surveying methodology. The relevant articles were processed by the NLP toolkit to identify articles that contain up to 20 properties clustered into 4 logical groups. The analysis showed increasing attention from the scientific communities towards Enhanced and Assisted living environments over the last 10 years and showed several trends in the specific research topics that fall into this scope. The case study demonstrates that the NLP toolkit can ease and speed up the review process and show valuable insights from the surveyed articles even without manually reading of most of the articles. Moreover, it pinpoints the most relevant articles which contain more properties and therefore, significantly reduces the manual work, while also generating informative tables, charts and graphs."
936,470fc4ab65883d59284abe77a2b55907a95203bb,10.1007/s10796-015-9589-7,"While systematic reviews (SRs) are positioned as an essential element of modern evidence-based medical practice, the creation and update of these reviews is resource intensive. In this research, we propose to leverage advanced analytics techniques for automatically classifying articles for inclusion and exclusion for systematic reviews. Specifically, we used soft-margin polynomial Support Vector Machine (SVM) as a classifier, exploited Unified Medical Language Systems (UMLS) for medical terms extraction, and examined various techniques to resolve the class imbalance issue. Through an empirical study, we demonstrated that soft-margin polynomial SVM achieves better classification performance than the existing algorithms used in current research, and the performance of the classifier can be further improved by using UMLS to identify medical terms in articles and applying re-sampling methods to resolve the class imbalance issue."
938,517ebccb921c305395dc163af61165e95fcf4572,10.1016/j.mex.2020.100831,"Within a systematic literature review (SLR), researchers are confronted with vast amounts of articles from scientific databases, which have to be manually evaluated regarding their relevance for a certain field of observation. The evaluation and filtering phase of prevalent SLR methodologies is therefore time consuming and hardly expressible to the intended audience. The proposed method applies natural language processing (NLP) on article meta data and a k-means clustering algorithm to automatically convert large article corpora into a distribution of focal topics. This allows efficient filtering as well as objectifying the process through the discussion of the clustering results. Beyond that, it allows to quickly identify scientific communities and therefore provides an iterative perspective for the so far linear SLR methodology.•NLP and k-means clustering to filter large article corpora during systematic literature reviews.•Automated clustering allows filtering very efficiently as well as effectively compared to manual selection.•Presentation and discussion of the clustering results helps to objectify the nontransparent filtering step in systematic literature reviews."
940,534a24f07a5d55fd8d13cf152c59acb75bae99a9,10.1016/j.jbi.2015.05.004,"Display Omitted Manual processes used in systematic reviews and comparative effectiveness studies are slow.We built a system to extract 3 facets (agent, object and endpoints) from comparative sentences.The system achieved 73%, 93%, and 73% accuracy for agents, objects, and endpoints respectively.The situated case-study of Metformin shows how the system results can inform a systematic review.The gaps shown in the tabular summary can help to focus where future work should be directed. Preparing a systematic review can take hundreds of hours to complete, but the process of reconciling different results from multiple studies is the bedrock of evidence-based medicine. We introduce a two-step approach to automatically extract three facets - two entities (the agent and object) and the way in which the entities are compared (the endpoint) - from direct comparative sentences in full-text articles. The system does not require a user to predefine entities in advance and thus can be used in domains where entity recognition is difficult or unavailable. As with a systematic review, the tabular summary produced using the automatically extracted facets shows how experimental results differ between studies. Experiments were conducted using a collection of more than 2million sentences from three journals Diabetes, Carcinogenesis and Endocrinology and two machine learning algorithms, support vector machines (SVM) and a general linear model (GLM). F1 and accuracy measures for the SVM and GLM differed by only 0.01 across all three comparison facets in a randomly selected set of test sentences. The system achieved the best performance of 92% for objects, whereas the accuracy for both agent and endpoints was 73%. F1 scores were higher for objects (0.77) than for endpoints (0.51) or agents (0.47). A situated evaluation of Metformin, a drug to treat diabetes, showed system accuracy of 95%, 83% and 79% for the object, endpoint and agent respectively. The situated evaluation had higher F1 scores of 0.88, 0.64 and 0.62 for object, endpoint, and agent respectively. On average, only 5.31% of the sentences in a full-text article are direct comparisons, but the tabular summaries suggest that these sentences provide a rich source of currently underutilized information that can be used to accelerate the systematic review process and identify gaps where future research should be focused."
947,70dcd086804f20d33f38b4159e5925774ad691aa,10.1016/j.jclinepi.2008.10.016,"Abstract Objective To assist investigators planning, coordinating, and conducting systematic reviews in the selection of data-extraction tools for conducting systematic reviews. Study Design and Setting We constructed an initial table listing available data-collection tools and reflecting our experience with these tools and their performance. An international group of experts iteratively reviewed the table and reflected on the performance of the tools until no new insights and consensus resulted. Results Several tools are available to manage data in systematic reviews, including paper and pencil, spreadsheets, web-based surveys, electronic databases, and web-based specialized software. Each tool offers benefits and drawbacks: specialized web-based software is well suited in most ways, but is associated with higher setup costs. Other approaches vary in their setup costs and difficulty, training requirements, portability and accessibility, versatility, progress tracking, and the ability to manage, present, store, and retrieve data. Conclusion Available funding, number and location of reviewers, data needs, and the complexity of the project should govern the selection of a data-extraction tool when conducting systematic reviews."
954,7e5954bde873c9f5bf74d2414d4002377a8e4624,10.1186/1471-2105-11-S7-S15,"Background
Nuclear factor kappa B (NF-κB) is a chief nuclear transcription factor that controls the transcription of various genes; and its activation is tightly controlled by Inhibitor kappa B kinase (IKK). The irregular transcription of NF-κB has been linked to auto-immune disorders, cancer and other diseases. The IKK complex is composed of three units, IKKα, IKKβ, and the regulatory domain NEMO, of which IKKβ is well understood in the canonical pathway. Therefore, the inhibition of IKKβ by drugs forms the molecular basis for anti-inflammatory drug research."
956,820ea9e99b281b1b12cb7754e9d99376bd05da8f,10.1016/j.jbi.2013.10.005,"Objective: To determine whether SVM-based classifiers, which are trained on a combination of inclusion and common exclusion articles, are useful to experts reviewing journal articles for inclusion during new systematic reviews. Methods: Test collections were built using the annotated reference files from 19 procedure and 4 drug systematic reviews. The classifiers were trained by balanced data sets, which were sampled using random sampling. This approach compared two balanced data sets, one with a combination of included and commonly excluded articles and one with a combination of included and excluded articles. AUCs were used as evaluation metrics. Results: The AUCs of the classifiers, which were trained on the balanced data set with included and commonly excluded articles, were significantly higher than those of the classifiers, which were trained on the balanced data set with included and excluded articles. Conclusion: Automatic, high-quality article classifiers using machine learning could reduce the workload of experts performing systematic reviews when topic-specific data are scarce. In particular, when used as training data, a combination of included and commonly excluded articles is more helpful than a combination of included and excluded articles."
958,85e0b98480a455c8061ad5f61dbfca387e482675,10.1186/s13643-018-0724-7,"Screening candidate studies for inclusion in a systematic review is time-consuming when conducted manually. Automation tools could reduce the human effort devoted to screening. Existing methods use supervised machine learning which train classifiers to identify relevant words in the abstracts of candidate articles that have previously been labelled by a human reviewer for inclusion or exclusion. Such classifiers typically reduce the number of abstracts requiring manual screening by about 50%. We extracted four key characteristics of observational studies (population, exposure, confounders and outcomes) from the text of titles and abstracts for all articles retrieved using search strategies from systematic reviews. Our screening method excluded studies if they did not meet a predefined set of characteristics. The method was evaluated using three systematic reviews. Screening results were compared to the actual inclusion list of the reviews. The best screening threshold rule identified studies that mentioned both exposure (E) and outcome (O) in the study abstract. This screening rule excluded 93.7% of retrieved studies with a recall of 98%. Filtering studies for inclusion in a systematic review based on the detection of key study characteristics in abstracts significantly outperformed standard approaches to automated screening and appears worthy of further development and evaluation."
959,88c15577c1a7c6761c8829bf4e471dc8267d2658,10.1016/j.infsof.2010.05.003,"Context: Aspect-oriented programming (AOP) promises to improve many facets of software quality by providing better modularization and separation of concerns, which may have system wide affect. There have been numerous claims in favor and against AOP compared with traditional programming languages such as Objective Oriented and Structured Programming Languages. However, there has been no attempt to systematically review and report the available evidence in the literature to support the claims made in favor or against AOP compared with non-AOP approaches. Objective: This research aimed to systematically identify, analyze, and report the evidence published in the literature to support the claims made in favor or against AOP compared with non-AOP approaches. Method: We performed a systematic literature review of empirical studies of AOP based development, published in major software engineering journals and conference proceedings. Results: Our search strategy identified 3307 papers, of which 22 were identified as reporting empirical studies comparing AOP with non-AOP approaches. Based on the analysis of the data extracted from those 22 papers, our findings show that for performance, code size, modularity, and evolution related characteristics, a majority of the studies reported positive effects, a few studies reported insignificant effects, and no study reported negative effects; however, for cognition and language mechanism, negative effects were reported. Conclusion: AOP is likely to have positive effect on performance, code size, modularity, and evolution. However its effect on cognition and language mechanism is less likely to be positive. Care should be taken using AOP outside the context in which it has been validated."
961,8d6bce146d838d80c651c07ab03fa14570b7df43,10.1016/j.jbi.2017.06.018,"Citation screening, an integral process within systematic reviews that identifies citations relevant to the underlying research question, is a time-consuming and resource-intensive task. During the screening task, analysts manually assign a label to each citation, to designate whether a citation is eligible for inclusion in the review. Recently, several studies have explored the use of active learning in text classification to reduce the human workload involved in the screening task. However, existing approaches require a significant amount of manually labelled citations for the text classification to achieve a robust performance. In this paper, we propose a semi-supervised method that identifies relevant citations as early as possible in the screening process by exploiting the pairwise similarities between labelled and unlabelled citations to improve the classification performance without additional manual labelling effort. Our approach is based on the hypothesis that similar citations share the same label (e.g., if one citation should be included, then other similar citations should be included also). To calculate the similarity between labelled and unlabelled citations we investigate two different feature spaces, namely a bag-of-words and a spectral embedding based on the bag-of-words. The semi-supervised method propagates the classification codes of manually labelled citations to neighbouring unlabelled citations in the feature space. The automatically labelled citations are combined with the manually labelled citations to form an augmented training set. For evaluation purposes, we apply our method to reviews from clinical and public health. The results show that our semi-supervised method with label propagation achieves statistically significant improvements over two state-of-the-art active learning approaches across both clinical and public health reviews."
966,a4582de7632cbb041f327646687469b8bf0d2848,10.1016/j.jbi.2015.09.003,"Display Omitted Automated citation finding can augment manual literature search.We built a gold standard with citations from 653 guideline recommendations.The query expansion method vs. PubMed improved recall with non-significant loss on precision.The unsupervised citation ranking approach performed better than standard PubMed ranking and a machine learning classifier. ObjectiveLiterature database search is a crucial step in the development of clinical practice guidelines and systematic reviews. In the age of information technology, the process of literature search is still conducted manually, therefore it is costly, slow and subject to human errors. In this research, we sought to improve the traditional search approach using innovative query expansion and citation ranking approaches. MethodsWe developed a citation retrieval system composed of query expansion and citation ranking methods. The methods are unsupervised and easily integrated over the PubMed search engine. To validate the system, we developed a gold standard consisting of citations that were systematically searched and screened to support the development of cardiovascular clinical practice guidelines. The expansion and ranking methods were evaluated separately and compared with baseline approaches. ResultsCompared with the baseline PubMed expansion, the query expansion algorithm improved recall (80.2% vs. 51.5%) with small loss on precision (0.4% vs. 0.6%). The algorithm could find all citations used to support a larger number of guideline recommendations than the baseline approach (64.5% vs. 37.2%, p<0.001). In addition, the citation ranking approach performed better than PubMed's most recent ranking (average precision +6.5%, [email protected] +21.1%, p<0.001), PubMed's rank by relevance (average precision +6.1%, [email protected] +14.8%, p<0.001), and the machine learning classifier that identifies scientifically sound studies from MEDLINE citations (average precision +4.9%, [email protected] +4.2%, p<0.001). ConclusionsOur unsupervised query expansion and ranking techniques are more flexible and effective than PubMed's default search engine behavior and the machine learning classifier. Automated citation finding is promising to augment the traditional literature search."
972,bb78a74af76863fe88f64110b41aa308947b43fa,10.1007/s11219-017-9386-2,"Testing safety-critical systems is crucial since a failure or malfunction may result in death or serious injuries to people, equipment, or environment. An important challenge in testing is the derivation of test cases that can identify the potential faults. Model-based testing adopts models of a system under test and/or its environment to derive test artifacts. This paper aims to provide a systematic mapping study to identify, analyze, and describe the state-of-the-art advances in model-based testing for software safety. The systematic mapping study is conducted as a multi-phase study selection process using the published literature in major software engineering journals and conference proceedings. We reviewed 751 papers and 36 of them have been selected as primary studies to answer our research questions. Based on the analysis of the data extraction process, we discuss the primary trends and approaches and present the identified obstacles. This study shows that model-based testing can provide important benefits for software safety testing. Several solution directions have been identified, but further research is critical for reliable model-based testing approach for safety."
974,ce34535828226035a20af43a4755499bfe3ba33d,10.1016/j.jbi.2016.06.001,"Display Omitted We propose a topic detection method based on paragraph vectors.The method is integrated with an active learner to accelerate citation screening.The method outperforms LDA when applied to clinical and public health reviews. Systematic reviews require expert reviewers to manually screen thousands of citations in order to identify all relevant articles to the review. Active learning text classification is a supervised machine learning approach that has been shown to significantly reduce the manual annotation workload by semi-automating the citation screening process of systematic reviews. In this paper, we present a new topic detection method that induces an informative representation of studies, to improve the performance of the underlying active learner. Our proposed topic detection method uses a neural network-based vector space model to capture semantic similarities between documents. We firstly represent documents within the vector space, and cluster the documents into a predefined number of clusters. The centroids of the clusters are treated as latent topics. We then represent each document as a mixture of latent topics. For evaluation purposes, we employ the active learning strategy using both our novel topic detection method and a baseline topic model (i.e., Latent Dirichlet Allocation). Results obtained demonstrate that our method is able to achieve a high sensitivity of eligible studies and a significantly reduced manual annotation cost when compared to the baseline method. This observation is consistent across two clinical and three public health reviews. The tool introduced in this work is available from https://nactem.ac.uk/pvtopic/."
979,d6d62e7d4bd108a0b548ee6bdb4cfb662ca3d1ff,10.1016/j.jbi.2016.03.026,"Display Omitted Full-texts in PDF format are the main sources for the systematic reviews development.PDF reports used in Cochrane systematic reviews were used as gold standard.The multi-pass sieve framework is a powerful approach in text classification.Filtering semi-structured and metadata improved IE performance in full-texts. ObjectivesData extraction from original study reports is a time-consuming, error-prone process in systematic review development. Information extraction (IE) systems have the potential to assist humans in the extraction task, however majority of IE systems were not designed to work on Portable Document Format (PDF) document, an important and common extraction source for systematic review. In a PDF document, narrative content is often mixed with publication metadata or semi-structured text, which add challenges to the underlining natural language processing algorithm. Our goal is to categorize PDF texts for strategic use by IE systems. MethodsWe used an open-source tool to extract raw texts from a PDF document and developed a text classification algorithm that follows a multi-pass sieve framework to automatically classify PDF text snippets (for brevity, texts) into TITLE, ABSTRACT, BODYTEXT, SEMISTRUCTURE, and METADATA categories. To validate the algorithm, we developed a gold standard of PDF reports that were included in the development of previous systematic reviews by the Cochrane Collaboration. In a two-step procedure, we evaluated (1) classification performance, and compared it with machine learning classifier, and (2) the effects of the algorithm on an IE system that extracts clinical outcome mentions. ResultsThe multi-pass sieve algorithm achieved an accuracy of 92.6%, which was 9.7% (p<0.001) higher than the best performing machine learning classifier that used a logistic regression algorithm. F-measure improvements were observed in the classification of TITLE (+15.6%), ABSTRACT (+54.2%), BODYTEXT (+3.7%), SEMISTRUCTURE (+34%), and MEDADATA (+14.2%). In addition, use of the algorithm to filter semi-structured texts and publication metadata improved performance of the outcome extraction system (F-measure +4.1%, p=0.002). It also reduced of number of sentences to be processed by 44.9% (p<0.001), which corresponds to a processing time reduction of 50% (p=0.005). ConclusionsThe rule-based multi-pass sieve framework can be used effectively in categorizing texts extracted from PDF documents. Text classification is an important prerequisite step to leverage information extraction from PDF documents."
982,db8d4ae8d82fb2ae7d7eb1c0013ad7174a1bb1a5,10.1016/J.JCLINEPI.2004.11.024,"Abstract Background and Objective Extracting data from primary articles is an essential component in conducting systematic reviews. Incorrect data extraction can lead to false conclusions. The objective of this study was to retrospectively repeat the data extraction in all systematic reviews conducted by the Cochrane Cystic Fibrosis and Genetic Disorders Group. Study Design and Setting For each review, data extraction was conducted, by an experienced statistician, for the same publications used by the reviewers. Results were compared with those obtained by the reviewers. Results Errors were found in 20 of 34 reviews, including incorrect calculations made when converting data in primary articles into data required for the review (2 reviews) and misinterpretation of data that were reported in the primary article (7 reviews). All data-handling errors led to changes in the summary results, but none of these affected the review conclusions. Conclusions Important errors were identified in a high proportion of reviews. A variety of problems relating to the reporting of results within a review were identified, but these did not lead to substantial changes in any conclusion."
984,e1e48e1ff640ec9bd3c5b61c9f30c0957b61dec6,10.1186/s13643-019-0942-7,"Here, we outline a method of applying existing machine learning (ML) approaches to aid citation screening in an on-going broad and shallow systematic review of preclinical animal studies. The aim is to achieve a high-performing algorithm comparable to human screening that can reduce human resources required for carrying out this step of a systematic review. We applied ML approaches to a broad systematic review of animal models of depression at the citation screening stage. We tested two independently developed ML approaches which used different classification models and feature sets. We recorded the performance of the ML approaches on an unseen validation set of papers using sensitivity, specificity and accuracy. We aimed to achieve 95% sensitivity and to maximise specificity. The classification model providing the most accurate predictions was applied to the remaining unseen records in the dataset and will be used in the next stage of the preclinical biomedical sciences systematic review. We used a cross-validation technique to assign ML inclusion likelihood scores to the human screened records, to identify potential errors made during the human screening process (error analysis). ML approaches reached 98.7% sensitivity based on learning from a training set of 5749 records, with an inclusion prevalence of 13.2%. The highest level of specificity reached was 86%. Performance was assessed on an independent validation dataset. Human errors in the training and validation sets were successfully identified using the assigned inclusion likelihood from the ML model to highlight discrepancies. Training the ML algorithm on the corrected dataset improved the specificity of the algorithm without compromising sensitivity. Error analysis correction leads to a 3% improvement in sensitivity and specificity, which increases precision and accuracy of the ML algorithm. This work has confirmed the performance and application of ML algorithms for screening in systematic reviews of preclinical animal studies. It has highlighted the novel use of ML algorithms to identify human error. This needs to be confirmed in other reviews with different inclusion prevalence levels, but represents a promising approach to integrating human decisions and automation in systematic review methodology."
987,f89b9e714d521797450e8b896779afbc680ce3d8,10.1016/j.conctc.2019.100443,"Abstract Background More than 90% of clinical-trial compounds fail to demonstrate sufficient efficacy and safety. To help alleviate this issue, systematic literature review and meta-analysis (SLR), which synthesize current evidence for a research question, can be applied to preclinical evidence to identify the most promising therapeutics. However, these methods remain time-consuming and labor-intensive. Here, we introduce an economic formula to estimate the expense of SLR for academic institutions and pharmaceutical companies. Methods We estimate the manual effort involved in SLR by quantifying the amount of labor required and the total associated labor cost. We begin with an empirical estimation and derive a formula that quantifies and describes the cost. Results The formula estimated that each SLR costs approximately $141,194.80. We found that on average, the ten largest pharmaceutical companies publish 118.71 and the ten major academic institutions publish 132.16 SLRs per year. On average, the total cost of all SLRs per year to each academic institution amounts to $18,660,304.77 and for each pharmaceutical company is $16,761,234.71. Discussion It appears that SLR is an important, but costly mechanisms to assess the totality of evidence. Conclusions With the increase in the number of publications, the significant time and cost of SLR may pose a barrier to their consistent application to assess the promise of clinical trials thoroughly. We call on investigators and developers to develop automated solutions to help with the assessment of preclinical evidence particularly. The formula we introduce provides a cost baseline against which the efficiency of automation can be measured."
989,fb8e6a050d1a20e65c784793e29116727efae298,10.1016/j.eswax.2020.100030,"Abstract Citation screening is a labour-intensive part of the process of a systematic literature review that identifies citations eligible for inclusion in the review. In this paper, we present an automatic text classification approach that aims to prioritise eligible citations earlier than ineligible ones and thus reduces the manual labelling effort that is involved in the screening process. e.g. by automatically excluding lower ranked citations. To improve the performance of the text classifier, we develop a novel neural network-based feature extraction method. Unlike previous approaches to citation screening that employ unsupervised feature extraction methods to address a supervised classification task, our proposed method extracts document features in a supervised setting. In particular, our method generates a feature representation for documents, which is explicitly optimised to discriminate between eligible and ineligible citations. The generated document representation is subsequently used to train a text classifier. Experiments show that our feature extraction method obtains average workload savings of 56% when evaluated across 23 medical systematic reviews. The proposed method outperforms 10 baseline feature extraction methods by approximately 6% in terms of the WSS@95% metric."
992,ff976d928c130b6f50522a93d940f617d99a2bbd,10.1186/s13643-018-0740-7,"Systematic reviews (SR) are vital to health care, but have become complicated and time-consuming, due to the rapid expansion of evidence to be synthesised. Fortunately, many tasks of systematic reviews have the potential to be automated or may be assisted by automation. Recent advances in natural language processing, text mining and machine learning have produced new algorithms that can accurately mimic human endeavour in systematic review activity, faster and more cheaply. Automation tools need to be able to work together, to exchange data and results. Therefore, we initiated the International Collaboration for the Automation of Systematic Reviews (ICASR), to successfully put all the parts of automation of systematic review production together. The first meeting was held in Vienna in October 2015. We established a set of principles to enable tools to be developed and integrated into toolkits. This paper sets out the principles devised at that meeting, which cover the need for improvement in efficiency of SR tasks, automation across the spectrum of SR tasks, continuous improvement, adherence to high quality standards, flexibility of use and combining components, the need for a collaboration and varied skills, the desire for open source, shared code and evaluation, and a requirement for replicability through rigorous and open evaluation. Automation has a great potential to improve the speed of systematic reviews. Considerable work is already being done on many of the steps involved in a review. The ‘Vienna Principles’ set out in this paper aim to guide a more coordinated effort which will allow the integration of work by separate teams and build on the experience, code and evaluations done by the many teams working across the globe."
997,808e7ddafd750a97592cfb3f06d0ec0ef1c51a44,10.1080/08993408.2022.2039504,No abstract found
1002,26ea73ad1becdc787314148f77b41365e1a333a2,10.1016/j.ajsl.2021.11.004,"Abstract Digital transformation and automation in the shipping industry is resulting disruptive changes to ship design, operations, and manning that aim to enhance safety, efficiency, and the environmental sustainability of maritime logistics. While there is growing research interest in these areas, examining the role of human element in the new smart shipping context is largely neglected. Through a systematic literature review, this paper aims to explore the multi-dimensional impact of autonomous shipping technology resulting from the application of Industry 4.0 and future industrial revolutions on seafarers. The impacts include the changing role of seafarers on-board and the strategies required to engage seafarers in their transition from traditional shipping to autonomous and smart shipping. The paper concludes that Industry 4.0 is being challenged for its shortfall in recognition of the importance of human role and its intelligence in the expected current industrial revolution. As a result, there is a demand to look further and beyond Industry 4.0 by introducing the next generation of industrial revolution, namely Industry 5.0. This paper suggests that the impact of this revolution in the maritime industry can be defined by concepts such as Maritime 5.0, Shipping 5.0, Seafarer 5.0, Maritime Education and Training 5.0 (MET 5.0)."
1009,b95436862467de2461f69e3e935e4a1f44be2c94,10.1016/J.ASOC.2021.107765,"Abstract The systematic literature review (SLR) process is separated into several steps to increase rigor and reproducibility. The selection of primary studies (i.e., citation screening) is an important step in the SLR process. The citation screening process aims to identify the relevant primary studies fairly and with high rigor using selection criteria. Through the study selection criteria, reviewers determine whether an article should be included or excluded from the SLR. However, the screening process is highly time-consuming and error-prone as the researchers must read each title and possibly hundreds to thousands of abstracts and full-text documents. This study aims to automate the citation screening process using Deep Learning algorithms. With this, it is aimed to reduce the time and costs of the citation screening process and increase the precision and recall of the relevant primary studies. A Multi-Channel Convolutional Neural Network (CNN) is proposed, which can automatically classify a given set of citations. As the architecture uses the title and abstract as features, our end-to-end pipeline is domain-independent. We have performed six experiments to assess the performance of Multi-Channel CNNs across 20 publicly available systematic literature review datasets. It was shown that for 18 out of 20 review datasets, the proposed method achieved significant workload savings of at least 10%, while in several cases, our model yielded a statistically significantly better performance over two benchmark review datasets. We conclude that Multi-Channel CNNs are effective for the citation screening process in SLRs. Multi-Channel CNNs perform best on large datasets of over 2500 samples with few abstracts missing."
